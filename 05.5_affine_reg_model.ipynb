{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee94cda0",
   "metadata": {},
   "source": [
    "Affine registration of FA images. (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a72721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import monai\n",
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "import footsteps\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2129f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "spatial_size = (144,144,144)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8ecdb9",
   "metadata": {},
   "source": [
    "The input images are known to be $140\\times140\\times140$, and we will pad them out to $144$ in each dimension.\n",
    "\n",
    "We assume that each dimension in `spatial_size` is divisible by $2^{\\texttt{num}\\_\\texttt{scales}-1}$, because we will downsample by a factor of $2$ a bunch of times to produce images at different scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7490849",
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_dir = './dti_fit_images/fa'\n",
    "fa_key = 'fa'\n",
    "data = [{fa_key:path, \"filename\":os.path.basename(path)} for path in glob.glob(os.path.join(fa_dir,'*'))]\n",
    "data_train, data_valid = monai.data.utils.partition_dataset(data, ratios=(8,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f6318",
   "metadata": {},
   "source": [
    "`fa_keys` is a list mapping index to key for scale at that index: $0$ is the base resolution, $1$ is downscaled by a factor of $2$, $2$ is further downscaled by a factor of $2$, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b368d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_transforms = [\n",
    "    monai.transforms.LoadImageD(keys=fa_key),\n",
    "    monai.transforms.AddChannelD(keys=fa_key),\n",
    "    monai.transforms.SpatialPadD(keys=fa_key, spatial_size=spatial_size, mode=\"constant\"),\n",
    "    monai.transforms.ToTensorD(keys=fa_key),\n",
    "    monai.transforms.ToDeviceD(keys=fa_key, device=device),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc2c3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control the overall scale of affine transform\n",
    "a=0.5\n",
    "\n",
    "S = spatial_size[0]\n",
    "\n",
    "rand_affine_params = {\n",
    "    'prob':1.,\n",
    "    'mode': 'bilinear',\n",
    "    'padding_mode': 'zeros',\n",
    "    'spatial_size':spatial_size,\n",
    "    'cache_grid':True,\n",
    "    'rotate_range': (a*np.pi/2,)*3,\n",
    "    'shear_range': (0,)*6, # no shearing\n",
    "    'translate_range': (a*S/16,)*3,\n",
    "    'scale_range': (a*0.4,)*3,\n",
    "}\n",
    "\n",
    "rand_affine_transform = monai.transforms.RandAffineD(keys=fa_key, **rand_affine_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed049b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_valid = monai.transforms.Compose(base_transforms + [rand_affine_transform])\n",
    "transform_train = monai.transforms.Compose(base_transforms + [rand_affine_transform])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2206482e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds_train = monai.data.CacheDataset(data_train, transform_train)\n",
    "ds_valid = monai.data.CacheDataset(data_valid, transform_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e54a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "warp = monai.networks.blocks.Warp(mode=\"bilinear\", padding_mode=\"zeros\")\n",
    "\n",
    "def mse_loss(b1, b2):\n",
    "    \"\"\"Return image similarity loss given two batches b1 and b2 of shape (batch_size, channels, H,W,D).\n",
    "    It is scaled up a bit here.\"\"\"\n",
    "    return 10000*((b1-b2)**2).mean()\n",
    "\n",
    "def ncc_loss(b1, b2):\n",
    "    \"\"\"Return the negative NCC loss given two batches b1 and b2 of shape (batch_size, channels, H,W,D).\n",
    "    It is averaged over batches and channels.\"\"\"\n",
    "    mu1 = b1.mean(dim=(2,3,4)) # means\n",
    "    mu2 = b2.mean(dim=(2,3,4))\n",
    "    alpha1 = (b1**2).mean(dim=(2,3,4)) # second moments\n",
    "    alpha2 = (b2**2).mean(dim=(2,3,4))\n",
    "    alpha12 = (b1*b2).mean(dim=(2,3,4)) # cross term\n",
    "    numerator = alpha12 - mu1*mu2\n",
    "    denominator = torch.sqrt((alpha1 - mu1**2) * (alpha2-mu2**2))\n",
    "    ncc = numerator / denominator\n",
    "    return -ncc.mean() # average over batches and channels\n",
    "\n",
    "def compose_ddf(u,v):\n",
    "    \"\"\"Compose two displacement fields, return the displacement that warps by v followed by u\"\"\"\n",
    "    return u + warp(v,u)\n",
    "\n",
    "_, H, W, D = ds_train[0][fa_key].shape\n",
    "\n",
    "# Compute discrete spatial derivatives\n",
    "def diff_and_trim(array, axis):\n",
    "    \"\"\"Take the discrete difference along a spatial axis, which should be 2,3, or 4.\n",
    "    Return a difference tensor with all spatial axes trimmed by 1.\"\"\"\n",
    "    return torch.diff(array, axis=axis)[:, :, :(H-1), :(W-1), :(D-1)]\n",
    "\n",
    "def size_of_spatial_derivative(u):\n",
    "    \"\"\"Return the squared Frobenius norm of the spatial derivative of the given displacement field.\n",
    "    To clarify, this is about the derivative of the actual displacement field map, not the deformation\n",
    "    that the displacement field map defines. The expected input shape is (batch,3,H,W,D).\n",
    "    Output shape is (batch).\"\"\"\n",
    "    dx = diff_and_trim(u, 2)\n",
    "    dy = diff_and_trim(u, 3)\n",
    "    dz = diff_and_trim(u, 4)\n",
    "    return(dx**2 + dy**2 + dz**2).sum(axis=1).mean(axis=[1,2,3])\n",
    "\n",
    "def compose_affine(u,v):\n",
    "    \"\"\"Return the product u.v of two affine transforms given as tensors of shape (b,3,4)\n",
    "    where b is in the batch dimension.\"\"\"\n",
    "    b=u.shape[0]\n",
    "    last_row = torch.tensor([0,0,0,1],device=u.device, dtype=u.dtype).view((1,1,4))\n",
    "    last_row = torch.repeat_interleave(last_row,b,dim=0)\n",
    "    u2 = torch.cat([u,last_row], dim=1)\n",
    "    v2 = torch.cat([v,last_row], dim=1)\n",
    "    return torch.matmul(u2, v2)[:,:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2648207",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate = torch.eye(4)\n",
    "translate[:3,3] = torch.tensor(spatial_size)/2\n",
    "translate_inv = torch.linalg.inv(translate)\n",
    "translate = translate.to(device)\n",
    "translate_inv = translate_inv.to(device)\n",
    "def center_transform(transform):\n",
    "    dv = transform.device\n",
    "    return torch.matmul(translate_inv.to(dv),torch.matmul(transform,translate.to(dv)))\n",
    "def uncenter_transform(transform):\n",
    "    dv = transform.device\n",
    "    return torch.matmul(translate.to(dv),torch.matmul(transform,translate_inv.to(dv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4226bfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEMP\n",
    "import util\n",
    "dl = monai.data.DataLoader(ds_train, shuffle=True, batch_size=1, drop_last=True)\n",
    "it = iter(dl)\n",
    "d1 = next(it)[fa_key].cpu()\n",
    "d2 = next(it)[fa_key].cpu()\n",
    "affine_transform = monai.networks.layers.AffineTransform(spatial_size)\n",
    "t = center_transform(torch.linalg.inv( d1.meta['affine'].float() ))\n",
    "util.preview_image(\n",
    "    affine_transform(d1,uncenter_transform(t))[0,0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674e8f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelOutput = namedtuple(\"ModelOutput\", \"affine,warped_moving,sim_loss,regularization_loss,supervised_loss,all_loss,true_theta\")\n",
    "\n",
    "class AffineRegModel(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 compute_sim_loss,\n",
    "                 down_convolutions,\n",
    "                 depth,\n",
    "                 max_channels,\n",
    "                 init_channels,\n",
    "                 spatial_size,\n",
    "                 lambda_reg = 1.,\n",
    "                 cnn_dropout=0.1,\n",
    "                 fc_dropout=0.1,\n",
    "                 fc_hidden_layers = None\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Create affine registration model\n",
    "        \n",
    "        Args:\n",
    "            compute_sim_loss: A function that compares two batches of images and returns a similarity loss\n",
    "            down_convolutions: How many stride=2 convolutions to include in the down-convolution part of the unets\n",
    "                               when at the original image scale. We assume the original image size is divisible by\n",
    "                               2**down_convolutions\n",
    "            depth: Total number of layers in half of the unet. Increase this to increase model capacity.\n",
    "                   Must be >= down_convolutions\n",
    "            max_channels: As you go to deeper layers, channels grow by powers of two... up to a maximum given here.\n",
    "            init_channels: how many channels in the first layer\n",
    "            spatial_size: The spatial size of the input images as a 3-tuple.\n",
    "            cnn_dropout:\n",
    "            fc_dropout:\n",
    "            fc_hidden_layers: List of hidden layer sizes for the fully connected network at the end. By default\n",
    "                              it's an empty list, which means the fully connected network simply goes from\n",
    "                              the flattened CNN output to the entries of an affine matrix.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.compute_sim_loss = compute_sim_loss\n",
    "        self.lambda_reg = lambda_reg\n",
    "        \n",
    "        self.reg_net_architecture_info = []\n",
    "        if depth < down_convolutions:\n",
    "            raise ValueError(\"Must have depth >= down_convolutions\")\n",
    "            \n",
    "        self.spatial_size = spatial_size\n",
    "        cnn_spatial_size_factor = 2**down_convolutions\n",
    "            \n",
    "        # (We will assume that the original image size is divisible by 2**n.)\n",
    "        for i,d in enumerate(spatial_size):\n",
    "            if d%cnn_spatial_size_factor != 0:\n",
    "                raise ValueError(f\"Since down_convolutions={down_convolutions} spatial dimension must be divisible by {cnn_spatial_size_factor}, but got size {d} in spatial dimension {i}.\")\n",
    "        \n",
    "        self.cnn_output_spatial_size = [s // 2**down_convolutions for s in spatial_size]\n",
    "        \n",
    "        num_twos = down_convolutions # The number of 2's we will put in the sequence of convolutional strides.\n",
    "        num_ones = depth-down_convolutions # The number of 1's\n",
    "        num_one_two_pairs = min(num_ones, num_twos) # The number of 1,2 pairs to stick in the middle\n",
    "        stride_sequence = (2,)*(num_twos-num_one_two_pairs) + (1,2)*num_one_two_pairs + (1,)*(num_ones-num_one_two_pairs)\n",
    "        channel_sequence = [min(init_channels*2**c,max_channels) for c in range(num_twos+num_ones)]\n",
    "        \n",
    "        self.cnn_output_flattened_size = channel_sequence[-1]*np.prod(self.cnn_output_spatial_size)\n",
    "        self.stride_sequence = stride_sequence\n",
    "        self.channel_sequence = channel_sequence\n",
    "\n",
    "        \n",
    "        cnn_layers = []\n",
    "        for i in range(depth):\n",
    "            in_channels = channel_sequence[i-1] if i>0 else 2\n",
    "            cnn_layers.append(monai.networks.blocks.Convolution(\n",
    "                spatial_dims=3,\n",
    "                in_channels=in_channels,\n",
    "                out_channels=channel_sequence[i],\n",
    "                dropout=cnn_dropout,\n",
    "                strides=stride_sequence[i]\n",
    "            ))\n",
    "        \n",
    "        self.cnn = torch.nn.Sequential(*cnn_layers)\n",
    "        \n",
    "        fc_layers = []\n",
    "        fc_layer_sizes = fc_hidden_layers if fc_hidden_layers is not None else []\n",
    "        fc_layer_sizes.append(4*3)\n",
    "        for i in range(len(fc_layer_sizes)):\n",
    "            fc_layers.append(\n",
    "                torch.nn.Linear(\n",
    "                    self.cnn_output_flattened_size  if i==0 else fc_layer_sizes[i-1],\n",
    "                    fc_layer_sizes[i]),\n",
    "            )\n",
    "            if i!=len(fc_layer_sizes)-1:\n",
    "                fc_layers.append(torch.nn.Dropout(fc_dropout))\n",
    "                fc_layers.append(torch.nn.PReLU())\n",
    "        \n",
    "        self.fc = torch.nn.Sequential(*fc_layers)\n",
    "        \n",
    "        \n",
    "        # We interpret the output of self.fc as a difference from the identity matrix, and we want\n",
    "        # that difference to start training as zero\n",
    "        self.fc[-1].weight.data.zero_()\n",
    "        self.fc[-1].bias.data.zero_()\n",
    "        \n",
    "        # Affine matrix for identity transform with shape 1,3,4\n",
    "        self.id134 = torch.cat([torch.eye(3), torch.zeros(3).unsqueeze(1)], dim=1).unsqueeze(0)\n",
    "        \n",
    "        # Affine transformer that operates in MONAI style coordinates\n",
    "        self.affine_transform = monai.networks.layers.AffineTransform(self.spatial_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, img_A, img_B, compute_warped_B=False) -> ModelOutput:\n",
    "        \"\"\"\n",
    "        img_A: target image\n",
    "        img_B: moving image\n",
    "        \"\"\"\n",
    "        \n",
    "        cnn_output = self.cnn(torch.cat([img_A,img_B], dim=1))\n",
    "        cnn_output_flattened = cnn_output.view(-1, self.cnn_output_flattened_size)\n",
    "        theta_minus_id = self.fc(cnn_output_flattened).view(-1, 3, 4)\n",
    "        \n",
    "        # This sum conveniently broadcasts over the batch dimension\n",
    "        id134 = self.id134.to(theta_minus_id.device)\n",
    "        theta = theta_minus_id + id134\n",
    "        \n",
    "        # apply transform with torch coordinates interpretation\n",
    "#         grid = torch.nn.functional.affine_grid(theta, img_B.size(), align_corners=False)\n",
    "#         warped_B = torch.nn.functional.grid_sample(img_B, grid, align_corners=False)\n",
    "\n",
    "        # apply transform with MONAI coordinates interpretation\n",
    "        if compute_warped_B:\n",
    "            last_row = torch.tensor([0,0,0,1],device=theta.device, dtype=theta.dtype).view((1,1,4))\n",
    "            last_row = torch.repeat_interleave(last_row,theta.shape[0],dim=0)\n",
    "            theta_uncentered = torch.cat([theta,last_row],dim=1)\n",
    "            theta_uncentered = uncenter_transform(theta_uncentered)\n",
    "            warped_B = self.affine_transform(img_B,theta_uncentered)\n",
    "        else:\n",
    "            warped_B=None\n",
    "        \n",
    "        # compute image similarity\n",
    "#         sim_loss = self.compute_sim_loss(img_A, warped_B)\n",
    "    \n",
    "        # get the ground truth correct transform, MONAI coordinates\n",
    "        with torch.no_grad():\n",
    "            Ta = img_A.meta['affine'].float()\n",
    "            Tb = img_B.meta['affine'].float()\n",
    "            true_theta = center_transform(torch.linalg.solve(Tb, Ta))[:,:3,:]\n",
    "        \n",
    "        # supervision\n",
    "        supervised_loss = ((theta - true_theta.to(theta.device))**2).mean()\n",
    "        \n",
    "        # Frobenius norm loss\n",
    "#         regularization_loss = (theta_minus_id**2).sum()\n",
    "\n",
    "        # ICon loss\n",
    "#         cnn_output_rev = self.cnn(torch.cat([img_B,img_A], dim=1))\n",
    "#         cnn_output_rev_flattened = cnn_output_rev.view(-1, self.cnn_output_flattened_size)\n",
    "#         theta_rev_minus_id = self.fc(cnn_output_rev_flattened).view(-1, 3, 4)\n",
    "#         theta_rev = theta_rev_minus_id + id134\n",
    "#         regularization_loss = ((compose_affine(theta,theta_rev) - id134)**2).mean()\n",
    "#         regularization_loss += ((compose_affine(theta_rev,theta) - id134)**2).mean()\n",
    "\n",
    "        # since we computed theta_rev we might as well include a comparison of that to ground truth\n",
    "#         with torch.no_grad():\n",
    "#             true_theta_rev = torch.linalg.solve(Ta, Tb)[:,:3,:]\n",
    "#         sim_loss += ((theta_rev - true_theta_rev.to(theta.device))**2).mean()\n",
    "        \n",
    "        regularization_loss = torch.tensor(0)\n",
    "        sim_loss = torch.tensor(0)\n",
    "        \n",
    "        return ModelOutput(\n",
    "            affine = theta,\n",
    "            warped_moving = warped_B,\n",
    "            sim_loss = sim_loss,\n",
    "            regularization_loss = regularization_loss,\n",
    "            supervised_loss = supervised_loss,\n",
    "#             all_loss = sim_loss + self.lambda_reg*regularization_loss,\n",
    "            all_loss = supervised_loss,\n",
    "            true_theta= true_theta,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7fdfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not necessarily needed and not sufficiently tested\n",
    "\n",
    "# Conversions between torch and monai coords\n",
    "S1,S2,S3 = spatial_size\n",
    "M_to_T = torch.tensor([[0, 0, 2/(S3-1), 0], [0,2/(S2-1),0,0], [2/(S1-1),0,0,0], [0,0,0,1]], dtype=torch.float32).to(device)\n",
    "T_to_M = torch.linalg.inv(M_to_T)\n",
    "\n",
    "def get_correct_transform_torch_coords(img_A, img_B):\n",
    "    \n",
    "    Ta = img_A.meta['affine'].float()\n",
    "    Tb = img_B.meta['affine'].float()\n",
    "    \n",
    "    # Correct transform in monai coords: Tb * Ta^{-1}\n",
    "    T = torch.linalg.solve(Tb, Ta)\n",
    "    \n",
    "    # Convert to torch coords\n",
    "    dv = T.device\n",
    "    T_torch = torch.matmul(M_to_T.to(dv), torch.matmul(T, T_to_M.to(dv)))\n",
    "    \n",
    "    return T_torch[:,:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfaa119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossCurves:\n",
    "    def __init__(self, name : str, spatial_dims : tuple = None):\n",
    "        self.name = name\n",
    "        \n",
    "        self.epochs =[]\n",
    "        self.all_losses = []\n",
    "        self.sim_losses = []\n",
    "        self.regularization_losses = []\n",
    "        self.supervised_losses = []\n",
    "        \n",
    "        self.clear_buffers()\n",
    "        \n",
    "    def clear_buffers(self):\n",
    "        self.all_losses_buffer = []\n",
    "        self.sim_losses_buffer = []\n",
    "        self.supervised_losses_buffer = []\n",
    "        self.regularization_losses_buffer = []\n",
    "        \n",
    "    def add_to_buffer(self, model_output : ModelOutput):\n",
    "        self.all_losses_buffer.append(model_output.all_loss.item())\n",
    "        self.sim_losses_buffer.append(model_output.sim_loss.item())\n",
    "        self.supervised_losses_buffer.append(model_output.supervised_loss.item())\n",
    "        self.regularization_losses_buffer.append(model_output.regularization_loss.item())\n",
    "\n",
    "    def aggregate_buffers_for_epoch(self, epoch : int):\n",
    "        self.epochs.append(epoch)\n",
    "        self.all_losses.append(np.mean(self.all_losses_buffer))\n",
    "        self.sim_losses.append(np.mean(self.sim_losses_buffer))\n",
    "        self.regularization_losses.append(np.mean(self.regularization_losses_buffer))\n",
    "        self.supervised_losses.append(np.mean(self.supervised_losses_buffer))\n",
    "        self.clear_buffers()\n",
    "        \n",
    "    def plot(self, savepath=None):\n",
    "        fig, axs = plt.subplots(1,4,figsize = (15,5))\n",
    "        axs[0].plot(self.epochs, self.all_losses)\n",
    "        axs[0].set_title(f\"{self.name}: overall loss\")\n",
    "        axs[1].plot(self.epochs, self.sim_losses)\n",
    "        axs[1].set_title(f\"{self.name}: similarity loss\")\n",
    "        axs[2].plot(self.epochs, self.regularization_losses, label=\"regularization loss\")\n",
    "        axs[2].set_title(f\"{self.name}: regularization loss\")\n",
    "        axs[3].plot(self.epochs, self.supervised_losses, label=\"supervised loss\")\n",
    "        axs[3].set_title(f\"{self.name}: supervised loss\")\n",
    "        for ax in axs:\n",
    "            ax.set_xlabel(\"epoch\")\n",
    "        if savepath is not None:\n",
    "            plt.savefig(savepath)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217b30b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lncc_loss = monai.losses.LocalNormalizedCrossCorrelationLoss(\n",
    "    spatial_dims=3,\n",
    "    kernel_size=5,\n",
    "    smooth_nr = 0,\n",
    "    smooth_dr = 1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732bfeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = monai.data.DataLoader(ds_train, shuffle=True, batch_size=1, drop_last=True)\n",
    "dl_valid = monai.data.DataLoader(ds_valid, shuffle=True, batch_size=2, drop_last=True)\n",
    "max_epochs = 400\n",
    "validate_when = lambda e : ((e%5==0) and (e!=0)) or (e==max_epochs-1)\n",
    "\n",
    "model = AffineRegModel(\n",
    "    compute_sim_loss = mse_loss,\n",
    "    down_convolutions=4,\n",
    "    depth=8,\n",
    "    max_channels=128,\n",
    "    init_channels=8,\n",
    "    spatial_size=spatial_size,\n",
    "    lambda_reg=10,\n",
    "    fc_hidden_layers=[512]\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "schedule_lr = True\n",
    "min_lr=1e-6\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.985)\n",
    "\n",
    "loss_curves_train = LossCurves(\"training\")\n",
    "loss_curves_valid = LossCurves(\"validation\", spatial_dims=spatial_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e309a14",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "\n",
    "for e in range(max_epochs):\n",
    "#     current_lr = scheduler.get_last_lr()[0]\n",
    "    current_lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    print(f'Epoch {e+1}/{max_epochs} (LR = {current_lr:.1e}):')\n",
    "    \n",
    "    # Train\n",
    "    model.train()\n",
    "    dl_train_iter = iter(dl_train)\n",
    "    while True:\n",
    "        try:\n",
    "            b1 = next(dl_train_iter)\n",
    "            b2 = next(dl_train_iter)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        model_output = model(b1['fa'], b2['fa'])\n",
    "        model_output.all_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_curves_train.add_to_buffer(model_output)\n",
    "        del(model_output)\n",
    "\n",
    "    if schedule_lr:\n",
    "        if scheduler.get_last_lr()[0] > min_lr:\n",
    "            scheduler.step()\n",
    "\n",
    "    loss_curves_train.aggregate_buffers_for_epoch(e)\n",
    "    print(f\"\\tTraining loss: {loss_curves_train.all_losses[-1]:.4f} (sup={loss_curves_train.supervised_losses[-1]:.4f}, sim={loss_curves_train.sim_losses[-1]:.4f}, reg={loss_curves_train.regularization_losses[-1]:.4f})\")\n",
    "    \n",
    "    # Validate\n",
    "    if validate_when(e):\n",
    "        model.eval()\n",
    "        dl_valid_iter = iter(dl_valid)\n",
    "        while True:\n",
    "            try:\n",
    "                b1 = next(dl_valid_iter)\n",
    "                b2 = next(dl_valid_iter)\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model_output = model(b1['fa'], b2['fa'])\n",
    "                loss_curves_valid.add_to_buffer(model_output)\n",
    "        loss_curves_valid.aggregate_buffers_for_epoch(e) \n",
    "        print(\"\\tValidation loss:\", loss_curves_valid.all_losses[-1])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd70383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE\n",
    "\n",
    "loss_curves_train.plot(savepath = footsteps.output_dir + 'loss_plot_train.png')\n",
    "loss_curves_valid.plot(savepath = footsteps.output_dir + 'loss_plot_valid.png')\n",
    "\n",
    "with open(footsteps.output_dir + 'loss_curves.p', 'wb') as f:\n",
    "    pickle.dump([loss_curves_train, loss_curves_valid],f)\n",
    "\n",
    "torch.save(model.state_dict(), footsteps.output_dir + 'model_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e32faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD\n",
    "\n",
    "experiment_name_to_load = \"affine with ICon\"\n",
    "load_dir = os.path.join('results', experiment_name_to_load)\n",
    "\n",
    "model.load_state_dict(torch.load(os.path.join(load_dir,'model_state_dict.pth')))\n",
    "\n",
    "with open(os.path.join(load_dir,'loss_curves.p'), 'rb') as f:\n",
    "    loss_curves_train, loss_curves_valid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42565b2b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import util\n",
    "\n",
    "# Choose whether to view performance on training or on validation data\n",
    "dl = monai.data.DataLoader(ds_train, shuffle=True, batch_size=1, drop_last=True)\n",
    "it = iter(dl)\n",
    "d1 = next(it)\n",
    "d2 = next(it)\n",
    "\n",
    "img_A = d1['fa']\n",
    "img_B = d2['fa']\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    model_output = model(img_A,img_B, compute_warped_B=True)\n",
    "\n",
    "img_B_warped = model_output.warped_moving\n",
    "\n",
    "preview_slices = (80,80,80)\n",
    "\n",
    "print(\"moving:\")\n",
    "util.preview_image(img_B[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"warped moving:\")\n",
    "util.preview_image(img_B_warped[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"target:\")\n",
    "util.preview_image(img_A[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"checkerboard of warped moving and target:\")\n",
    "util.preview_checkerboard(img_A[0,0].cpu(), img_B_warped[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"affine transform matrix:\")\n",
    "print(model_output.affine[0].as_tensor().cpu())\n",
    "print(\"the correct affine transform matrix:\")\n",
    "print(model_output.true_theta[0].cpu())\n",
    "true_theta = model_output.true_theta.to(img_B.device)\n",
    "last_row = torch.tensor([0,0,0,1],device=true_theta.device, dtype=true_theta.dtype).view((1,1,4))\n",
    "last_row = torch.repeat_interleave(last_row,true_theta.shape[0],dim=0)\n",
    "true_theta_uncentered = torch.cat([true_theta,last_row],dim=1)\n",
    "true_theta_uncentered = uncenter_transform(true_theta_uncentered)\n",
    "img_B_warped_correct = model.affine_transform(img_B,true_theta_uncentered)\n",
    "print(\"moving image warped by the correct affine transform\")\n",
    "util.preview_image(img_B_warped_correct[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"sim loss:\", model_output.sim_loss.item())\n",
    "print(\"regularization loss:\", model_output.regularization_loss.item())\n",
    "print(\"overall loss:\", model_output.all_loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
