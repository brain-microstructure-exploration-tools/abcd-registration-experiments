{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee94cda0",
   "metadata": {},
   "source": [
    "Affine registration of FA images. (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a72721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import monai\n",
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "import footsteps\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2129f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "spatial_size = (72,72,72)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8ecdb9",
   "metadata": {},
   "source": [
    "The input images are known to be $140\\times140\\times140$, and we will pad them out to $144$ in each dimension.\n",
    "\n",
    "We assume that each dimension in `spatial_size` is divisible by $2^{\\texttt{num}\\_\\texttt{scales}-1}$, because we will downsample by a factor of $2$ a bunch of times to produce images at different scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7490849",
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_dir = './dti_fit_images/fa'\n",
    "fa_key = 'fa'\n",
    "data = [{fa_key:path, \"filename\":os.path.basename(path)} for path in glob.glob(os.path.join(fa_dir,'*'))]\n",
    "data_train, data_valid = monai.data.utils.partition_dataset(data, ratios=(8,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f6318",
   "metadata": {},
   "source": [
    "`fa_keys` is a list mapping index to key for scale at that index: $0$ is the base resolution, $1$ is downscaled by a factor of $2$, $2$ is further downscaled by a factor of $2$, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b368d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_transforms = [\n",
    "    monai.transforms.LoadImageD(keys=fa_key),\n",
    "    monai.transforms.AddChannelD(keys=fa_key),\n",
    "    monai.transforms.ResizeD(keys=fa_key, spatial_size=spatial_size),\n",
    "    monai.transforms.ToTensorD(keys=fa_key),\n",
    "    monai.transforms.ToDeviceD(keys=fa_key, device=device),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc2c3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control the overall scale of affine transform\n",
    "a=0.1\n",
    "\n",
    "S = spatial_size[0]\n",
    "\n",
    "rand_affine_params = {\n",
    "    'prob':1.,\n",
    "    'mode': 'bilinear',\n",
    "    'padding_mode': 'zeros',\n",
    "    'spatial_size':spatial_size,\n",
    "    'cache_grid':True,\n",
    "    'rotate_range': (a*np.pi/2,)*3,\n",
    "    'shear_range': (0,)*6, # no shearing\n",
    "    'translate_range': (a*S/16,)*3,\n",
    "    'scale_range': (a*0.4,)*3, # no scaling\n",
    "}\n",
    "\n",
    "rand_affine_transform = monai.transforms.RandAffineD(keys=fa_key, **rand_affine_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed049b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_valid = monai.transforms.Compose(base_transforms + [rand_affine_transform])\n",
    "transform_train = monai.transforms.Compose(base_transforms + [rand_affine_transform])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2206482e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds_train = monai.data.CacheDataset(data_train, transform_train)\n",
    "ds_valid = monai.data.CacheDataset(data_valid, transform_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e54a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "warp = monai.networks.blocks.Warp(mode=\"bilinear\", padding_mode=\"zeros\")\n",
    "\n",
    "def mse_loss(b1, b2):\n",
    "    \"\"\"Return image similarity loss given two batches b1 and b2 of shape (batch_size, channels, H,W,D).\n",
    "    It is scaled up a bit here.\"\"\"\n",
    "    return 10000*((b1-b2)**2).mean()\n",
    "\n",
    "def ncc_loss(b1, b2):\n",
    "    \"\"\"Return the negative NCC loss given two batches b1 and b2 of shape (batch_size, channels, H,W,D).\n",
    "    It is averaged over batches and channels.\"\"\"\n",
    "    mu1 = b1.mean(dim=(2,3,4)) # means\n",
    "    mu2 = b2.mean(dim=(2,3,4))\n",
    "    alpha1 = (b1**2).mean(dim=(2,3,4)) # second moments\n",
    "    alpha2 = (b2**2).mean(dim=(2,3,4))\n",
    "    alpha12 = (b1*b2).mean(dim=(2,3,4)) # cross term\n",
    "    numerator = alpha12 - mu1*mu2\n",
    "    denominator = torch.sqrt((alpha1 - mu1**2) * (alpha2-mu2**2))\n",
    "    ncc = numerator / denominator\n",
    "    return -ncc.mean() # average over batches and channels\n",
    "\n",
    "def compose_ddf(u,v):\n",
    "    \"\"\"Compose two displacement fields, return the displacement that warps by v followed by u\"\"\"\n",
    "    return u + warp(v,u)\n",
    "\n",
    "_, H, W, D = ds_train[0][fa_key].shape\n",
    "\n",
    "# Compute discrete spatial derivatives\n",
    "def diff_and_trim(array, axis):\n",
    "    \"\"\"Take the discrete difference along a spatial axis, which should be 2,3, or 4.\n",
    "    Return a difference tensor with all spatial axes trimmed by 1.\"\"\"\n",
    "    return torch.diff(array, axis=axis)[:, :, :(H-1), :(W-1), :(D-1)]\n",
    "\n",
    "def size_of_spatial_derivative(u):\n",
    "    \"\"\"Return the squared Frobenius norm of the spatial derivative of the given displacement field.\n",
    "    To clarify, this is about the derivative of the actual displacement field map, not the deformation\n",
    "    that the displacement field map defines. The expected input shape is (batch,3,H,W,D).\n",
    "    Output shape is (batch).\"\"\"\n",
    "    dx = diff_and_trim(u, 2)\n",
    "    dy = diff_and_trim(u, 3)\n",
    "    dz = diff_and_trim(u, 4)\n",
    "    return(dx**2 + dy**2 + dz**2).sum(axis=1).mean(axis=[1,2,3])\n",
    "\n",
    "def compose_affine_34(u,v):\n",
    "    \"\"\"Return the product u.v of two affine transforms given as tensors of shape (b,3,4)\n",
    "    where b is in the batch dimension.\"\"\"\n",
    "    b=u.shape[0]\n",
    "    last_row = torch.tensor([0,0,0,1],device=u.device, dtype=u.dtype).view((1,1,4))\n",
    "    last_row = torch.repeat_interleave(last_row,b,dim=0)\n",
    "    u2 = torch.cat([u,last_row], dim=1)\n",
    "    v2 = torch.cat([v,last_row], dim=1)\n",
    "    return torch.matmul(u2, v2)[:,:3,:]\n",
    "\n",
    "def compose_affine_44(u,v):\n",
    "    \"\"\"Return the product u.v of two affine transforms given as tensors of shape (b,4,4)\n",
    "    where b is in the batch dimension.\"\"\"\n",
    "    return torch.matmul(u,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75d4b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate = torch.eye(4)\n",
    "translate[:3,3] = torch.tensor(spatial_size)/2\n",
    "translate_inv = torch.linalg.inv(translate)\n",
    "translate = translate.to(device)\n",
    "translate_inv = translate_inv.to(device)\n",
    "def center_transform(transform):\n",
    "    dv = transform.device\n",
    "    return torch.matmul(translate_inv.to(dv),torch.matmul(transform,translate.to(dv)))\n",
    "def uncenter_transform(transform):\n",
    "    dv = transform.device\n",
    "    return torch.matmul(translate.to(dv),torch.matmul(transform,translate_inv.to(dv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674e8f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelOutput = namedtuple(\"ModelOutput\", \"affine,warped_moving,sim_loss,regularization_loss,supervised_loss,all_loss,true_theta\")\n",
    "\n",
    "class AffineRegModel(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 compute_sim_loss,\n",
    "                 down_convolutions,\n",
    "                 depth,\n",
    "                 max_channels,\n",
    "                 init_channels,\n",
    "                 spatial_size,\n",
    "                 lambda_reg = 1.,\n",
    "                 cnn_dropout=0.1,\n",
    "                 fc_dropout=0.1,\n",
    "                 fc_hidden_layers = None\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Create affine registration model\n",
    "        \n",
    "        Args:\n",
    "            compute_sim_loss: A function that compares two batches of images and returns a similarity loss\n",
    "            down_convolutions: How many stride=2 convolutions to include in the down-convolution part of the unets\n",
    "                               when at the original image scale. We assume the original image size is divisible by\n",
    "                               2**down_convolutions\n",
    "            depth: Total number of layers in half of the unet. Increase this to increase model capacity.\n",
    "                   Must be >= down_convolutions\n",
    "            max_channels: As you go to deeper layers, channels grow by powers of two... up to a maximum given here.\n",
    "            init_channels: how many channels in the first layer\n",
    "            spatial_size: The spatial size of the input images as a 3-tuple.\n",
    "            cnn_dropout:\n",
    "            fc_dropout:\n",
    "            fc_hidden_layers: List of hidden layer sizes for the fully connected network at the end. By default\n",
    "                              it's an empty list, which means the fully connected network simply goes from\n",
    "                              the flattened CNN output to the entries of an affine matrix.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.compute_sim_loss = compute_sim_loss\n",
    "        self.lambda_reg = lambda_reg\n",
    "        \n",
    "        self.reg_net_architecture_info = []\n",
    "        if depth < down_convolutions:\n",
    "            raise ValueError(\"Must have depth >= down_convolutions\")\n",
    "            \n",
    "        self.spatial_size = spatial_size\n",
    "        cnn_spatial_size_factor = 2**down_convolutions\n",
    "            \n",
    "        # (We will assume that the original image size is divisible by 2**n.)\n",
    "        for i,d in enumerate(spatial_size):\n",
    "            if d%cnn_spatial_size_factor != 0:\n",
    "                raise ValueError(f\"Since down_convolutions={down_convolutions} spatial dimension must be divisible by {cnn_spatial_size_factor}, but got size {d} in spatial dimension {i}.\")\n",
    "        \n",
    "        self.cnn_output_spatial_size = [s // 2**down_convolutions for s in spatial_size]\n",
    "        \n",
    "        num_twos = down_convolutions # The number of 2's we will put in the sequence of convolutional strides.\n",
    "        num_ones = depth-down_convolutions # The number of 1's\n",
    "        num_one_two_pairs = min(num_ones, num_twos) # The number of 1,2 pairs to stick in the middle\n",
    "        stride_sequence = (2,)*(num_twos-num_one_two_pairs) + (1,2)*num_one_two_pairs + (1,)*(num_ones-num_one_two_pairs)\n",
    "        channel_sequence = [min(init_channels*2**c,max_channels) for c in range(num_twos+num_ones)]\n",
    "        \n",
    "        self.cnn_output_flattened_size = channel_sequence[-1]*np.prod(self.cnn_output_spatial_size)\n",
    "        self.stride_sequence = stride_sequence\n",
    "        self.channel_sequence = channel_sequence\n",
    "\n",
    "        \n",
    "        cnn_layers = []\n",
    "        for i in range(depth):\n",
    "            in_channels = channel_sequence[i-1] if i>0 else 2\n",
    "            cnn_layers.append(monai.networks.blocks.Convolution(\n",
    "                spatial_dims=3,\n",
    "                in_channels=in_channels,\n",
    "                out_channels=channel_sequence[i],\n",
    "                dropout=cnn_dropout,\n",
    "                strides=stride_sequence[i]\n",
    "            ))\n",
    "        \n",
    "        self.cnn = torch.nn.Sequential(*cnn_layers)\n",
    "        \n",
    "        fc_layers = []\n",
    "        fc_layer_sizes = fc_hidden_layers if fc_hidden_layers is not None else []\n",
    "        fc_layer_sizes.append(4*3)\n",
    "        for i in range(len(fc_layer_sizes)):\n",
    "            fc_layers.append(\n",
    "                torch.nn.Linear(\n",
    "                    self.cnn_output_flattened_size  if i==0 else fc_layer_sizes[i-1],\n",
    "                    fc_layer_sizes[i]),\n",
    "            )\n",
    "            if i!=len(fc_layer_sizes)-1:\n",
    "                fc_layers.append(torch.nn.Dropout(fc_dropout))\n",
    "                fc_layers.append(torch.nn.PReLU())\n",
    "        \n",
    "        self.fc = torch.nn.Sequential(*fc_layers)\n",
    "        \n",
    "        \n",
    "        # We interpret the output of self.fc as a difference from the identity matrix, and we want\n",
    "        # that difference to start training as zero\n",
    "        self.fc[-1].weight.data.zero_()\n",
    "        self.fc[-1].bias.data.zero_()\n",
    "        \n",
    "        # Affine matrix for identity transform with shape 1,3,4\n",
    "        self.id134 = torch.cat([torch.eye(3), torch.zeros(3).unsqueeze(1)], dim=1).unsqueeze(0)\n",
    "        \n",
    "        # Affine transformer that operates in MONAI style coordinates\n",
    "        self.affine_transform = monai.networks.layers.AffineTransform(self.spatial_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, img_A, img_B, compute_warped_B=False) -> ModelOutput:\n",
    "        \"\"\"\n",
    "        img_A: target image\n",
    "        img_B: moving image\n",
    "        \"\"\"\n",
    "        \n",
    "        cnn_output = self.cnn(torch.cat([img_A,img_B], dim=1))\n",
    "        cnn_output_flattened = cnn_output.view(-1, self.cnn_output_flattened_size)\n",
    "        theta_minus_id = self.fc(cnn_output_flattened).view(-1, 3, 4)\n",
    "        \n",
    "        # This sum conveniently broadcasts over the batch dimension\n",
    "        id134 = self.id134.to(theta_minus_id.device)\n",
    "        theta = theta_minus_id + id134\n",
    "        \n",
    "        # apply transform with torch coordinates interpretation\n",
    "#         grid = torch.nn.functional.affine_grid(theta, img_B.size(), align_corners=False)\n",
    "#         warped_B = torch.nn.functional.grid_sample(img_B, grid, align_corners=False)\n",
    "\n",
    "        # apply transform with MONAI coordinates interpretation\n",
    "        if compute_warped_B:\n",
    "            last_row = torch.tensor([0,0,0,1],device=theta.device, dtype=theta.dtype).view((1,1,4))\n",
    "            last_row = torch.repeat_interleave(last_row,theta.shape[0],dim=0)\n",
    "            theta_uncentered = torch.cat([theta,last_row],dim=1)\n",
    "            theta_uncentered = uncenter_transform(theta_uncentered)\n",
    "            warped_B = self.affine_transform(img_B,theta_uncentered)\n",
    "        else:\n",
    "            warped_B=None\n",
    "        \n",
    "        # compute image similarity\n",
    "#         sim_loss = self.compute_sim_loss(img_A, warped_B)\n",
    "    \n",
    "        # get the ground truth correct transform, centered MONAI coordinates\n",
    "        with torch.no_grad():\n",
    "            Ta = img_A.meta['affine'].float()\n",
    "            Tb = img_B.meta['affine'].float()\n",
    "            true_theta = center_transform(torch.linalg.solve(Tb, Ta))[:,:3,:]\n",
    "        \n",
    "        # supervision\n",
    "        supervised_loss = ((theta - true_theta.to(theta.device))**2).mean()\n",
    "        \n",
    "        # Frobenius norm loss\n",
    "#         regularization_loss = (theta_minus_id**2).sum()\n",
    "\n",
    "        # ICon loss\n",
    "#         cnn_output_rev = self.cnn(torch.cat([img_B,img_A], dim=1))\n",
    "#         cnn_output_rev_flattened = cnn_output_rev.view(-1, self.cnn_output_flattened_size)\n",
    "#         theta_rev_minus_id = self.fc(cnn_output_rev_flattened).view(-1, 3, 4)\n",
    "#         theta_rev = theta_rev_minus_id + id134\n",
    "#         regularization_loss = ((compose_affine_34(theta,theta_rev) - id134)**2).mean()\n",
    "#         regularization_loss += ((compose_affine_34(theta_rev,theta) - id134)**2).mean()\n",
    "\n",
    "        # since we computed theta_rev we might as well include a comparison of that to ground truth\n",
    "#         with torch.no_grad():\n",
    "#             true_theta_rev = torch.linalg.solve(Ta, Tb)[:,:3,:]\n",
    "#         sim_loss += ((theta_rev - true_theta_rev.to(theta.device))**2).mean()\n",
    "        \n",
    "        regularization_loss = torch.tensor(0)\n",
    "        sim_loss = torch.tensor(0)\n",
    "        \n",
    "        return ModelOutput(\n",
    "            affine = theta,\n",
    "            warped_moving = warped_B,\n",
    "            sim_loss = sim_loss,\n",
    "            regularization_loss = regularization_loss,\n",
    "            supervised_loss = supervised_loss,\n",
    "#             all_loss = sim_loss + self.lambda_reg*regularization_loss,\n",
    "            all_loss = supervised_loss,\n",
    "            true_theta= true_theta,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae1db5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multistep version of the above\n",
    "\n",
    "ModelOutput = namedtuple(\"ModelOutput\", \"affine,warped_moving,sim_loss,regularization_loss,supervised_loss,all_loss,true_theta\")\n",
    "\n",
    "class MultistepAffineRegModel(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 compute_sim_loss,\n",
    "                 down_convolutions,\n",
    "                 depth,\n",
    "                 max_channels,\n",
    "                 init_channels,\n",
    "                 spatial_size,\n",
    "                 lambda_reg = 1.,\n",
    "                 cnn_dropout=0.1,\n",
    "                 fc_dropout=0.1,\n",
    "                 fc_hidden_layers = None,\n",
    "                 num_steps=1,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Create affine registration model\n",
    "        \n",
    "        Args:\n",
    "            compute_sim_loss: A function that compares two batches of images and returns a similarity loss\n",
    "            down_convolutions: How many stride=2 convolutions to include in the down-convolution part of the unets\n",
    "                               when at the original image scale. We assume the original image size is divisible by\n",
    "                               2**down_convolutions\n",
    "            depth: Total number of layers in half of the unet. Increase this to increase model capacity.\n",
    "                   Must be >= down_convolutions\n",
    "            max_channels: As you go to deeper layers, channels grow by powers of two... up to a maximum given here.\n",
    "            init_channels: how many channels in the first layer\n",
    "            spatial_size: The spatial size of the input images as a 3-tuple.\n",
    "            cnn_dropout:\n",
    "            fc_dropout:\n",
    "            fc_hidden_layers: List of hidden layer sizes for the fully connected network at the end. By default\n",
    "                              it's an empty list, which means the fully connected network simply goes from\n",
    "                              the flattened CNN output to the entries of an affine matrix.\n",
    "            num_steps: Number of steps in the iterative multi step feedback approach\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.compute_sim_loss = compute_sim_loss\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.num_steps = num_steps\n",
    "        \n",
    "        self.reg_net_architecture_info = []\n",
    "        if depth < down_convolutions:\n",
    "            raise ValueError(\"Must have depth >= down_convolutions\")\n",
    "            \n",
    "        self.spatial_size = spatial_size\n",
    "        cnn_spatial_size_factor = 2**down_convolutions\n",
    "            \n",
    "        # (We will assume that the original image size is divisible by 2**n.)\n",
    "        for i,d in enumerate(spatial_size):\n",
    "            if d%cnn_spatial_size_factor != 0:\n",
    "                raise ValueError(f\"Since down_convolutions={down_convolutions} spatial dimension must be divisible by {cnn_spatial_size_factor}, but got size {d} in spatial dimension {i}.\")\n",
    "        \n",
    "        self.cnn_output_spatial_size = [s // 2**down_convolutions for s in spatial_size]\n",
    "        \n",
    "        num_twos = down_convolutions # The number of 2's we will put in the sequence of convolutional strides.\n",
    "        num_ones = depth-down_convolutions # The number of 1's\n",
    "        num_one_two_pairs = min(num_ones, num_twos) # The number of 1,2 pairs to stick in the middle\n",
    "        stride_sequence = (2,)*(num_twos-num_one_two_pairs) + (1,2)*num_one_two_pairs + (1,)*(num_ones-num_one_two_pairs)\n",
    "        channel_sequence = [min(init_channels*2**c,max_channels) for c in range(num_twos+num_ones)]\n",
    "        \n",
    "        self.cnn_output_flattened_size = channel_sequence[-1]*np.prod(self.cnn_output_spatial_size)\n",
    "        self.stride_sequence = stride_sequence\n",
    "        self.channel_sequence = channel_sequence\n",
    "\n",
    "        \n",
    "        cnn_layers = []\n",
    "        for i in range(depth):\n",
    "            in_channels = channel_sequence[i-1] if i>0 else 2\n",
    "            cnn_layers.append(monai.networks.blocks.Convolution(\n",
    "                spatial_dims=3,\n",
    "                in_channels=in_channels,\n",
    "                out_channels=channel_sequence[i],\n",
    "                dropout=cnn_dropout,\n",
    "                strides=stride_sequence[i]\n",
    "            ))\n",
    "        \n",
    "        self.cnn = torch.nn.Sequential(*cnn_layers)\n",
    "        \n",
    "        fc_layers = []\n",
    "        fc_layer_sizes = fc_hidden_layers if fc_hidden_layers is not None else []\n",
    "        fc_layer_sizes.append(4*3)\n",
    "        for i in range(len(fc_layer_sizes)):\n",
    "            fc_layers.append(\n",
    "                torch.nn.Linear(\n",
    "                    self.cnn_output_flattened_size  if i==0 else fc_layer_sizes[i-1],\n",
    "                    fc_layer_sizes[i]),\n",
    "            )\n",
    "            if i!=len(fc_layer_sizes)-1:\n",
    "                fc_layers.append(torch.nn.Dropout(fc_dropout))\n",
    "                fc_layers.append(torch.nn.PReLU())\n",
    "        \n",
    "        self.fc = torch.nn.Sequential(*fc_layers)\n",
    "        \n",
    "        \n",
    "        # We interpret the output of self.fc as a difference from the identity matrix, and we want\n",
    "        # that difference to start training as zero\n",
    "        self.fc[-1].weight.data.zero_()\n",
    "        self.fc[-1].bias.data.zero_()\n",
    "        \n",
    "        # Affine matrix for identity transform with shape 1,3,4\n",
    "        self.id134 = torch.cat([torch.eye(3), torch.zeros(3).unsqueeze(1)], dim=1).unsqueeze(0)\n",
    "        \n",
    "        # Affine transformer that operates in MONAI style coordinates\n",
    "        self.affine_transform = monai.networks.layers.AffineTransform(self.spatial_size)\n",
    "        \n",
    "    def apply_warp(self, theta, img_B):\n",
    "        \"\"\"Interpreting the 3x4 affine matrix theta in an *image-centered* version of MONAI coordinates,\n",
    "        warp img_B and return the warped image.\"\"\"\n",
    "        last_row = torch.tensor([0,0,0,1],device=theta.device, dtype=theta.dtype).view((1,1,4))\n",
    "        last_row = torch.repeat_interleave(last_row,theta.shape[0],dim=0)\n",
    "        theta_uncentered = torch.cat([theta,last_row],dim=1)\n",
    "        theta_uncentered = uncenter_transform(theta_uncentered)\n",
    "        warped_B = self.affine_transform(img_B,theta_uncentered)\n",
    "        return warped_B\n",
    "    \n",
    "    def forward_one_step(self, img_A, img_B):\n",
    "        \"\"\"\n",
    "        img_A: target image\n",
    "        img_B: moving image\n",
    "        \"\"\"\n",
    "        \n",
    "        cnn_output = self.cnn(torch.cat([img_A,img_B], dim=1))\n",
    "        cnn_output_flattened = cnn_output.view(-1, self.cnn_output_flattened_size)\n",
    "        theta_minus_id = self.fc(cnn_output_flattened).view(-1, 3, 4)\n",
    "        \n",
    "        # This sum conveniently broadcasts over the batch dimension\n",
    "        id134 = self.id134.to(theta_minus_id.device)\n",
    "        theta = theta_minus_id + id134\n",
    "        \n",
    "        return theta\n",
    "    \n",
    "    def forward(self, img_A, img_B, num_steps=None) -> ModelOutput:\n",
    "        \"\"\"\n",
    "        img_A: target image\n",
    "        img_B: moving image\n",
    "        num_steps: Keep this as None during training so that we know what we are training with.\n",
    "                   It's provided so that during evaluation we can do more steps.\n",
    "        \"\"\"\n",
    "        if num_steps is None:\n",
    "            num_steps = self.num_steps\n",
    "        \n",
    "        for i in range(num_steps):\n",
    "            warped_B = img_B if i==0 else self.apply_warp(theta_composite, img_B)\n",
    "            theta = self.forward_one_step(img_A, warped_B)\n",
    "            theta_composite = theta if i==0 else compose_affine_34(theta, theta_composite)\n",
    "        warped_B = self.apply_warp(theta_composite, img_B)\n",
    "        sim_loss = self.compute_sim_loss(img_A, warped_B)\n",
    "        \n",
    "        # get the ground truth correct transform, centered version of MONAI coordinates\n",
    "        with torch.no_grad():\n",
    "            Ta = img_A.meta['affine'].float()\n",
    "            Tb = img_B.meta['affine'].float()\n",
    "            true_theta = center_transform(torch.linalg.solve(Tb, Ta))[:,:3,:]\n",
    "        \n",
    "        \n",
    "        return ModelOutput(\n",
    "            affine = theta,\n",
    "            warped_moving = warped_B,\n",
    "            sim_loss = sim_loss,\n",
    "            regularization_loss = torch.tensor(0),\n",
    "            supervised_loss = torch.tensor(0),\n",
    "            all_loss = sim_loss,\n",
    "            true_theta= true_theta,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9ccef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential version of the above\n",
    "\n",
    "ModelOutput = namedtuple(\"ModelOutput\", \"affine,warped_moving,sim_loss,regularization_loss,supervised_loss,all_loss,true_theta\")\n",
    "\n",
    "class ExponentialAffineRegModel(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 compute_sim_loss,\n",
    "                 down_convolutions,\n",
    "                 depth,\n",
    "                 max_channels,\n",
    "                 init_channels,\n",
    "                 spatial_size,\n",
    "                 lambda_reg = 1.,\n",
    "                 cnn_dropout=0.1,\n",
    "                 fc_dropout=0.1,\n",
    "                 fc_hidden_layers = None,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Create affine registration model\n",
    "        \n",
    "        Args:\n",
    "            compute_sim_loss: A function that compares two batches of images and returns a similarity loss\n",
    "            down_convolutions: How many stride=2 convolutions to include in the down-convolution part of the unets\n",
    "                               when at the original image scale. We assume the original image size is divisible by\n",
    "                               2**down_convolutions\n",
    "            depth: Total number of layers in half of the unet. Increase this to increase model capacity.\n",
    "                   Must be >= down_convolutions\n",
    "            max_channels: As you go to deeper layers, channels grow by powers of two... up to a maximum given here.\n",
    "            init_channels: how many channels in the first layer\n",
    "            spatial_size: The spatial size of the input images as a 3-tuple.\n",
    "            cnn_dropout:\n",
    "            fc_dropout:\n",
    "            fc_hidden_layers: List of hidden layer sizes for the fully connected network at the end. By default\n",
    "                              it's an empty list, which means the fully connected network simply goes from\n",
    "                              the flattened CNN output to the entries of an affine matrix.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.compute_sim_loss = compute_sim_loss\n",
    "        self.lambda_reg = lambda_reg\n",
    "        \n",
    "        self.reg_net_architecture_info = []\n",
    "        if depth < down_convolutions:\n",
    "            raise ValueError(\"Must have depth >= down_convolutions\")\n",
    "            \n",
    "        self.spatial_size = spatial_size\n",
    "        cnn_spatial_size_factor = 2**down_convolutions\n",
    "            \n",
    "        # (We will assume that the original image size is divisible by 2**n.)\n",
    "        for i,d in enumerate(spatial_size):\n",
    "            if d%cnn_spatial_size_factor != 0:\n",
    "                raise ValueError(f\"Since down_convolutions={down_convolutions} spatial dimension must be divisible by {cnn_spatial_size_factor}, but got size {d} in spatial dimension {i}.\")\n",
    "        \n",
    "        self.cnn_output_spatial_size = [s // 2**down_convolutions for s in spatial_size]\n",
    "        \n",
    "        num_twos = down_convolutions # The number of 2's we will put in the sequence of convolutional strides.\n",
    "        num_ones = depth-down_convolutions # The number of 1's\n",
    "        num_one_two_pairs = min(num_ones, num_twos) # The number of 1,2 pairs to stick in the middle\n",
    "        stride_sequence = (2,)*(num_twos-num_one_two_pairs) + (1,2)*num_one_two_pairs + (1,)*(num_ones-num_one_two_pairs)\n",
    "        channel_sequence = [min(init_channels*2**c,max_channels) for c in range(num_twos+num_ones)]\n",
    "        \n",
    "        self.cnn_output_flattened_size = channel_sequence[-1]*np.prod(self.cnn_output_spatial_size)\n",
    "        self.stride_sequence = stride_sequence\n",
    "        self.channel_sequence = channel_sequence\n",
    "\n",
    "        \n",
    "        cnn_layers = []\n",
    "        for i in range(depth):\n",
    "            in_channels = channel_sequence[i-1] if i>0 else 2\n",
    "            cnn_layers.append(monai.networks.blocks.Convolution(\n",
    "                spatial_dims=3,\n",
    "                in_channels=in_channels,\n",
    "                out_channels=channel_sequence[i],\n",
    "                dropout=cnn_dropout,\n",
    "                strides=stride_sequence[i]\n",
    "            ))\n",
    "        \n",
    "        self.cnn = torch.nn.Sequential(*cnn_layers)\n",
    "        \n",
    "        fc_layers = []\n",
    "        fc_layer_sizes = fc_hidden_layers if fc_hidden_layers is not None else []\n",
    "        fc_layer_sizes.append(4*3)\n",
    "        for i in range(len(fc_layer_sizes)):\n",
    "            fc_layers.append(\n",
    "                torch.nn.Linear(\n",
    "                    self.cnn_output_flattened_size  if i==0 else fc_layer_sizes[i-1],\n",
    "                    fc_layer_sizes[i]),\n",
    "            )\n",
    "            if i!=len(fc_layer_sizes)-1:\n",
    "                fc_layers.append(torch.nn.Dropout(fc_dropout))\n",
    "                fc_layers.append(torch.nn.PReLU())\n",
    "        \n",
    "        self.fc = torch.nn.Sequential(*fc_layers)\n",
    "        \n",
    "        \n",
    "        # We interpret the output of self.fc as a difference from the identity matrix, and we want\n",
    "        # that difference to start training as zero\n",
    "        self.fc[-1].weight.data.zero_()\n",
    "        self.fc[-1].bias.data.zero_()\n",
    "        \n",
    "        # Affine matrix for identity transform with shape 1,3,4\n",
    "        self.id134 = torch.cat([torch.eye(3), torch.zeros(3).unsqueeze(1)], dim=1).unsqueeze(0)\n",
    "        \n",
    "        # Affine transformer that operates in MONAI style coordinates\n",
    "        self.affine_transform = monai.networks.layers.AffineTransform(self.spatial_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, img_A, img_B):\n",
    "        \"\"\"\n",
    "        img_A: target image\n",
    "        img_B: moving image\n",
    "        \"\"\"\n",
    "        \n",
    "        cnn_output = self.cnn(torch.cat([img_A,img_B], dim=1))\n",
    "        cnn_output_flattened = cnn_output.view(-1, self.cnn_output_flattened_size)\n",
    "        alpha = self.fc(cnn_output_flattened).view(-1, 3, 4)\n",
    "        \n",
    "        last_row = torch.tensor([0,0,0,0],device=alpha.device, dtype=alpha.dtype).view((1,1,4))\n",
    "        last_row = torch.repeat_interleave(last_row,alpha.shape[0],dim=0)\n",
    "        alpha_4_4 = torch.cat([alpha,last_row],dim=1)\n",
    "        theta = torch.matrix_exp(alpha_4_4)\n",
    "        theta_uncentered = uncenter_transform(theta)\n",
    "        warped_B = self.affine_transform(img_B,theta_uncentered)\n",
    "        \n",
    "        sim_loss = self.compute_sim_loss(img_A, warped_B)\n",
    "        \n",
    "        # get the ground truth correct transform, centered version of MONAI coordinates\n",
    "        with torch.no_grad():\n",
    "            Ta = img_A.meta['affine'].float()\n",
    "            Tb = img_B.meta['affine'].float()\n",
    "            true_theta = center_transform(torch.linalg.solve(Tb, Ta))[:,:3,:]\n",
    "        \n",
    "        \n",
    "        return ModelOutput(\n",
    "            affine = theta,\n",
    "            warped_moving = warped_B,\n",
    "            sim_loss = sim_loss,\n",
    "            regularization_loss = torch.tensor(0),\n",
    "            supervised_loss = torch.tensor(0),\n",
    "            all_loss = sim_loss,\n",
    "            true_theta= true_theta,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af44f554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_to_infinitessimal_euclidean_transform(a):\n",
    "    \"\"\"Take a batch of vectors of shape (b,6) and return the corresponding batch of\n",
    "    euclidean-group-lie-algebra elements of shape (b,4,4)\"\"\"\n",
    "    alpha = torch.zeros((a.shape[0],4,4), dtype=a.dtype,device=a.device)\n",
    "    alpha[:,0,1] = a[:,0]\n",
    "    alpha[:,0,2] = a[:,1]\n",
    "    alpha[:,1,2] = a[:,2]\n",
    "\n",
    "    alpha[:,1,0] = -a[:,0]\n",
    "    alpha[:,2,0] = -a[:,1]\n",
    "    alpha[:,2,1] = -a[:,2]\n",
    "\n",
    "    alpha[:,0,3] = a[:,3]\n",
    "    alpha[:,1,3] = a[:,4]\n",
    "    alpha[:,2,3] = a[:,5]\n",
    "    return alpha\n",
    "\n",
    "def vec_to_infinitessimal_similitude(a):\n",
    "    \"\"\"Take a batch of vectors of shape (b,7) and return the corresponding batch of\n",
    "    similitude-group-lie-algebra elements of shape (b,4,4)\"\"\"\n",
    "    alpha = torch.zeros((a.shape[0],4,4), dtype=a.dtype,device=a.device)\n",
    "    alpha[:,0,1] = a[:,0]\n",
    "    alpha[:,0,2] = a[:,1]\n",
    "    alpha[:,1,2] = a[:,2]\n",
    "\n",
    "    alpha[:,1,0] = -a[:,0]\n",
    "    alpha[:,2,0] = -a[:,1]\n",
    "    alpha[:,2,1] = -a[:,2]\n",
    "\n",
    "    alpha[:,0,3] = a[:,3]\n",
    "    alpha[:,1,3] = a[:,4]\n",
    "    alpha[:,2,3] = a[:,5]\n",
    "    \n",
    "    alpha[:,0,0] = a[:,6]\n",
    "    alpha[:,1,1] = a[:,6]\n",
    "    alpha[:,2,2] = a[:,6]\n",
    "    return alpha\n",
    "\n",
    "from scipy.linalg import logm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a923fb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still experimenting with this-- need to think about how to deal with complex values\n",
    "\n",
    "def batch_matrix_log(m, error_threshold=1e-3):\n",
    "    \"\"\"Given a batch of square matrices, shape (b,n,n), take the matrix log of each of them\"\"\"\n",
    "    logs = []\n",
    "    for i in range(m.shape[0]):\n",
    "        log,error = logm(m[i], disp=False)\n",
    "        if error > error_threshold:\n",
    "            print(\"WARNING: Matrix log error exceeded threshold.\", file=sys.stderr)\n",
    "        logs.append(torch.tensor(log))\n",
    "    batched_logs = torch.stack(logs, dim=0)\n",
    "    if batched_logs.is_complex():\n",
    "        if batched_logs.imag.abs().max() > error_threshold*1000:\n",
    "            print(batched_logs.imag.abs().max())\n",
    "            print(\"WARNING: Matrix log ended up with imaginary part exceeding threshold.\", file=sys.stderr)\n",
    "        batched_logs = batched_logs.real\n",
    "    return torch.tensor(batched_logs, dtype=m.dtype, device=m.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cf7c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rigid version of the above\n",
    "\n",
    "ModelOutput = namedtuple(\"ModelOutput\", \"affine,warped_moving,sim_loss,regularization_loss,supervised_loss,all_loss,true_theta\")\n",
    "from enum import Enum\n",
    "class MultistepMode(Enum):\n",
    "    GP = 0\n",
    "    ALG = 1\n",
    "\n",
    "class ExponentialRigidRegModel(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 compute_sim_loss,\n",
    "                 down_convolutions,\n",
    "                 depth,\n",
    "                 max_channels,\n",
    "                 init_channels,\n",
    "                 spatial_size,\n",
    "                 lambda_reg = 1.,\n",
    "                 cnn_dropout=0.1,\n",
    "                 fc_dropout=0.1,\n",
    "                 fc_hidden_layers = None,\n",
    "                 allow_scaling = False,\n",
    "                 supervised = False,\n",
    "                 num_steps = 1,\n",
    "                 multistep_mode = \"gp-compose\"\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Create rigid registration model, which aligns images using euclidean transformations.\n",
    "        \n",
    "        Args:\n",
    "            compute_sim_loss: A function that compares two batches of images and returns a similarity loss\n",
    "                              Not used if supervised==True.\n",
    "            down_convolutions: How many stride=2 convolutions to include in the down-convolution part of the unets\n",
    "                               when at the original image scale. We assume the original image size is divisible by\n",
    "                               2**down_convolutions\n",
    "            depth: Total number of layers in half of the unet. Increase this to increase model capacity.\n",
    "                   Must be >= down_convolutions\n",
    "            max_channels: As you go to deeper layers, channels grow by powers of two... up to a maximum given here.\n",
    "            init_channels: how many channels in the first layer\n",
    "            spatial_size: The spatial size of the input images as a 3-tuple.\n",
    "            cnn_dropout:\n",
    "            fc_dropout:\n",
    "            fc_hidden_layers: List of hidden layer sizes for the fully connected network at the end. By default\n",
    "                              it's an empty list, which means the fully connected network simply goes from\n",
    "                              the flattened CNN output to the entries of an affine matrix.\n",
    "            allow_scaling: Enable this to allow not only euclidean transformations but also direct similitudes.\n",
    "                           (Definitely enable this if your images are differently cropped or if your brains\n",
    "                           have different scale, e.g. in a developmental study)\n",
    "            supervised: Enable this to use a supervised loss instead of unsupervised image similarity loss.\n",
    "                        Depending on multistep_mode (even if num_steps==1), the supervision can be based on\n",
    "                        transforms or on lie algebra elements.\n",
    "            num_steps: Number of steps in the iterative multi step feedback approach\n",
    "            multistep_mode: If set to 'gp-compose' then at each step we compute a new affine transform\n",
    "                            and we update by composing transforms. If set to 'alg-add' then at each step\n",
    "                            we compute a new lie algebra element and we update by adding. They really are\n",
    "                            different approaches because of the BCH formula.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.compute_sim_loss = compute_sim_loss\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.supervised = supervised\n",
    "        self.num_steps = num_steps\n",
    "        \n",
    "        self.reg_net_architecture_info = []\n",
    "        if depth < down_convolutions:\n",
    "            raise ValueError(\"Must have depth >= down_convolutions\")\n",
    "            \n",
    "        self.spatial_size = spatial_size\n",
    "        cnn_spatial_size_factor = 2**down_convolutions\n",
    "            \n",
    "        # (We will assume that the original image size is divisible by 2**n.)\n",
    "        for i,d in enumerate(spatial_size):\n",
    "            if d%cnn_spatial_size_factor != 0:\n",
    "                raise ValueError(f\"Since down_convolutions={down_convolutions} spatial dimension must be divisible by {cnn_spatial_size_factor}, but got size {d} in spatial dimension {i}.\")\n",
    "        \n",
    "        self.cnn_output_spatial_size = [s // 2**down_convolutions for s in spatial_size]\n",
    "        \n",
    "        num_twos = down_convolutions # The number of 2's we will put in the sequence of convolutional strides.\n",
    "        num_ones = depth-down_convolutions # The number of 1's\n",
    "        num_one_two_pairs = min(num_ones, num_twos) # The number of 1,2 pairs to stick in the middle\n",
    "        stride_sequence = (2,)*(num_twos-num_one_two_pairs) + (1,2)*num_one_two_pairs + (1,)*(num_ones-num_one_two_pairs)\n",
    "        channel_sequence = [min(init_channels*2**c,max_channels) for c in range(num_twos+num_ones)]\n",
    "        \n",
    "        self.cnn_output_flattened_size = channel_sequence[-1]*np.prod(self.cnn_output_spatial_size)\n",
    "        self.stride_sequence = stride_sequence\n",
    "        self.channel_sequence = channel_sequence\n",
    "\n",
    "        \n",
    "        cnn_layers = []\n",
    "        for i in range(depth):\n",
    "            in_channels = channel_sequence[i-1] if i>0 else 2\n",
    "            cnn_layers.append(monai.networks.blocks.Convolution(\n",
    "                spatial_dims=3,\n",
    "                in_channels=in_channels,\n",
    "                out_channels=channel_sequence[i],\n",
    "                dropout=cnn_dropout,\n",
    "                strides=stride_sequence[i]\n",
    "            ))\n",
    "        \n",
    "        self.cnn = torch.nn.Sequential(*cnn_layers)\n",
    "        \n",
    "        self.allow_scaling = allow_scaling\n",
    "        if allow_scaling:\n",
    "            last_layer_size = 7\n",
    "            self.vec_to_infinitessimal_generator = vec_to_infinitessimal_similitude\n",
    "        else:\n",
    "            last_layer_size = 6\n",
    "            self.vec_to_infinitessimal_generator = vec_to_infinitessimal_euclidean_transform\n",
    "        \n",
    "        fc_layers = []\n",
    "        fc_layer_sizes = fc_hidden_layers if fc_hidden_layers is not None else []\n",
    "        fc_layer_sizes.append(last_layer_size)\n",
    "        for i in range(len(fc_layer_sizes)):\n",
    "            fc_layers.append(\n",
    "                torch.nn.Linear(\n",
    "                    self.cnn_output_flattened_size  if i==0 else fc_layer_sizes[i-1],\n",
    "                    fc_layer_sizes[i]),\n",
    "            )\n",
    "            if i!=len(fc_layer_sizes)-1:\n",
    "                fc_layers.append(torch.nn.Dropout(fc_dropout))\n",
    "                fc_layers.append(torch.nn.PReLU())\n",
    "        \n",
    "        self.fc = torch.nn.Sequential(*fc_layers)\n",
    "        \n",
    "        \n",
    "        # We interpret the output of self.fc as a difference from the identity matrix, and we want\n",
    "        # that difference to start training as zero\n",
    "        self.fc[-1].weight.data.zero_()\n",
    "        self.fc[-1].bias.data.zero_()\n",
    "        \n",
    "        # Affine matrix for identity transform with shape 1,3,4\n",
    "        self.id134 = torch.cat([torch.eye(3), torch.zeros(3).unsqueeze(1)], dim=1).unsqueeze(0)\n",
    "        \n",
    "        # Affine transformer that operates in MONAI style coordinates\n",
    "        self.affine_transform = monai.networks.layers.AffineTransform(self.spatial_size)\n",
    "        \n",
    "        if multistep_mode==\"gp-compose\":\n",
    "            self.multistep_mode = MultistepMode.GP\n",
    "        elif multistep_mode==\"alg-add\":\n",
    "            self.multistep_mode = MultistepMode.ALG\n",
    "        else:\n",
    "            raise ValueError(\"Unrecognized multistep_mode.\")\n",
    "        \n",
    "    def apply_warp(self, theta, img_B):\n",
    "        \"\"\"Interpreting the 4x4 affine matrix theta in an *image-centered* version of MONAI coordinates,\n",
    "        warp img_B and return the warped image.\"\"\"\n",
    "        theta_uncentered = uncenter_transform(theta)\n",
    "        warped_B = self.affine_transform(img_B,theta_uncentered)\n",
    "        return warped_B\n",
    "    \n",
    "    def forward_one_step(self, img_A, img_B):\n",
    "        \"\"\"\n",
    "        img_A: target image\n",
    "        img_B: moving image\n",
    "        \"\"\"\n",
    "        \n",
    "        cnn_output = self.cnn(torch.cat([img_A,img_B], dim=1))\n",
    "        cnn_output_flattened = cnn_output.view(-1, self.cnn_output_flattened_size)\n",
    "        a = self.fc(cnn_output_flattened)\n",
    "        alpha = self.vec_to_infinitessimal_generator(a)\n",
    "        return alpha\n",
    "\n",
    "    def forward(self, img_A, img_B, num_steps=None) -> ModelOutput:\n",
    "        \"\"\"\n",
    "        img_A: target image\n",
    "        img_B: moving image\n",
    "        num_steps: Keep this as None during training so that we know what we are training with.\n",
    "                   It's provided so that during evaluation we can do more steps.\n",
    "        \"\"\"\n",
    "        if num_steps is None:\n",
    "            num_steps = self.num_steps\n",
    "        \n",
    "        if self.multistep_mode == MultistepMode.GP:\n",
    "            for i in range(num_steps):\n",
    "                warped_B = img_B if i==0 else self.apply_warp(theta_composite, img_B)\n",
    "                alpha = self.forward_one_step(img_A, warped_B)\n",
    "                theta = torch.matrix_exp(alpha)\n",
    "                theta_composite = theta if i==0 else compose_affine_44(theta, theta_composite)\n",
    "            theta = theta_composite\n",
    "            warped_B = self.apply_warp(theta, img_B)\n",
    "            \n",
    "        elif self.multistep_mode == MultistepMode.ALG:\n",
    "            for i in range(num_steps):\n",
    "                warped_B = img_B if i==0 else self.apply_warp(theta_from_alpha_sum, img_B)\n",
    "                alpha = self.forward_one_step(img_A, warped_B)\n",
    "                alpha_sum = alpha if i==0 else alpha + alpha_sum\n",
    "                theta_from_alpha_sum = torch.matrix_exp(alpha_sum)\n",
    "            alpha = alpha_sum\n",
    "            warped_B = self.apply_warp(torch.matrix_exp(alpha), img_B)\n",
    "            \n",
    "        \n",
    "        if not self.supervised:\n",
    "            sim_loss = self.compute_sim_loss(img_A, warped_B)\n",
    "        else:\n",
    "            sim_loss = torch.tensor(0)\n",
    "        \n",
    "        # get the ground truth correct transform, centered version of MONAI coordinates\n",
    "        Ta = img_A.meta['affine'].float()\n",
    "        Tb = img_B.meta['affine'].float()\n",
    "        true_theta = center_transform(torch.linalg.solve(Tb, Ta))\n",
    "        \n",
    "        if self.supervised:\n",
    "            if self.multistep_mode == MultistepMode.GP:\n",
    "                supervised_loss = ((theta - true_theta.to(theta.device))**2).sum(dim=(1,2)).mean()\n",
    "            elif self.multistep_mode == MultistepMode.ALG:\n",
    "                # TODO: Instead of using matrix log we might want to actually generate the transforms for augmentation\n",
    "                # via exponentiation in the first place.\n",
    "                true_alpha = batch_matrix_log(true_theta)\n",
    "                supervised_loss = ((alpha - true_alpha.to(theta.device))**2).sum(dim=(1,2)).mean()\n",
    "        else:\n",
    "            supervised_loss = torch.tensor(0)\n",
    "        \n",
    "        return ModelOutput(\n",
    "            affine = theta,\n",
    "            warped_moving = warped_B,\n",
    "            sim_loss = sim_loss,\n",
    "            regularization_loss = torch.tensor(0),\n",
    "            supervised_loss = supervised_loss,\n",
    "            all_loss = sim_loss if not self.supervised else supervised_loss,\n",
    "            true_theta= true_theta,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf38bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not necessarily needed and not sufficiently tested\n",
    "\n",
    "# Conversions between torch and monai coords\n",
    "S1,S2,S3 = spatial_size\n",
    "M_to_T = torch.tensor([[0, 0, 2/(S3-1), 0], [0,2/(S2-1),0,0], [2/(S1-1),0,0,0], [0,0,0,1]], dtype=torch.float32).to(device)\n",
    "T_to_M = torch.linalg.inv(M_to_T)\n",
    "\n",
    "def get_correct_transform_torch_coords(img_A, img_B):\n",
    "    \n",
    "    Ta = img_A.meta['affine'].float()\n",
    "    Tb = img_B.meta['affine'].float()\n",
    "    \n",
    "    # Correct transform in monai coords: Tb * Ta^{-1}\n",
    "    T = torch.linalg.solve(Tb, Ta)\n",
    "    \n",
    "    # Convert to torch coords\n",
    "    dv = T.device\n",
    "    T_torch = torch.matmul(M_to_T.to(dv), torch.matmul(T, T_to_M.to(dv)))\n",
    "    \n",
    "    return T_torch[:,:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfaa119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossCurves:\n",
    "    def __init__(self, name : str, spatial_dims : tuple = None):\n",
    "        self.name = name\n",
    "        \n",
    "        self.epochs =[]\n",
    "        self.all_losses = []\n",
    "        self.sim_losses = []\n",
    "        self.regularization_losses = []\n",
    "        self.supervised_losses = []\n",
    "        \n",
    "        self.clear_buffers()\n",
    "        \n",
    "    def clear_buffers(self):\n",
    "        self.all_losses_buffer = []\n",
    "        self.sim_losses_buffer = []\n",
    "        self.supervised_losses_buffer = []\n",
    "        self.regularization_losses_buffer = []\n",
    "        \n",
    "    def add_to_buffer(self, model_output : ModelOutput):\n",
    "        self.all_losses_buffer.append(model_output.all_loss.item())\n",
    "        self.sim_losses_buffer.append(model_output.sim_loss.item())\n",
    "        self.supervised_losses_buffer.append(model_output.supervised_loss.item())\n",
    "        self.regularization_losses_buffer.append(model_output.regularization_loss.item())\n",
    "\n",
    "    def aggregate_buffers_for_epoch(self, epoch : int):\n",
    "        self.epochs.append(epoch)\n",
    "        self.all_losses.append(np.mean(self.all_losses_buffer))\n",
    "        self.sim_losses.append(np.mean(self.sim_losses_buffer))\n",
    "        self.regularization_losses.append(np.mean(self.regularization_losses_buffer))\n",
    "        self.supervised_losses.append(np.mean(self.supervised_losses_buffer))\n",
    "        self.clear_buffers()\n",
    "        \n",
    "    def plot(self, savepath=None):\n",
    "        fig, axs = plt.subplots(1,4,figsize = (15,5))\n",
    "        axs[0].plot(self.epochs, self.all_losses)\n",
    "        axs[0].set_title(f\"{self.name}: overall loss\")\n",
    "        axs[1].plot(self.epochs, self.sim_losses)\n",
    "        axs[1].set_title(f\"{self.name}: similarity loss\")\n",
    "        axs[2].plot(self.epochs, self.regularization_losses, label=\"regularization loss\")\n",
    "        axs[2].set_title(f\"{self.name}: regularization loss\")\n",
    "        axs[3].plot(self.epochs, self.supervised_losses, label=\"supervised loss\")\n",
    "        axs[3].set_title(f\"{self.name}: supervised loss\")\n",
    "        for ax in axs:\n",
    "            ax.set_xlabel(\"epoch\")\n",
    "        if savepath is not None:\n",
    "            plt.savefig(savepath)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217b30b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lncc_loss = monai.losses.LocalNormalizedCrossCorrelationLoss(\n",
    "    spatial_dims=3,\n",
    "    kernel_size=5,\n",
    "    smooth_nr = 0,\n",
    "    smooth_dr = 1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3699c218",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = monai.data.DataLoader(ds_train, shuffle=True, batch_size=4, drop_last=True)\n",
    "dl_valid = monai.data.DataLoader(ds_valid, shuffle=True, batch_size=4, drop_last=True)\n",
    "max_epochs = 300\n",
    "e = 0 # epoch counter\n",
    "validate_when = lambda e : ((e%5==0) and (e!=0)) or (e==max_epochs-1)\n",
    "\n",
    "model = ExponentialRigidRegModel(\n",
    "    compute_sim_loss = ncc_loss,\n",
    "    down_convolutions=3,\n",
    "    depth=6,\n",
    "    max_channels=256,\n",
    "    init_channels=64,\n",
    "    spatial_size=spatial_size,\n",
    "    lambda_reg=10,\n",
    "    fc_hidden_layers=[],\n",
    "    allow_scaling=True,\n",
    "    supervised=True,\n",
    "    num_steps=1,\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "schedule_lr = True\n",
    "min_lr=1e-7\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "\n",
    "loss_curves_train = LossCurves(\"training\")\n",
    "loss_curves_valid = LossCurves(\"validation\", spatial_dims=spatial_size)\n",
    "print(\"Number of trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5278baf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPORARY CELL-- playing around with rebuilding data loaders with different affine transform strength\n",
    "# maybe scheduling the affine transform strength can allow it to gradually learn up to large transforms\n",
    "\n",
    "# Control the overall scale of affine transform\n",
    "a=1.0\n",
    "\n",
    "S = spatial_size[0]\n",
    "\n",
    "rand_affine_params = {\n",
    "    'prob':1.,\n",
    "    'mode': 'bilinear',\n",
    "    'padding_mode': 'zeros',\n",
    "    'spatial_size':spatial_size,\n",
    "    'cache_grid':True,\n",
    "    'rotate_range': (a*np.pi/2,)*3,\n",
    "    'shear_range': (0,)*6, # no shearing\n",
    "    'translate_range': (a*S/16,)*3,\n",
    "    'scale_range': (a*0.4,)*3, # no scaling\n",
    "}\n",
    "\n",
    "rand_affine_transform = monai.transforms.RandAffineD(keys=fa_key, **rand_affine_params)\n",
    "\n",
    "transform_valid = monai.transforms.Compose(base_transforms + [rand_affine_transform])\n",
    "transform_train = monai.transforms.Compose(base_transforms + [rand_affine_transform])\n",
    "\n",
    "ds_train = monai.data.CacheDataset(data_train, transform_train)\n",
    "ds_valid = monai.data.CacheDataset(data_valid, transform_valid)\n",
    "\n",
    "dl_train = monai.data.DataLoader(ds_train, shuffle=True, batch_size=6, drop_last=True)\n",
    "dl_valid = monai.data.DataLoader(ds_valid, shuffle=True, batch_size=6, drop_last=True)\n",
    "\n",
    "# max_epochs = 100\n",
    "# for g in optimizer.param_groups:\n",
    "#     g['lr'] = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e309a14",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "\n",
    "while e<max_epochs:\n",
    "#     current_lr = scheduler.get_last_lr()[0]\n",
    "    current_lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    print(f'Epoch {e+1}/{max_epochs} (LR = {current_lr:.1e}):')\n",
    "    \n",
    "    # Train\n",
    "    model.train()\n",
    "    dl_train_iter = iter(dl_train)\n",
    "    while True:\n",
    "        try:\n",
    "            b1 = next(dl_train_iter)\n",
    "            b2 = next(dl_train_iter)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        model_output = model(b1['fa'], b2['fa'])\n",
    "        model_output.all_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_curves_train.add_to_buffer(model_output)\n",
    "        del(model_output)\n",
    "\n",
    "    if schedule_lr:\n",
    "        if scheduler.get_last_lr()[0] > min_lr:\n",
    "            scheduler.step()\n",
    "\n",
    "    loss_curves_train.aggregate_buffers_for_epoch(e)\n",
    "    print(f\"\\tTraining loss: {loss_curves_train.all_losses[-1]:.4f} (sup={loss_curves_train.supervised_losses[-1]:.4f}, sim={loss_curves_train.sim_losses[-1]:.4f}, reg={loss_curves_train.regularization_losses[-1]:.4f})\")\n",
    "    \n",
    "    # Validate\n",
    "    if validate_when(e):\n",
    "        model.eval()\n",
    "        dl_valid_iter = iter(dl_valid)\n",
    "        while True:\n",
    "            try:\n",
    "                b1 = next(dl_valid_iter)\n",
    "                b2 = next(dl_valid_iter)\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model_output = model(b1['fa'], b2['fa'])\n",
    "                loss_curves_valid.add_to_buffer(model_output)\n",
    "        loss_curves_valid.aggregate_buffers_for_epoch(e) \n",
    "        print(\"\\tValidation loss:\", loss_curves_valid.all_losses[-1])\n",
    "    \n",
    "    e += 1\n",
    "    \n",
    "    \n",
    "from IPython.display import Audio\n",
    "sound_file = '/home/ebrahim/Desktop/beep-01a.wav'\n",
    "Audio(sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd70383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE\n",
    "\n",
    "loss_curves_train.plot(savepath = footsteps.output_dir + 'loss_plot_train.png')\n",
    "loss_curves_valid.plot(savepath = footsteps.output_dir + 'loss_plot_valid.png')\n",
    "\n",
    "with open(footsteps.output_dir + 'loss_curves.p', 'wb') as f:\n",
    "    pickle.dump([loss_curves_train, loss_curves_valid],f)\n",
    "\n",
    "torch.save(model.state_dict(), footsteps.output_dir + 'model_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e32faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD\n",
    "\n",
    "experiment_name_to_load = \"rigid lo res matrix_exp\"\n",
    "load_dir = os.path.join('results', experiment_name_to_load)\n",
    "\n",
    "model.load_state_dict(torch.load(os.path.join(load_dir,'model_state_dict.pth')))\n",
    "\n",
    "with open(os.path.join(load_dir,'loss_curves.p'), 'rb') as f:\n",
    "    loss_curves_train, loss_curves_valid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ff3be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "\n",
    "# Load a pair of images for visualization of model performance below\n",
    "\n",
    "# Choose whether to view performance on training or on validation data\n",
    "dl = monai.data.DataLoader(ds_valid, shuffle=True, batch_size=1, drop_last=True)\n",
    "it = iter(dl)\n",
    "d1 = next(it)\n",
    "d2 = next(it)\n",
    "img_A = d1['fa']\n",
    "img_B = d2['fa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d285c635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, see what it does to untransformed images\n",
    "\n",
    "# img_A = monai.data.MetaTensor(\n",
    "#     monai.transforms.Compose(base_transforms)(random.choice(data_valid))[fa_key].unsqueeze(0),\n",
    "#     affine=torch.eye(4).unsqueeze(0)\n",
    "# )\n",
    "# img_B = monai.data.MetaTensor(\n",
    "#     monai.transforms.Compose(base_transforms)(random.choice(data_valid))[fa_key].unsqueeze(0),\n",
    "#     affine=torch.eye(4).unsqueeze(0)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42565b2b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    model_output = model(img_A,img_B)\n",
    "\n",
    "img_B_warped = model_output.warped_moving\n",
    "\n",
    "preview_slices = tuple(np.array(spatial_size)//2)\n",
    "\n",
    "print(\"moving:\")\n",
    "util.preview_image(img_B[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"warped moving:\")\n",
    "util.preview_image(img_B_warped[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"target:\")\n",
    "util.preview_image(img_A[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"checkerboard of warped moving and target:\")\n",
    "util.preview_checkerboard(img_A[0,0].cpu(), img_B_warped[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"affine transform matrix:\")\n",
    "print(model_output.affine[0].cpu())\n",
    "print(\"the correct affine transform matrix:\")\n",
    "print(model_output.true_theta[0].cpu())\n",
    "true_theta = model_output.true_theta.to(img_B.device)\n",
    "true_theta_uncentered = uncenter_transform(true_theta)\n",
    "img_B_warped_correct = model.affine_transform(img_B,true_theta_uncentered)\n",
    "print(\"moving image warped by the correct affine transform\")\n",
    "util.preview_image(img_B_warped_correct[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"sim loss:\", model_output.sim_loss.item())\n",
    "print(\"regularization loss:\", model_output.regularization_loss.item())\n",
    "print(\"overall loss:\", model_output.all_loss.item())\n",
    "print(\"checkerboard of correct warped moving and target:\")\n",
    "util.preview_checkerboard(img_A[0,0].cpu(), img_B_warped_correct[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"Similarity loss if we had the correct transform:\",model.compute_sim_loss(img_A,img_B_warped_correct).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ea6c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(name):\n",
    "    model = ExponentialRigidRegModel(\n",
    "        compute_sim_loss = ncc_loss,\n",
    "        down_convolutions=3,\n",
    "        depth=5,\n",
    "        max_channels=256,\n",
    "        init_channels=64,\n",
    "        spatial_size=spatial_size,\n",
    "        lambda_reg=10,\n",
    "        fc_hidden_layers=[],\n",
    "        allow_scaling=True,\n",
    "        supervised=False,\n",
    "        num_steps=1,\n",
    "    ).to(device)\n",
    "    load_dir = os.path.join('results', name)\n",
    "    model.load_state_dict(torch.load(os.path.join(load_dir,'model_state_dict.pth')))\n",
    "    return model\n",
    "\n",
    "model1 = create_model('multinetwork-multistep-3')\n",
    "model2 = create_model('multinetwork-multistep-2')\n",
    "model3 = create_model('multinetwork-multistep-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fe7580",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import util\n",
    "\n",
    "# Load a pair of images for visualization of model performance below\n",
    "\n",
    "# Choose whether to view performance on training or on validation data\n",
    "dl = monai.data.DataLoader(ds_valid, shuffle=True, batch_size=1, drop_last=True)\n",
    "it = iter(dl)\n",
    "d1 = next(it)\n",
    "d2 = next(it)\n",
    "img_A = d1['fa']\n",
    "img_B = d2['fa']\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    model3.eval()\n",
    "    model_output = model3(img_A,img_B)\n",
    "    affine = model_output.affine\n",
    "    img_B_warped = model3.apply_warp(affine, img_B)\n",
    "    \n",
    "    model2.eval()\n",
    "    model_output = model2(img_A,img_B_warped)\n",
    "    affine = compose_affine_44(model_output.affine, affine)\n",
    "    img_B_warped = model2.apply_warp(affine, img_B)\n",
    "    \n",
    "    model1.eval()\n",
    "    model_output = model1(img_A,img_B_warped)\n",
    "    affine = compose_affine_44(model_output.affine, affine)\n",
    "    img_B_warped = model1.apply_warp(affine, img_B)\n",
    "\n",
    "\n",
    "\n",
    "preview_slices = tuple(np.array(spatial_size)//2)\n",
    "\n",
    "print(\"moving:\")\n",
    "util.preview_image(img_B[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"warped moving:\")\n",
    "util.preview_image(img_B_warped[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"target:\")\n",
    "util.preview_image(img_A[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"checkerboard of warped moving and target:\")\n",
    "util.preview_checkerboard(img_A[0,0].cpu(), img_B_warped[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"affine transform matrix:\")\n",
    "print(model_output.affine[0].cpu())\n",
    "print(\"the correct affine transform matrix:\")\n",
    "print(model_output.true_theta[0].cpu())\n",
    "true_theta = model_output.true_theta.to(img_B.device)\n",
    "true_theta_uncentered = uncenter_transform(true_theta)\n",
    "img_B_warped_correct = model.affine_transform(img_B,true_theta_uncentered)\n",
    "print(\"moving image warped by the correct affine transform\")\n",
    "util.preview_image(img_B_warped_correct[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"sim loss:\", model_output.sim_loss.item())\n",
    "print(\"regularization loss:\", model_output.regularization_loss.item())\n",
    "print(\"overall loss:\", model_output.all_loss.item())\n",
    "print(\"checkerboard of correct warped moving and target:\")\n",
    "util.preview_checkerboard(img_A[0,0].cpu(), img_B_warped_correct[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"Similarity loss if we had the correct transform:\",model.compute_sim_loss(img_A,img_B_warped_correct).item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
