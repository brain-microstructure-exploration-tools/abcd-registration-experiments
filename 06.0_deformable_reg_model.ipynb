{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee94cda0",
   "metadata": {},
   "source": [
    "ICON or GradICON deformable registration of FA images. (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a72721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import monai\n",
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "import footsteps\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2129f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "spatial_size = (144,144,144)\n",
    "num_scales = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8ecdb9",
   "metadata": {},
   "source": [
    "The input images are known to be $140\\times140\\times140$, and we can pad them out to $144$ in each dimension.\n",
    "\n",
    "We assume that each dimension in `spatial_size` is divisible by $2^{\\texttt{num}\\_\\texttt{scales}-1}$, because we will downsample by a factor of $2$ a bunch of times to produce images at different scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7490849",
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_dir = './dti_fit_images/fa'\n",
    "fa_keys = [f'fa{i}' for i in range(num_scales)]\n",
    "fa_key = fa_keys[0] # a simpler way to refer to the first element of fa_keys\n",
    "data = [{fa_key:path, \"filename\":os.path.basename(path)} for path in glob.glob(os.path.join(fa_dir,'*'))]\n",
    "data_train, data_valid = monai.data.utils.partition_dataset(data, ratios=(8,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f6318",
   "metadata": {},
   "source": [
    "`fa_keys` is a list mapping index to key for scale at that index: $0$ is the base resolution, $1$ is downscaled by a factor of $2$, $2$ is further downscaled by a factor of $2$, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b368d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_transforms = [\n",
    "    monai.transforms.LoadImageD(keys=fa_key),\n",
    "    monai.transforms.AddChannelD(keys=fa_key),\n",
    "    monai.transforms.SpatialPadD(keys=fa_key, spatial_size=spatial_size, mode=\"constant\"),\n",
    "    monai.transforms.ToTensorD(keys=fa_key),\n",
    "    monai.transforms.ToDeviceD(keys=fa_key, device=device),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01570643",
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_scales>1:\n",
    "    add_scales_transforms = [\n",
    "        monai.transforms.CopyItemsD(keys=[fa_key], times=(num_scales-1), names=fa_keys[1:])\n",
    "    ]\n",
    "    add_scales_transforms += [\n",
    "        monai.transforms.ResizeD(keys=[fa_keys[i]], spatial_size=[s//2**i for s in spatial_size])\n",
    "        for i in range(1,num_scales)\n",
    "    ]\n",
    "else:\n",
    "    add_scales_transforms = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e759b26",
   "metadata": {},
   "source": [
    "The `add_scales_transforms` is a chain of transforms that adds downsampled versions of the base images, with keys coming from `fa_keys`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed049b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_valid = monai.transforms.Compose(base_transforms + add_scales_transforms)\n",
    "transform_train = monai.transforms.Compose(base_transforms + add_scales_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e54a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "warp = monai.networks.blocks.Warp(mode=\"bilinear\", padding_mode=\"zeros\")\n",
    "\n",
    "def mse_loss(b1, b2):\n",
    "    \"\"\"Return image similarity loss given two batches b1 and b2 of shape (batch_size, channels, H,W,D).\n",
    "    It is scaled up a bit here.\"\"\"\n",
    "    return 10000*((b1-b2)**2).mean()\n",
    "\n",
    "def ncc_loss(b1, b2):\n",
    "    \"\"\"Return the negative NCC loss given two batches b1 and b2 of shape (batch_size, channels, H,W,D).\n",
    "    It is averaged over batches and channels.\"\"\"\n",
    "    mu1 = b1.mean(dim=(2,3,4)) # means\n",
    "    mu2 = b2.mean(dim=(2,3,4))\n",
    "    alpha1 = (b1**2).mean(dim=(2,3,4)) # second moments\n",
    "    alpha2 = (b2**2).mean(dim=(2,3,4))\n",
    "    alpha12 = (b1*b2).mean(dim=(2,3,4)) # cross term\n",
    "    numerator = alpha12 - mu1*mu2\n",
    "    denominator = torch.sqrt((alpha1 - mu1**2) * (alpha2-mu2**2))\n",
    "    ncc = numerator / denominator\n",
    "    return -ncc.mean() # average over batches and channels\n",
    "\n",
    "def compose_ddf(u,v):\n",
    "    \"\"\"Compose two displacement fields, return the displacement that warps by v followed by u\"\"\"\n",
    "    return u + warp(v,u)\n",
    "\n",
    "H, W, D = spatial_size\n",
    "\n",
    "# Compute discrete spatial derivatives\n",
    "def diff_and_trim(array, axis):\n",
    "    \"\"\"Take the discrete difference along a spatial axis, which should be 2,3, or 4.\n",
    "    Return a difference tensor with all spatial axes trimmed by 1.\"\"\"\n",
    "    return torch.diff(array, axis=axis)[:, :, :(H-1), :(W-1), :(D-1)]\n",
    "\n",
    "def size_of_spatial_derivative(u):\n",
    "    \"\"\"Return the squared Frobenius norm of the spatial derivative of the given displacement field.\n",
    "    To clarify, this is about the derivative of the actual displacement field map, not the deformation\n",
    "    that the displacement field map defines. The expected input shape is (batch,3,H,W,D).\n",
    "    Output shape is (batch).\"\"\"\n",
    "    dx = diff_and_trim(u, 2)\n",
    "    dy = diff_and_trim(u, 3)\n",
    "    dz = diff_and_trim(u, 4)\n",
    "    return(dx**2 + dy**2 + dz**2).sum(axis=1).mean(axis=[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c0d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelOutput = namedtuple(\"ModelOutput\", \"all_loss, sim_loss, icon_loss, deformation_AB\")\n",
    "\n",
    "from enum import Enum\n",
    "class IconLossType(Enum):\n",
    "    ICON = 0\n",
    "    GRADICON = 1\n",
    "    \n",
    "# A deformable registration model\n",
    "class RegModel(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 lambda_reg,\n",
    "                 compute_sim_loss,\n",
    "                 down_convolutions,\n",
    "                 depth,\n",
    "                 max_channels,\n",
    "                 init_channels,\n",
    "                 icon_loss_type : IconLossType = IconLossType.GRADICON,\n",
    "                 downsample_early = True\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Create a deformable registration network\n",
    "        \n",
    "        Args:\n",
    "            lambda_reg: Hyperparameter for weight of icon/gradicon loss\n",
    "            compute_sim_loss: A function that compares two batches of images and returns a similarity loss\n",
    "            down_convolutions: How many stride=2 convolutions to include in the down-convolution part of the unets\n",
    "                               when at the original image scale. We assume the original image size is divisible by\n",
    "                               2**down_convolutions\n",
    "            depth: Total number of layers in half of the unet. Increase this to increase model capacity.\n",
    "                   Must be >= down_convolutions\n",
    "            max_channels: As you go to deeper layers, channels grow by powers of two... up to a maximum given here.\n",
    "            init_channels: how many channels in the first layer\n",
    "            icon_loss_type: whether to use ICON or GradICON\n",
    "            downsample_early: The CNN can contain strided and unstrided convolutions to achieve the requested\n",
    "                              depth; this paramter decides whether to prefer putting strided convolutions earlier\n",
    "                              or later.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.icon_loss_type = icon_loss_type\n",
    "        if depth < down_convolutions:\n",
    "            raise ValueError(\"Must have depth >= down_convolutions\")\n",
    "        # (We will assume that the original image size is divisible by 2**n.)\n",
    "\n",
    "        \n",
    "        \n",
    "        num_twos = down_convolutions # The number of 2's we will put in the sequence of convolutional strides.\n",
    "        num_ones = depth-down_convolutions # The number of 1's\n",
    "        num_one_two_pairs = min(num_ones, num_twos) # The number of 1,2 pairs to stick in the middle\n",
    "        if downsample_early:\n",
    "            stride_sequence = (2,)*(num_twos-num_one_two_pairs) + (1,2)*num_one_two_pairs + (1,)*(num_ones-num_one_two_pairs)\n",
    "        else:\n",
    "            stride_sequence = (1,)*(num_ones-num_one_two_pairs) + (1,2)*num_one_two_pairs + (2,)*(num_twos-num_one_two_pairs)\n",
    "        channel_sequence = [min(init_channels*2**c,max_channels) for c in range(num_twos+num_ones+1)]\n",
    "        \n",
    "        self.reg_net = monai.networks.nets.UNet(\n",
    "            3,  # spatial dims\n",
    "            2,  # input channels (one for fixed image and one for moving image)\n",
    "            3,  # output channels (to represent 3D displacement vector field)\n",
    "            channel_sequence,\n",
    "            stride_sequence,\n",
    "            dropout=0.2,\n",
    "            norm=\"batch\"\n",
    "        )\n",
    "        self.strides = stride_sequence\n",
    "        self.channels = channel_sequence\n",
    "        \n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.compute_sim_loss = compute_sim_loss\n",
    "    \n",
    "    def update_lambda_reg(self, new_lambda_reg):\n",
    "        self.lambda_reg = new_lambda_reg\n",
    "        \n",
    "    def forward(self, img_A, img_B) -> ModelOutput:\n",
    "        deformation_AB = self.reg_net(torch.cat([img_A, img_B], dim=1)) # deforms img_B to the space of img_A\n",
    "        deformation_BA = self.reg_net(torch.cat([img_B, img_A], dim=1)) # deforms img_A to the space of img_B\n",
    "\n",
    "        img_B_warped = warp(img_B, deformation_AB)\n",
    "        img_A_warped = warp(img_A, deformation_BA)\n",
    "        sim_loss_A = self.compute_sim_loss(img_A, img_B_warped)\n",
    "        sim_loss_B = self.compute_sim_loss(img_B, img_A_warped)\n",
    "        composite_deformation_A = compose_ddf(deformation_AB, deformation_BA)\n",
    "        composite_deformation_B = compose_ddf(deformation_BA, deformation_AB)\n",
    "        \n",
    "        if self.icon_loss_type == IconLossType.GRADICON:\n",
    "            icon_loss_A = size_of_spatial_derivative(composite_deformation_A).mean()\n",
    "            icon_loss_B = size_of_spatial_derivative(composite_deformation_B).mean()\n",
    "        elif self.icon_loss_type == IconLossType.ICON:\n",
    "            icon_loss_A = (composite_deformation_A**2).mean()\n",
    "            icon_loss_B = (composite_deformation_B**2).mean()\n",
    "        \n",
    "        sim_loss = sim_loss_A + sim_loss_B\n",
    "        icon_loss = icon_loss_A + icon_loss_B\n",
    "        \n",
    "        return ModelOutput(\n",
    "            all_loss = sim_loss + self.lambda_reg * icon_loss,\n",
    "            sim_loss = sim_loss,\n",
    "            icon_loss = icon_loss,\n",
    "            deformation_AB = deformation_AB\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674e8f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A resize transform that operates on batches of images\n",
    "class BatchResizer:\n",
    "    def __init__(self, spatial_dims):\n",
    "        self.resize = monai.transforms.Resize(spatial_dims)\n",
    "    def __call__(self, batch):\n",
    "        return torch.stack([self.resize(x) for x in monai.transforms.Decollated()(batch)])\n",
    "\n",
    "    \n",
    "# A multiscale version of the Model idea above\n",
    "class MultiscaleModel(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 lambda_reg,\n",
    "                 compute_sim_loss,\n",
    "                 num_subnetworks,\n",
    "                 down_convolutions,\n",
    "                 depth,\n",
    "                 max_channels,\n",
    "                 init_channels,\n",
    "                 icon_loss_type : IconLossType = IconLossType.GRADICON\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Create series of registration networks that operate at multiple scales\n",
    "        \n",
    "        Args:\n",
    "            lambda_reg: Hyperparameter for weight of icon/gradicon loss\n",
    "            compute_sim_loss: A function that compares two batches of images and returns a similarity loss\n",
    "            num_subnetworks: The number of scales to operate on.\n",
    "            down_convolutions: How many stride=2 convolutions to include in the down-convolution part of the unets\n",
    "                               when at the original image scale. We assume the original image size is divisible by\n",
    "                               2**down_convolutions\n",
    "            depth: Total number of layers in half of the unet. Increase this to increase model capacity.\n",
    "                   Must be >= down_convolutions\n",
    "            max_channels: As you go to deeper layers, channels grow by powers of two... up to a maximum given here.\n",
    "            init_channels: how many channels in the first layer\n",
    "            icon_loss_type: whether to use ICON or GradICON\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_subnetworks = num_subnetworks\n",
    "        self.reg_nets = torch.nn.ModuleList()\n",
    "        self.reg_net_architecture_info = []\n",
    "        self.icon_loss_type = icon_loss_type\n",
    "        n = down_convolutions # Amount of down-convolution for the original image size.\n",
    "        if depth < n:\n",
    "            raise ValueError(\"Must have depth >= down_convolutions\")\n",
    "        # (We will assume that the original image size is divisible by 2**n.)\n",
    "        for i in range(num_subnetworks):\n",
    "            # i is scale. i=0 is the original input image scale. Scale i is at a downsample factor of 2**i.\n",
    "            num_twos = n-i # The number of 2's we will put in the sequence of convolutional strides.\n",
    "            num_ones = i + (depth-n) # The number of 1's\n",
    "            num_one_two_pairs = min(num_ones, num_twos) # The number of 1,2 pairs to stick in the middle\n",
    "            stride_sequence = (2,)*(num_twos-num_one_two_pairs) + (1,2)*num_one_two_pairs + (1,)*(num_ones-num_one_two_pairs)\n",
    "            channel_sequence = [min(init_channels*2**c,max_channels) for c in range(num_twos+num_ones+1)]\n",
    "            self.reg_nets.append(\n",
    "                monai.networks.nets.UNet(\n",
    "                    3,  # spatial dims\n",
    "                    2,  # input channels (one for fixed image and one for moving image)\n",
    "                    3,  # output channels (to represent 3D displacement vector field)\n",
    "                    channel_sequence,\n",
    "                    stride_sequence,\n",
    "                    dropout=0.2,\n",
    "                    norm=\"batch\"\n",
    "                )\n",
    "            )\n",
    "            self.reg_net_architecture_info.append(\n",
    "                {'strides':stride_sequence,'channels':channel_sequence}\n",
    "            )\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.compute_sim_loss = compute_sim_loss\n",
    "        \n",
    "        self.batch_resizers = [BatchResizer([s//2**i for s in spatial_size]) for i in range(num_subnetworks-1)]\n",
    "    \n",
    "    def __str__(self):\n",
    "        s = f\"\"\"A stack of {self.num_subnetworks} UNets for predicting displacement fields\n",
    "        for pairwise 3D image registration.\"\"\"\n",
    "        for i in range(self.num_subnetworks):\n",
    "            info = self.reg_net_architecture_info[i]\n",
    "            s += f\"\\nAt scale {i}:\\n\\tchannels={info['channels']}\"\n",
    "            s += f\"\\n\\tstrides={info['strides']}\"\n",
    "        return s\n",
    "    \n",
    "    def update_lambda_reg(self, new_lambda_reg):\n",
    "        self.lambda_reg = new_lambda_reg\n",
    "        \n",
    "    def multiscale_reg_nets(self, img_A, img_B, cache_multiscale_warps=False):\n",
    "        \"\"\"\n",
    "        Here we expect img_A to be a list consisting of batches of target images:\n",
    "            img_A[0] is a batch of target images at the original resolution,\n",
    "            img_A[1] is a batch of target images downsampled by a factor of 2 in each dimension,\n",
    "            img_A[2] is a batch of target images downsampled by a factor of 4 in each dimension,\n",
    "            etc.\n",
    "        and similarly img_B is a list consisting of batches of moving images.\n",
    "        Returns the final displacement field (composed over all scales) for deforming img_B[0] to img_A[0].\n",
    "        \n",
    "        cache_multiscale_warps: Whether to cache the last-computed warps at each scale\n",
    "        in attributes phis and phi_comps.\n",
    "        phis[i] is the warp at scale i, where i=0 is the original image scale.\n",
    "        \"\"\"\n",
    "        \n",
    "        if cache_multiscale_warps:\n",
    "            self.phis=[None]*self.num_subnetworks\n",
    "            self.phi_comps=[None]*self.num_subnetworks\n",
    "        \n",
    "        for i in range(self.num_subnetworks - 1, -1, -1): # Run backwards to 0 from num_subnetworks-1\n",
    "            \n",
    "            # phi_up = Composite of warps up to scale i+1, operating at scale i\n",
    "            if i==self.num_subnetworks - 1:\n",
    "                pass # Base case: phi_up is the identity map.\n",
    "                # (We treat this case specially below to avoid complicating the computational graph with\n",
    "                # useless compositions with identity map)\n",
    "            else:\n",
    "                phi_up = self.batch_resizers[i](phi_comp)\n",
    "            \n",
    "            # warped_B = img_B at scale i warped by the composite of warps up to scale i+1\n",
    "            if i==self.num_subnetworks - 1:\n",
    "                warped_B = img_B[i] # Base case: \"the composite of warps up to scale i+1\" = the identity map\n",
    "            else:\n",
    "                warped_B = warp(img_B[i], phi_up)\n",
    "            \n",
    "            # phi = Warp from scale i, operating at scale i\n",
    "            phi = self.reg_nets[i](torch.cat([img_A[i], warped_B], dim=1))\n",
    "\n",
    "            # phi_comp = Composite of warps up to scale i, operating at scale i\n",
    "            if i==self.num_subnetworks - 1:\n",
    "                phi_comp = phi # Base case: phi_up = the identity map, i.e. chain to compose consists of phi only\n",
    "            else:\n",
    "                phi_comp = compose_ddf(phi,phi_up)\n",
    "        \n",
    "            if cache_multiscale_warps:\n",
    "                self.phis[i] = phi\n",
    "                self.phi_comps[i] = phi_comp\n",
    "        \n",
    "        return phi_comp\n",
    "        \n",
    "    def forward(self, img_A, img_B, cache_multiscale_warps=False) -> ModelOutput:\n",
    "        \"\"\"\n",
    "        Here we expect img_A to be a list consisting of batches of target images:\n",
    "            img_A[0] is a batch of target images at the original resolution,\n",
    "            img_A[1] is a batch of target images downsampled by a factor of 2 in each dimension,\n",
    "            img_A[2] is a batch of target images downsampled by a factor of 4 in each dimension,\n",
    "            etc.\n",
    "        and similarly img_B is a list consisting of batches of moving images.\n",
    "        \n",
    "        cache_multiscale_warps: Whether to cache the last-computed warps at each scale\n",
    "        in attributes phis and phi_comps.\n",
    "        phis[i] is the warp at scale i, where i=0 is the original image scale. Only warps from B to A are cached.\n",
    "        \"\"\"\n",
    "        deformation_AB = self.multiscale_reg_nets(img_A, img_B, cache_multiscale_warps) # deforms img_B to the space of img_A\n",
    "        deformation_BA = self.multiscale_reg_nets(img_B, img_A) # deforms img_A to the space of img_B\n",
    "\n",
    "        img_B0_warped = warp(img_B[0], deformation_AB)\n",
    "        img_A0_warped = warp(img_A[0], deformation_BA)\n",
    "        sim_loss_A = self.compute_sim_loss(img_A[0], img_B0_warped)\n",
    "        sim_loss_B = self.compute_sim_loss(img_B[0], img_A0_warped)\n",
    "        composite_deformation_A = compose_ddf(deformation_AB, deformation_BA)\n",
    "        composite_deformation_B = compose_ddf(deformation_BA, deformation_AB)\n",
    "        \n",
    "        if self.icon_loss_type == IconLossType.GRADICON:\n",
    "            icon_loss_A = size_of_spatial_derivative(composite_deformation_A).mean()\n",
    "            icon_loss_B = size_of_spatial_derivative(composite_deformation_B).mean()\n",
    "        elif self.icon_loss_type == IconLossType.ICON:\n",
    "            icon_loss_A = (composite_deformation_A**2).mean()\n",
    "            icon_loss_B = (composite_deformation_B**2).mean()\n",
    "        \n",
    "        sim_loss = sim_loss_A + sim_loss_B\n",
    "        icon_loss = icon_loss_A + icon_loss_B\n",
    "        \n",
    "        return ModelOutput(\n",
    "            all_loss = sim_loss + self.lambda_reg * icon_loss,\n",
    "            sim_loss = sim_loss,\n",
    "            icon_loss = icon_loss,\n",
    "            deformation_AB = deformation_AB\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2010a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JacobianDeterminant(torch.nn.Module):\n",
    "    \"\"\"Given a batch of displacement vector fields vf, compute the jacobian determinant scalar field.\"\"\"\n",
    "\n",
    "    def __init__(self, spatial_dims):\n",
    "        super().__init__()\n",
    "        self.spatial_dims = spatial_dims\n",
    "    def diff_and_trim(self, array, axis):\n",
    "        H,W,D = self.spatial_dims\n",
    "        return torch.diff(array, axis=axis)[:, :, :(H-1), :(W-1), :(D-1)]\n",
    "\n",
    "    def forward(self, vf):\n",
    "        \"\"\"\n",
    "        vf is assumed to be a vector field of shape (B,3,H,W,D),\n",
    "        and it is interpreted as a displacement field.\n",
    "        So it is defining a batch of discretely sampled maps from a subset of 3-space into 3-space,\n",
    "        namely (for batch index b) the map that sends point (x,y,z) to the point (x,y,z)+vf[b,:,x,y,z].\n",
    "        This function computes a jacobian determinant by taking discrete differences in each spatial direction.\n",
    "\n",
    "        Returns a numpy array of shape (b,H-1,W-1,D-1).\n",
    "        \"\"\"\n",
    "        dx = self.diff_and_trim(vf, 2)\n",
    "        dy = self.diff_and_trim(vf, 3)\n",
    "        dz = self.diff_and_trim(vf, 4)\n",
    "        \n",
    "        # Add derivative of identity map\n",
    "        dx[:,0] += 1\n",
    "        dy[:,1] += 1\n",
    "        dz[:,2] += 1\n",
    "\n",
    "        # Compute determinant at each spatial location\n",
    "        det = dx[:,0]*(dy[:,1]*dz[:,2]-dz[:,1]*dy[:,2]) - dy[:,0]*(dx[:,1]*dz[:,2] -\n",
    "                                                dz[:,1]*dx[:,2]) + dz[:,0]*(dx[:,1]*dy[:,2]-dy[:,1]*dx[:,2])\n",
    "\n",
    "        return det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfaa119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossCurves:\n",
    "    def __init__(self, name : str, include_folds : bool = False, spatial_dims : tuple = None):\n",
    "        self.name = name\n",
    "        \n",
    "        self.epochs =[]\n",
    "        self.all_losses = []\n",
    "        self.sim_losses = []\n",
    "        self.icon_losses = []\n",
    "        \n",
    "        self.include_folds = include_folds\n",
    "        if include_folds:\n",
    "            if spatial_dims is None:\n",
    "                raise Exception(\"Need argument spatial_dims to include fold count.\")\n",
    "            self.jacobian_determinant = JacobianDeterminant(spatial_dims)\n",
    "            self.fold_counts = []\n",
    "        \n",
    "        self.clear_buffers()\n",
    "        \n",
    "    def clear_buffers(self):\n",
    "        self.all_losses_buffer = []\n",
    "        self.sim_losses_buffer = []\n",
    "        self.icon_losses_buffer = []\n",
    "        \n",
    "        if self.include_folds:\n",
    "            self.fold_counts_buffer = []\n",
    "        \n",
    "    def add_to_buffer(self, model_output : ModelOutput):\n",
    "        self.all_losses_buffer.append(model_output.all_loss.item())\n",
    "        self.sim_losses_buffer.append(model_output.sim_loss.item())\n",
    "        self.icon_losses_buffer.append(model_output.icon_loss.item())\n",
    "        \n",
    "        if self.include_folds:\n",
    "            det = self.jacobian_determinant(model_output.deformation_AB)\n",
    "            num_folds = (det<0).sum(dim=(1,2,3))\n",
    "            num_folds_mean = num_folds.to(dtype=torch.float).mean().item() # average over batch\n",
    "            self.fold_counts_buffer.append(num_folds_mean)\n",
    "        \n",
    "    def aggregate_buffers_for_epoch(self, epoch : int):\n",
    "        self.epochs.append(epoch)\n",
    "        self.all_losses.append(np.mean(self.all_losses_buffer))\n",
    "        self.sim_losses.append(np.mean(self.sim_losses_buffer))\n",
    "        self.icon_losses.append(np.mean(self.icon_losses_buffer))\n",
    "        if self.include_folds:\n",
    "            self.fold_counts.append(np.mean(self.fold_counts_buffer))\n",
    "        self.clear_buffers()\n",
    "        \n",
    "    def plot(self, savepath=None):\n",
    "        fig, axs = plt.subplots(1,3 if not self.include_folds else 4,figsize = (15,5))\n",
    "        axs[0].plot(self.epochs, self.all_losses)\n",
    "        axs[0].set_title(f\"{self.name}: overall loss\")\n",
    "        axs[1].plot(self.epochs, self.sim_losses)\n",
    "        axs[1].set_title(f\"{self.name}: similarity loss\")\n",
    "        axs[2].plot(self.epochs, self.icon_losses, label=\"icon loss\")\n",
    "        axs[2].set_title(f\"{self.name}: icon loss\")\n",
    "        if self.include_folds:\n",
    "            axs[3].plot(self.epochs, self.fold_counts, label=\"average folds\")\n",
    "            axs[3].set_title(f\"{self.name}: average folds\")\n",
    "        for ax in axs:\n",
    "            ax.set_xlabel(\"epoch\")\n",
    "        if savepath is not None:\n",
    "            plt.savefig(savepath)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217b30b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lncc_loss = monai.losses.LocalNormalizedCrossCorrelationLoss(\n",
    "    spatial_dims=3,\n",
    "    kernel_size=5,\n",
    "    smooth_nr = 0,\n",
    "    smooth_dr = 1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de3c638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_scales_list(b):\n",
    "    \"\"\"Given a batch from dl_train or dl_valid, return the list of images at different scales that\n",
    "    would be suitable as input to a MultiscaleModel.\"\"\"\n",
    "    return list(map(lambda k : b[k], fa_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23566e4d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ds_train = monai.data.CacheDataset(data_train, transform_train)\n",
    "ds_valid = monai.data.CacheDataset(data_valid, transform_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cec1725",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Model creation and training for potentially multiscale approach, with no affine augmentation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732bfeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = monai.data.DataLoader(ds_train, shuffle=True, batch_size=2, drop_last=True)\n",
    "dl_valid = monai.data.DataLoader(ds_valid, shuffle=True, batch_size=2, drop_last=True)\n",
    "max_epochs = 4000\n",
    "\n",
    "validate_when = lambda e : ((e%5==0) and (e!=0)) or (e==max_epochs-1)\n",
    "print_aggregate_when = lambda e : (e%5==4) or (e==0) or (e==max_epochs-1)\n",
    "last_printed_epoch = -1\n",
    "\n",
    "schedule_lambda_reg = True\n",
    "lambda_reg_step_size = 0.05 # How much to increase lambda_reg each time it advances\n",
    "cooldown = 50 # How many epochs to allow before checking whether training loss increases and advacing lambda_reg if so\n",
    "cooldown_counter = cooldown\n",
    "lambda_reg_goal = 4 # Stop training once lambda_reg advances past this\n",
    "\n",
    "model = MultiscaleModel(\n",
    "    lambda_reg = 0.05,\n",
    "    compute_sim_loss = ncc_loss,\n",
    "    num_subnetworks=num_scales,\n",
    "    down_convolutions=4,\n",
    "    depth=4,\n",
    "    max_channels=256,\n",
    "    init_channels=32,\n",
    "    icon_loss_type=IconLossType.ICON,\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "schedule_lr = True\n",
    "min_lr=1e-5\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1**(1/3000))\n",
    "\n",
    "loss_curves_train = LossCurves(\"training\")\n",
    "loss_curves_valid = LossCurves(\"validation\", include_folds=True, spatial_dims=spatial_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f5605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e309a14",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "\n",
    "for e in range(max_epochs):\n",
    "#     current_lr = scheduler.get_last_lr()[0]\n",
    "    current_lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    print(f'Epoch {e+1}/{max_epochs} (LR = {current_lr:.1e}, lambda_reg = {model.lambda_reg:.1e}):')\n",
    "    \n",
    "    # Train\n",
    "    model.train()\n",
    "    dl_train_iter = iter(dl_train)\n",
    "    while True:\n",
    "        try:\n",
    "            b1 = next(dl_train_iter)\n",
    "            b2 = next(dl_train_iter)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        model_output = model(batch_to_scales_list(b1), batch_to_scales_list(b2))\n",
    "        model_output.all_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_curves_train.add_to_buffer(model_output)\n",
    "        del(model_output)\n",
    "\n",
    "    if schedule_lr:\n",
    "        if scheduler.get_last_lr()[0] > min_lr:\n",
    "            scheduler.step()\n",
    "\n",
    "    loss_curves_train.aggregate_buffers_for_epoch(e)\n",
    "    if print_aggregate_when(e):\n",
    "        l_all = np.mean(loss_curves_train.all_losses[last_printed_epoch+1:])\n",
    "        l_sim = np.mean(loss_curves_train.sim_losses[last_printed_epoch+1:])\n",
    "        l_icon = np.mean(loss_curves_train.icon_losses[last_printed_epoch+1:])\n",
    "        print(f\"\\tTraining loss: {l_all:.4f} (sim={l_sim:.4f}, ic={l_icon:.4f})\")\n",
    "        print(f\"\\t(aggregated from epochs {last_printed_epoch+2} to {len(loss_curves_train.all_losses)})\")\n",
    "        last_printed_epoch = e\n",
    "    \n",
    "    # Validate\n",
    "    if validate_when(e):\n",
    "        model.eval()\n",
    "        dl_valid_iter = iter(dl_valid)\n",
    "        while True:\n",
    "            try:\n",
    "                b1 = next(dl_valid_iter)\n",
    "                b2 = next(dl_valid_iter)\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model_output = model(batch_to_scales_list(b1), batch_to_scales_list(b2))\n",
    "                loss_curves_valid.add_to_buffer(model_output)\n",
    "        loss_curves_valid.aggregate_buffers_for_epoch(e) \n",
    "        print(\"\\tValidation loss:\", loss_curves_valid.all_losses[-1])\n",
    "        print(\"\\tAverage folds:\", loss_curves_valid.fold_counts[-1])\n",
    "    \n",
    "    if schedule_lambda_reg:\n",
    "        cooldown_counter -= 1;\n",
    "        if cooldown_counter<=0 and loss_curves_train.all_losses[-1] > loss_curves_train.all_losses[-2]:\n",
    "            print(f\"Updating lambda_reg.\")\n",
    "            model.update_lambda_reg(model.lambda_reg + lambda_reg_step_size)\n",
    "            cooldown_counter = cooldown # reset cooldown_counter\n",
    "        if model.lambda_reg > lambda_reg_goal:\n",
    "            print(\"Reached goal lambda_reg.\")\n",
    "            break\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23821225",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Model creation and training for single scale approach, with affine augmentation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b485f9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = monai.data.DataLoader(ds_train, shuffle=True, batch_size=2, drop_last=True)\n",
    "dl_valid = monai.data.DataLoader(ds_valid, shuffle=True, batch_size=2, drop_last=True)\n",
    "max_epochs = 4000\n",
    "\n",
    "validate_when = lambda e : ((e%5==0) and (e!=0)) or (e==max_epochs-1)\n",
    "print_aggregate_when = lambda e : (e%5==4) or (e==0) or (e==max_epochs-1)\n",
    "last_printed_epoch = -1\n",
    "\n",
    "schedule_lambda_reg = True\n",
    "lambda_reg_step_size = 0.05 # How much to increase lambda_reg each time it advances\n",
    "cooldown = 50 # How many epochs to allow before checking whether training loss increases and advacing lambda_reg if so\n",
    "cooldown_counter = cooldown\n",
    "lambda_reg_goal = 4 # Stop training once lambda_reg advances past this\n",
    "\n",
    "from customRandAffine import AffineAugmentation\n",
    "affine_aug = AffineAugmentation(spatial_size, 0.8)\n",
    "\n",
    "model = RegModel(\n",
    "    lambda_reg = 0.05,\n",
    "    compute_sim_loss = ncc_loss,\n",
    "    down_convolutions=4,\n",
    "    depth=4,\n",
    "    max_channels=256,\n",
    "    init_channels=32,\n",
    "    icon_loss_type=IconLossType.ICON,\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "schedule_lr = True\n",
    "min_lr=1e-5\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1**(1/3000))\n",
    "\n",
    "loss_curves_train = LossCurves(\"training\")\n",
    "loss_curves_valid = LossCurves(\"validation\", include_folds=True, spatial_dims=spatial_size)\n",
    "\n",
    "print(\"Number of trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124a9017",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "\n",
    "for e in range(max_epochs):\n",
    "#     current_lr = scheduler.get_last_lr()[0]\n",
    "    current_lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    print(f'Epoch {e+1}/{max_epochs} (LR = {current_lr:.1e}, lambda_reg = {model.lambda_reg:.1e}):')\n",
    "    \n",
    "    # Train\n",
    "    model.train()\n",
    "    dl_train_iter = iter(dl_train)\n",
    "    while True:\n",
    "        try:\n",
    "            b1 = next(dl_train_iter)\n",
    "            b2 = next(dl_train_iter)\n",
    "        except StopIteration:\n",
    "            break\n",
    "            \n",
    "        fixed, moving = affine_aug(b1[fa_key], b2[fa_key])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        model_output = model(fixed, moving)\n",
    "        model_output.all_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_curves_train.add_to_buffer(model_output)\n",
    "        del(model_output)\n",
    "\n",
    "    if schedule_lr:\n",
    "        if scheduler.get_last_lr()[0] > min_lr:\n",
    "            scheduler.step()\n",
    "\n",
    "    loss_curves_train.aggregate_buffers_for_epoch(e)\n",
    "    if print_aggregate_when(e):\n",
    "        l_all = np.mean(loss_curves_train.all_losses[last_printed_epoch+1:])\n",
    "        l_sim = np.mean(loss_curves_train.sim_losses[last_printed_epoch+1:])\n",
    "        l_icon = np.mean(loss_curves_train.icon_losses[last_printed_epoch+1:])\n",
    "        print(f\"\\tTraining loss: {l_all:.4f} (sim={l_sim:.4f}, ic={l_icon:.4f})\")\n",
    "        print(f\"\\t(aggregated from epochs {last_printed_epoch+2} to {len(loss_curves_train.all_losses)})\")\n",
    "        last_printed_epoch = e\n",
    "    \n",
    "    # Validate\n",
    "    if validate_when(e):\n",
    "        model.eval()\n",
    "        dl_valid_iter = iter(dl_valid)\n",
    "        while True:\n",
    "            try:\n",
    "                b1 = next(dl_valid_iter)\n",
    "                b2 = next(dl_valid_iter)\n",
    "            except StopIteration:\n",
    "                break\n",
    "            \n",
    "            fixed, moving = affine_aug(b1[fa_key], b2[fa_key])\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model_output = model(fixed, moving)\n",
    "                loss_curves_valid.add_to_buffer(model_output)\n",
    "        loss_curves_valid.aggregate_buffers_for_epoch(e) \n",
    "        print(\"\\tValidation loss:\", loss_curves_valid.all_losses[-1])\n",
    "        print(\"\\tAverage folds:\", loss_curves_valid.fold_counts[-1])\n",
    "    \n",
    "    if schedule_lambda_reg:\n",
    "        cooldown_counter -= 1;\n",
    "        if cooldown_counter<=0 and loss_curves_train.all_losses[-1] > loss_curves_train.all_losses[-2]:\n",
    "            print(f\"Updating lambda_reg.\")\n",
    "            model.update_lambda_reg(model.lambda_reg + lambda_reg_step_size)\n",
    "            cooldown_counter = cooldown # reset cooldown_counter\n",
    "        if model.lambda_reg > lambda_reg_goal:\n",
    "            print(\"Reached goal lambda_reg.\")\n",
    "            break\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41b6c8f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Saving, plotting, and loading\n",
    "\n",
    "(Relevant to all appraoches above)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd70383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE\n",
    "\n",
    "loss_curves_train.plot(savepath = footsteps.output_dir + 'loss_plot_train.png')\n",
    "loss_curves_valid.plot(savepath = footsteps.output_dir + 'loss_plot_valid.png')\n",
    "\n",
    "with open(footsteps.output_dir + 'loss_curves.p', 'wb') as f:\n",
    "    pickle.dump([loss_curves_train, loss_curves_valid],f)\n",
    "\n",
    "torch.save(model.state_dict(), footsteps.output_dir + 'model_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e32faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD\n",
    "\n",
    "experiment_name_to_load = \"2022-09-13-deformable\"\n",
    "load_dir = os.path.join('results', experiment_name_to_load)\n",
    "\n",
    "model.load_state_dict(torch.load(os.path.join(load_dir,'model_state_dict.pth')))\n",
    "\n",
    "with open(os.path.join(load_dir,'loss_curves.p'), 'rb') as f:\n",
    "    loss_curves_train, loss_curves_valid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235b38b2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Preview, for multiscale approach\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42565b2b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import util\n",
    "\n",
    "dl = dl_train # Choose whether to view performance on training or on validation data\n",
    "it = iter(dl)\n",
    "d1 = next(it)\n",
    "d2 = next(it)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    model_output = model(batch_to_scales_list(d1),batch_to_scales_list(d2))\n",
    "\n",
    "img_A = d1['fa0']\n",
    "img_B = d2['fa0']\n",
    "img_B_warped = warp(img_B, model_output.deformation_AB)\n",
    "\n",
    "preview_slices = (80,80,80)\n",
    "\n",
    "print(\"moving:\")\n",
    "util.preview_image(img_B[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"warped moving:\")\n",
    "util.preview_image(img_B_warped[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"target:\")\n",
    "util.preview_image(img_A[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"checkerboard of warped moving and target:\")\n",
    "util.preview_checkerboard(img_A[0,0].cpu(), img_B_warped[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"deformation vector field:\")\n",
    "util.preview_3D_vector_field(model_output.deformation_AB[0].cpu(), slices=preview_slices)\n",
    "print(\"deformed grid:\")\n",
    "util.preview_3D_deformation(model_output.deformation_AB[0].cpu(),5, slices=preview_slices)\n",
    "print(\"jacobian determinant:\")\n",
    "det = util.jacobian_determinant(model_output.deformation_AB[0].cpu())\n",
    "util.preview_image(det, normalize_by='slice', threshold=0, slices=preview_slices)\n",
    "print(\"sim loss:\", model_output.sim_loss.item())\n",
    "print(\"(grad?)icon loss:\", model_output.icon_loss.item())\n",
    "print(\"overall loss:\", model_output.all_loss.item())\n",
    "num_folds = (det<0).sum()\n",
    "print(\"Number of folds:\", num_folds, f\"(folding rate {100*num_folds/np.prod(det.shape)}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f5b1eb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Preview, for single scale approach with affine augmentation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ebb1c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import util\n",
    "\n",
    "dl = dl_valid # Choose whether to view performance on training or on validation data\n",
    "it = iter(dl)\n",
    "d1 = next(it)\n",
    "d2 = next(it)z\n",
    "\n",
    "img_A = d1[fa_key]\n",
    "img_B = d2[fa_key]\n",
    "\n",
    "# img_A, img_B  = affine_aug(img_A, img_B)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    model_output = model(img_A, img_B)\n",
    "\n",
    "img_B_warped = warp(img_B, model_output.deformation_AB)\n",
    "\n",
    "preview_slices = (80,80,80)\n",
    "\n",
    "print(\"moving:\")\n",
    "util.preview_image(img_B[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"warped moving:\")\n",
    "util.preview_image(img_B_warped[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"target:\")\n",
    "util.preview_image(img_A[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"checkerboard of warped moving and target:\")\n",
    "util.preview_checkerboard(img_A[0,0].cpu(), img_B_warped[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"deformation vector field:\")\n",
    "util.preview_3D_vector_field(model_output.deformation_AB[0].cpu(), slices=preview_slices)\n",
    "print(\"deformed grid:\")\n",
    "util.preview_3D_deformation(model_output.deformation_AB[0].cpu(),5, slices=preview_slices)\n",
    "print(\"jacobian determinant:\")\n",
    "det = util.jacobian_determinant(model_output.deformation_AB[0].cpu())\n",
    "util.preview_image(det, normalize_by='slice', threshold=0, slices=preview_slices)\n",
    "print(\"sim loss:\", model_output.sim_loss.item())\n",
    "print(\"(grad?)icon loss:\", model_output.icon_loss.item())\n",
    "print(\"overall loss:\", model_output.all_loss.item())\n",
    "num_folds = (det<0).sum()\n",
    "print(\"Number of folds:\", num_folds, f\"(folding rate {100*num_folds/np.prod(det.shape)}%)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
