{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee94cda0",
   "metadata": {},
   "source": [
    "GradICON deformable registration of FA images. (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22a72721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import monai\n",
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "import footsteps\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2129f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "spatial_size = (144,144,144)\n",
    "num_scales = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8ecdb9",
   "metadata": {},
   "source": [
    "The input images are known to be $140\\times140\\times140$, and we will pad them out to $144$ in each dimension.\n",
    "\n",
    "We assume that each dimension in `spatial_size` is divisible by $2^{\\texttt{num}\\_\\texttt{scales}-1}$, because we will downsample by a factor of $2$ a bunch of times to produce images at different scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7490849",
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_dir = 'dti_fit_images/fa'\n",
    "fa_keys = [f'fa{i}' for i in range(num_scales)]\n",
    "fa_key = fa_keys[0] # a simpler way to refer to the first element of fa_keys\n",
    "data = [{fa_key:path, \"filename\":os.path.basename(path)} for path in glob.glob(os.path.join(fa_dir,'*'))]\n",
    "data_train, data_valid = monai.data.utils.partition_dataset(data, ratios=(8,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f6318",
   "metadata": {},
   "source": [
    "`fa_keys` is a list mapping index to key for scale at that index: $0$ is the base resolution, $1$ is downscaled by a factor of $2$, $2$ is further downscaled by a factor of $2$, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b368d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_transforms = [\n",
    "    monai.transforms.LoadImageD(keys=fa_key),\n",
    "    monai.transforms.AddChannelD(keys=fa_key),\n",
    "    monai.transforms.SpatialPadD(keys=fa_key, spatial_size=spatial_size, mode=\"constant\"),\n",
    "    monai.transforms.ToTensorD(keys=fa_key),\n",
    "    monai.transforms.ToDeviceD(keys=fa_key, device=device),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cc2c3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control the overall scale of affine transform\n",
    "a=0.5\n",
    "\n",
    "S = spatial_size[0]\n",
    "\n",
    "rand_affine_params = {\n",
    "    'prob':1.,\n",
    "    'mode': 'bilinear',\n",
    "    'padding_mode': 'zeros',\n",
    "    'spatial_size':spatial_size,\n",
    "    'cache_grid':True,\n",
    "    'rotate_range': (a*np.pi/2,)*3,\n",
    "    'shear_range': (0,)*6, # no shearing\n",
    "    'translate_range': (a*S/16,)*3,\n",
    "    'scale_range': (a*0.4,)*3,\n",
    "}\n",
    "\n",
    "rand_affine_transform = monai.transforms.RandAffineD(keys=fa_key, **rand_affine_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01570643",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_scales_transforms = [\n",
    "    monai.transforms.CopyItemsD(keys=[fa_key], times=(num_scales-1), names=fa_keys[1:])\n",
    "]\n",
    "add_scales_transforms += [\n",
    "    monai.transforms.ResizeD(keys=[fa_keys[i]], spatial_size=[s//2**i for s in spatial_size])\n",
    "    for i in range(1,num_scales)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e759b26",
   "metadata": {},
   "source": [
    "The `add_scales_transforms` is a chain of transforms that adds downsampled versions of the base images, with keys coming from `fa_keys`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ed049b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_valid = monai.transforms.Compose(base_transforms + add_scales_transforms)\n",
    "transform_train = monai.transforms.Compose(base_transforms + [rand_affine_transform] + add_scales_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2206482e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 68/68 [00:05<00:00, 12.83it/s]\n",
      "Loading dataset: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:00<00:00, 25.98it/s]\n"
     ]
    }
   ],
   "source": [
    "ds_train = monai.data.CacheDataset(data_train, transform_train)\n",
    "ds_valid = monai.data.CacheDataset(data_valid, transform_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10e54a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monai.networks.blocks.Warp: Using PyTorch native grid_sample.\n"
     ]
    }
   ],
   "source": [
    "warp = monai.networks.blocks.Warp(mode=\"bilinear\", padding_mode=\"zeros\")\n",
    "\n",
    "def mse_loss(b1, b2):\n",
    "    \"\"\"Return image similarity loss given two batches b1 and b2 of shape (batch_size, channels, H,W,D).\n",
    "    It is scaled up a bit here.\"\"\"\n",
    "    return 10000*((b1-b2)**2).mean()\n",
    "\n",
    "def ncc_loss(b1, b2):\n",
    "    \"\"\"Return the negative NCC loss given two batches b1 and b2 of shape (batch_size, channels, H,W,D).\n",
    "    It is averaged over batches and channels.\"\"\"\n",
    "    mu1 = b1.mean(dim=(2,3,4)) # means\n",
    "    mu2 = b2.mean(dim=(2,3,4))\n",
    "    alpha1 = (b1**2).mean(dim=(2,3,4)) # second moments\n",
    "    alpha2 = (b2**2).mean(dim=(2,3,4))\n",
    "    alpha12 = (b1*b2).mean(dim=(2,3,4)) # cross term\n",
    "    numerator = alpha12 - mu1*mu2\n",
    "    denominator = torch.sqrt((alpha1 - mu1**2) * (alpha2-mu2**2))\n",
    "    ncc = numerator / denominator\n",
    "    return -ncc.mean() # average over batches and channels\n",
    "\n",
    "def compose_ddf(u,v):\n",
    "    \"\"\"Compose two displacement fields, return the displacement that warps by v followed by u\"\"\"\n",
    "    return u + warp(v,u)\n",
    "\n",
    "_, H, W, D = ds_train[0][fa_key].shape\n",
    "\n",
    "# Compute discrete spatial derivatives\n",
    "def diff_and_trim(array, axis):\n",
    "    \"\"\"Take the discrete difference along a spatial axis, which should be 2,3, or 4.\n",
    "    Return a difference tensor with all spatial axes trimmed by 1.\"\"\"\n",
    "    return torch.diff(array, axis=axis)[:, :, :(H-1), :(W-1), :(D-1)]\n",
    "\n",
    "def size_of_spatial_derivative(u):\n",
    "    \"\"\"Return the squared Frobenius norm of the spatial derivative of the given displacement field.\n",
    "    To clarify, this is about the derivative of the actual displacement field map, not the deformation\n",
    "    that the displacement field map defines. The expected input shape is (batch,3,H,W,D).\n",
    "    Output shape is (batch).\"\"\"\n",
    "    dx = diff_and_trim(u, 2)\n",
    "    dy = diff_and_trim(u, 3)\n",
    "    dz = diff_and_trim(u, 4)\n",
    "    return(dx**2 + dy**2 + dz**2).sum(axis=1).mean(axis=[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0c0d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelOutput = namedtuple(\"ModelOutput\", \"all_loss, sim_loss, gradicon_loss, deformation_AB\")\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, lambda_reg, compute_sim_loss):\n",
    "        super().__init__()\n",
    "        self.reg_net = monai.networks.nets.UNet(\n",
    "            3,  # spatial dims\n",
    "            2,  # input channels (one for fixed image and one for moving image)\n",
    "            3,  # output channels (to represent 3D displacement vector field)\n",
    "            (32, 32, 32, 32, 64),  # channel sequence\n",
    "            (2, 2, 2, 2),  # convolutional strides\n",
    "            dropout=0.2,\n",
    "            norm=\"batch\"\n",
    "        )\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.compute_sim_loss = compute_sim_loss\n",
    "    \n",
    "    def update_lambda_reg(self, new_lambda_reg):\n",
    "        self.lambda_reg = new_lambda_reg\n",
    "\n",
    "    def forward(self, img_A, img_B) -> ModelOutput:\n",
    "        img_pair_AB = torch.cat((img_A, img_B), dim=1)\n",
    "        img_pair_BA = img_pair_AB[:,[1,0]]\n",
    "\n",
    "        deformation_AB = self.reg_net(img_pair_AB) # deforms img_B to the space of img_A\n",
    "        deformation_BA = self.reg_net(img_pair_BA) # deforms img_A to the space of img_B\n",
    "\n",
    "        img_B_warped = warp(img_B, deformation_AB)\n",
    "        img_A_warped = warp(img_A, deformation_BA)\n",
    "        sim_loss_A = self.compute_sim_loss(img_A, img_B_warped)\n",
    "        sim_loss_B = self.compute_sim_loss(img_B, img_A_warped)\n",
    "        composite_deformation_A = compose_ddf(deformation_AB, deformation_BA)\n",
    "        composite_deformation_B = compose_ddf(deformation_BA, deformation_AB)\n",
    "        gradicon_loss_A = size_of_spatial_derivative(composite_deformation_A).mean()\n",
    "        gradicon_loss_B = size_of_spatial_derivative(composite_deformation_B).mean()\n",
    "        \n",
    "        sim_loss = sim_loss_A + sim_loss_B\n",
    "        gradicon_loss = gradicon_loss_A + gradicon_loss_B\n",
    "        \n",
    "        return ModelOutput(\n",
    "            all_loss = sim_loss + self.lambda_reg * gradicon_loss,\n",
    "            sim_loss = sim_loss,\n",
    "            gradicon_loss = gradicon_loss,\n",
    "            deformation_AB = deformation_AB\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "674e8f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A resize transform that operates on batches of images\n",
    "class BatchResizer:\n",
    "    def __init__(self, spatial_dims):\n",
    "        self.resize = monai.transforms.Resize(spatial_dims)\n",
    "    def __call__(self, batch):\n",
    "        return torch.stack([self.resize(x) for x in monai.transforms.Decollated()(batch)])\n",
    "    \n",
    "# A multiscale version of the Model idea above\n",
    "class MultiscaleModel(torch.nn.Module):\n",
    "    def __init__(self, lambda_reg, compute_sim_loss, num_subnetworks):\n",
    "        super().__init__()\n",
    "        self.num_subnetworks = num_subnetworks\n",
    "        self.reg_nets = torch.nn.ModuleList()\n",
    "        for i in range(num_subnetworks):\n",
    "            # i is scale. i=0 is the original input image scale. Scale i is at a downsample factor of 2**i.\n",
    "            n = 4 # Amount of down-convolution for the original image size.\n",
    "            # (We will assume that the original image size is divisible by 2**n.)\n",
    "            num_twos = n-i # The number of 2's we will put in the sequence of convolutional strides.\n",
    "            num_ones = min(i,n-i) # The number of 1's\n",
    "            stride_sequence = (1,2)*num_ones + (2,)*(num_twos-num_ones)\n",
    "            channel_sequence = [min(8*2**c,64) for c in range(num_twos+num_ones+1)]\n",
    "            self.reg_nets.append(\n",
    "                monai.networks.nets.UNet(\n",
    "                    3,  # spatial dims\n",
    "                    2,  # input channels (one for fixed image and one for moving image)\n",
    "                    3,  # output channels (to represent 3D displacement vector field)\n",
    "                    channel_sequence,\n",
    "                    stride_sequence,\n",
    "                    dropout=0.2,\n",
    "                    norm=\"batch\"\n",
    "                )\n",
    "            )\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.compute_sim_loss = compute_sim_loss\n",
    "        \n",
    "        self.batch_resizers = [BatchResizer([s//2**i for s in spatial_size]) for i in range(num_subnetworks-1)]\n",
    "    \n",
    "    def update_lambda_reg(self, new_lambda_reg):\n",
    "        self.lambda_reg = new_lambda_reg\n",
    "        \n",
    "    def multiscale_reg_nets(self, img_A, img_B):\n",
    "        \"\"\"\n",
    "        Here we expect img_A to be a list consisting of batches of target images:\n",
    "            img_A[0] is a batch of target images at the original resolution,\n",
    "            img_A[1] is a batch of target images downsampled by a factor of 2 in each dimension,\n",
    "            img_A[2] is a batch of target images downsampled by a factor of 4 in each dimension,\n",
    "            etc.\n",
    "        and similarly img_B is a list consisting of batches of moving images.\n",
    "        Returns the final displacement field (composed over all scales) for deforming img_B[0] to img_A[0].\n",
    "        \"\"\"\n",
    "        \n",
    "        i = self.num_subnetworks - 1\n",
    "        phi = self.reg_nets[i](torch.cat([img_A[i], img_B[i]], dim=1)) # Warp from scale i, operating at scale i\n",
    "        phi_up = self.batch_resizers[i-1](phi) # Warp from scale i, operating at scale i-1\n",
    "        \n",
    "        for i in range(self.num_subnetworks - 1, -1, -1): # Run backwards to 0 from num_subnetworks-1\n",
    "            \n",
    "            # phi_up = Composite of warps up to scale i+1, operating at scale i\n",
    "            if i==self.num_subnetworks - 1:\n",
    "                pass # Base case: phi_up is the identity map.\n",
    "                # (We treat this case specially below to avoid complicating the computational graph with\n",
    "                # useless compositions with identity map)\n",
    "            else:\n",
    "                phi_up = self.batch_resizers[i](phi_comp)\n",
    "            \n",
    "            # warped_B = img_B at scale i warped by the composite of warps up to scale i+1\n",
    "            if i==self.num_subnetworks - 1:\n",
    "                warped_B = img_B[i] # Base case: \"the composite of warps up to scale i+1\" = the identity map\n",
    "            else:\n",
    "                warped_B = warp(img_B[i], phi_up)\n",
    "            \n",
    "            # phi = Warp from scale i, operating at scale i\n",
    "            phi = self.reg_nets[i](torch.cat([img_A[i], warped_B], dim=1))\n",
    "\n",
    "            # phi_comp = Composite of warps up to scale i, operating at scale i\n",
    "            if i==self.num_subnetworks - 1:\n",
    "                phi_comp = phi # Base case: phi_up = the identity map, i.e. chain to compose consists of phi only\n",
    "            else:\n",
    "                phi_comp = compose_ddf(phi,phi_up)\n",
    "        \n",
    "        return phi_comp\n",
    "        \n",
    "    def forward(self, img_A, img_B) -> ModelOutput:\n",
    "        \"\"\"\n",
    "        Here we expect img_A to be a list consisting of batches of target images:\n",
    "            img_A[0] is a batch of target images at the original resolution,\n",
    "            img_A[1] is a batch of target images downsampled by a factor of 2 in each dimension,\n",
    "            img_A[2] is a batch of target images downsampled by a factor of 4 in each dimension,\n",
    "            etc.\n",
    "        and similarly img_B is a list consisting of batches of moving images.\n",
    "        \"\"\"\n",
    "        deformation_AB = self.multiscale_reg_nets(img_A, img_B) # deforms img_B to the space of img_A\n",
    "        deformation_BA = self.multiscale_reg_nets(img_B, img_A) # deforms img_A to the space of img_B\n",
    "\n",
    "        img_B0_warped = warp(img_B[0], deformation_AB)\n",
    "        img_A0_warped = warp(img_A[0], deformation_BA)\n",
    "        sim_loss_A = self.compute_sim_loss(img_A[0], img_B0_warped)\n",
    "        sim_loss_B = self.compute_sim_loss(img_B[0], img_A0_warped)\n",
    "        composite_deformation_A = compose_ddf(deformation_AB, deformation_BA)\n",
    "        composite_deformation_B = compose_ddf(deformation_BA, deformation_AB)\n",
    "        gradicon_loss_A = size_of_spatial_derivative(composite_deformation_A).mean()\n",
    "        gradicon_loss_B = size_of_spatial_derivative(composite_deformation_B).mean()\n",
    "        \n",
    "        sim_loss = sim_loss_A + sim_loss_B\n",
    "        gradicon_loss = gradicon_loss_A + gradicon_loss_B\n",
    "        \n",
    "        return ModelOutput(\n",
    "            all_loss = sim_loss + self.lambda_reg * gradicon_loss,\n",
    "            sim_loss = sim_loss,\n",
    "            gradicon_loss = gradicon_loss,\n",
    "            deformation_AB = deformation_AB\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e2010a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JacobianDeterminant(torch.nn.Module):\n",
    "    \"\"\"Given a batch of displacement vector fields vf, compute the jacobian determinant scalar field.\"\"\"\n",
    "\n",
    "    def __init__(self, spatial_dims):\n",
    "        super().__init__()\n",
    "        self.spatial_dims = spatial_dims\n",
    "    def diff_and_trim(self, array, axis):\n",
    "        H,W,D = self.spatial_dims\n",
    "        return torch.diff(array, axis=axis)[:, :, :(H-1), :(W-1), :(D-1)]\n",
    "\n",
    "    def forward(self, vf):\n",
    "        \"\"\"\n",
    "        vf is assumed to be a vector field of shape (B,3,H,W,D),\n",
    "        and it is interpreted as a displacement field.\n",
    "        So it is defining a batch of discretely sampled maps from a subset of 3-space into 3-space,\n",
    "        namely (for batch index b) the map that sends point (x,y,z) to the point (x,y,z)+vf[b,:,x,y,z].\n",
    "        This function computes a jacobian determinant by taking discrete differences in each spatial direction.\n",
    "\n",
    "        Returns a numpy array of shape (b,H-1,W-1,D-1).\n",
    "        \"\"\"\n",
    "        dx = self.diff_and_trim(vf, 2)\n",
    "        dy = self.diff_and_trim(vf, 3)\n",
    "        dz = self.diff_and_trim(vf, 4)\n",
    "        \n",
    "        # Add derivative of identity map\n",
    "        dx[:,0] += 1\n",
    "        dy[:,1] += 1\n",
    "        dz[:,2] += 1\n",
    "\n",
    "        # Compute determinant at each spatial location\n",
    "        det = dx[:,0]*(dy[:,1]*dz[:,2]-dz[:,1]*dy[:,2]) - dy[:,0]*(dx[:,1]*dz[:,2] -\n",
    "                                                dz[:,1]*dx[:,2]) + dz[:,0]*(dx[:,1]*dy[:,2]-dy[:,1]*dx[:,2])\n",
    "\n",
    "        return det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cfaa119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossCurves:\n",
    "    def __init__(self, name : str, include_folds : bool = False, spatial_dims : tuple = None):\n",
    "        self.name = name\n",
    "        \n",
    "        self.epochs =[]\n",
    "        self.all_losses = []\n",
    "        self.sim_losses = []\n",
    "        self.gradicon_losses = []\n",
    "        \n",
    "        self.include_folds = include_folds\n",
    "        if include_folds:\n",
    "            if spatial_dims is None:\n",
    "                raise Exception(\"Need argument spatial_dims to include fold count.\")\n",
    "            self.jacobian_determinant = JacobianDeterminant(spatial_dims)\n",
    "            self.fold_counts = []\n",
    "        \n",
    "        self.clear_buffers()\n",
    "        \n",
    "    def clear_buffers(self):\n",
    "        self.all_losses_buffer = []\n",
    "        self.sim_losses_buffer = []\n",
    "        self.gradicon_losses_buffer = []\n",
    "        \n",
    "        if self.include_folds:\n",
    "            self.fold_counts_buffer = []\n",
    "        \n",
    "    def add_to_buffer(self, model_output : ModelOutput):\n",
    "        self.all_losses_buffer.append(model_output.all_loss.item())\n",
    "        self.sim_losses_buffer.append(model_output.sim_loss.item())\n",
    "        self.gradicon_losses_buffer.append(model_output.gradicon_loss.item())\n",
    "        \n",
    "        if self.include_folds:\n",
    "            det = self.jacobian_determinant(model_output.deformation_AB)\n",
    "            num_folds = (det<0).sum(dim=(1,2,3))\n",
    "            num_folds_mean = num_folds.to(dtype=torch.float).mean().item() # average over batch\n",
    "            self.fold_counts_buffer.append(num_folds_mean)\n",
    "        \n",
    "    def aggregate_buffers_for_epoch(self, epoch : int):\n",
    "        self.epochs.append(epoch)\n",
    "        self.all_losses.append(np.mean(self.all_losses_buffer))\n",
    "        self.sim_losses.append(np.mean(self.sim_losses_buffer))\n",
    "        self.gradicon_losses.append(np.mean(self.gradicon_losses_buffer))\n",
    "        if self.include_folds:\n",
    "            self.fold_counts.append(np.mean(self.fold_counts_buffer))\n",
    "        self.clear_buffers()\n",
    "        \n",
    "    def plot(self, savepath=None):\n",
    "        fig, axs = plt.subplots(1,3 if not self.include_folds else 4,figsize = (15,5))\n",
    "        axs[0].plot(self.epochs, self.all_losses)\n",
    "        axs[0].set_title(f\"{self.name}: overall loss\")\n",
    "        axs[1].plot(self.epochs, self.sim_losses)\n",
    "        axs[1].set_title(f\"{self.name}: similarity loss\")\n",
    "        axs[2].plot(self.epochs, self.gradicon_losses, label=\"gradicon loss\")\n",
    "        axs[2].set_title(f\"{self.name}: gradicon loss\")\n",
    "        if self.include_folds:\n",
    "            axs[3].plot(self.epochs, self.fold_counts, label=\"average folds\")\n",
    "            axs[3].set_title(f\"{self.name}: average folds\")\n",
    "        for ax in axs:\n",
    "            ax.set_xlabel(\"epoch\")\n",
    "        if savepath is not None:\n",
    "            plt.savefig(savepath)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "217b30b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lncc_loss = monai.losses.LocalNormalizedCrossCorrelationLoss(\n",
    "    spatial_dims=3,\n",
    "    kernel_size=5,\n",
    "    smooth_nr = 0,\n",
    "    smooth_dr = 1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31229ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_scales_list(b):\n",
    "    \"\"\"Given a batch from dl_train or dl_valid, return the list of images at different scales that\n",
    "    would be suitable as input to a MultiscaleModel.\"\"\"\n",
    "    return list(map(lambda k : b[k], fa_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "732bfeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = monai.data.DataLoader(ds_train, shuffle=True, batch_size=1, drop_last=True)\n",
    "dl_valid = monai.data.DataLoader(ds_valid, shuffle=True, batch_size=2, drop_last=True)\n",
    "max_epochs = 300\n",
    "validate_when = lambda e : ((e%2==0) and (e!=0)) or (e==max_epochs-1)\n",
    "lambda_reg_step_size = 0.01 # How much to increase lambda_reg each time it advances\n",
    "cooldown = 5 # How many epochs to allow before checking whether training loss increases and advacing lambda_reg if so\n",
    "cooldown_counter = cooldown\n",
    "lambda_reg_goal = 0.2 # Stop training once lambda_reg advances past this\n",
    "model = MultiscaleModel(\n",
    "    lambda_reg = 0,\n",
    "    compute_sim_loss = ncc_loss,\n",
    "    num_subnetworks=num_scales\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# min_lr=1e-5\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.954992586)\n",
    "loss_curves_train = LossCurves(\"training\")\n",
    "loss_curves_valid = LossCurves(\"validation\", include_folds=True, spatial_dims=(144,144,144))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fb14cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TESTING; DELETE THIS CELL\n",
    "\n",
    "it = iter(dl_train)\n",
    "d1 = next(it)\n",
    "d2 = next(it)\n",
    "a = list(map(lambda k : d1[k], fa_keys))\n",
    "b = list(map(lambda k : d2[k], fa_keys))\n",
    "model(a,b).all_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e309a14",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300 (LR = 1.0e-03, lambda_reg = 0.0e+00):\n",
      "\tTraining loss: -1.1175 (-1.1175,43.7104)\n",
      "Epoch 2/300 (LR = 1.0e-03, lambda_reg = 0.0e+00):\n",
      "\tTraining loss: -1.2127 (-1.2127,43.2884)\n",
      "Epoch 3/300 (LR = 1.0e-03, lambda_reg = 0.0e+00):\n",
      "\tTraining loss: -1.2808 (-1.2808,42.9596)\n",
      "\tValidation loss: -1.5844969153404236\n",
      "\tAverage folds: 152591.75\n",
      "Epoch 4/300 (LR = 1.0e-03, lambda_reg = 0.0e+00):\n",
      "\tTraining loss: -1.2664 (-1.2664,42.3718)\n",
      "Epoch 5/300 (LR = 1.0e-03, lambda_reg = 0.0e+00):\n",
      "\tTraining loss: -1.3154 (-1.3154,42.5186)\n",
      "\tValidation loss: -1.6106345355510712\n",
      "\tAverage folds: 145397.25\n",
      "Epoch 6/300 (LR = 1.0e-03, lambda_reg = 0.0e+00):\n",
      "\tTraining loss: -1.3178 (-1.3178,42.3591)\n",
      "Epoch 7/300 (LR = 1.0e-03, lambda_reg = 0.0e+00):\n",
      "\tTraining loss: -1.3027 (-1.3027,42.6714)\n",
      "\tValidation loss: -1.6265175342559814\n",
      "\tAverage folds: 147743.625\n",
      "Updating lambda_reg.\n",
      "Epoch 8/300 (LR = 1.0e-03, lambda_reg = 1.0e-02):\n",
      "\tTraining loss: -0.9415 (-1.2867,34.5146)\n",
      "Epoch 9/300 (LR = 1.0e-03, lambda_reg = 1.0e-02):\n",
      "\tTraining loss: -1.0771 (-1.2584,18.1326)\n",
      "\tValidation loss: -1.4713642299175262\n",
      "\tAverage folds: 124054.25\n",
      "Epoch 10/300 (LR = 1.0e-03, lambda_reg = 1.0e-02):\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "\n",
    "for e in range(max_epochs):\n",
    "#     current_lr = scheduler.get_last_lr()[0]\n",
    "    current_lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    print(f'Epoch {e+1}/{max_epochs} (LR = {current_lr:.1e}, lambda_reg = {model.lambda_reg:.1e}):')\n",
    "    \n",
    "    # Train\n",
    "    model.train()\n",
    "    dl_train_iter = iter(dl_train)\n",
    "    while True:\n",
    "        try:\n",
    "            b1 = next(dl_train_iter)\n",
    "            b2 = next(dl_train_iter)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        model_output = model(batch_to_scales_list(b1), batch_to_scales_list(b2))\n",
    "        model_output.all_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_curves_train.add_to_buffer(model_output)\n",
    "        del(model_output)\n",
    "\n",
    "#     if scheduler.get_last_lr()[0] > min_lr:\n",
    "#         scheduler.step()\n",
    "\n",
    "    loss_curves_train.aggregate_buffers_for_epoch(e)\n",
    "    print(f\"\\tTraining loss: {loss_curves_train.all_losses[-1]:.4f} (sim={loss_curves_train.sim_losses[-1]:.4f}, ic={loss_curves_train.gradicon_losses[-1]:.4f})\")\n",
    "    \n",
    "    # Validate\n",
    "    if validate_when(e):\n",
    "        model.eval()\n",
    "        dl_valid_iter = iter(dl_valid)\n",
    "        while True:\n",
    "            try:\n",
    "                b1 = next(dl_valid_iter)\n",
    "                b2 = next(dl_valid_iter)\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model_output = model(batch_to_scales_list(b1), batch_to_scales_list(b2))\n",
    "                loss_curves_valid.add_to_buffer(model_output)\n",
    "        loss_curves_valid.aggregate_buffers_for_epoch(e) \n",
    "        print(\"\\tValidation loss:\", loss_curves_valid.all_losses[-1])\n",
    "        print(\"\\tAverage folds:\", loss_curves_valid.fold_counts[-1])\n",
    "    \n",
    "    cooldown_counter -= 1;\n",
    "    if cooldown_counter<=0 and loss_curves_train.all_losses[-1] > loss_curves_train.all_losses[-2]:\n",
    "        print(f\"Updating lambda_reg.\")\n",
    "        model.update_lambda_reg(model.lambda_reg + lambda_reg_step_size)\n",
    "        cooldown_counter = cooldown # reset cooldown_counter\n",
    "    if model.lambda_reg > lambda_reg_goal:\n",
    "        print(\"Reached goal lambda_reg.\")\n",
    "        break\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd70383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE\n",
    "\n",
    "loss_curves_train.plot(savepath = footsteps.output_dir + 'loss_plot_train.png')\n",
    "loss_curves_valid.plot(savepath = footsteps.output_dir + 'loss_plot_valid.png')\n",
    "\n",
    "with open(footsteps.output_dir + 'loss_curves.p', 'wb') as f:\n",
    "    pickle.dump([loss_curves_train, loss_curves_valid],f)\n",
    "\n",
    "torch.save(model.state_dict(), footsteps.output_dir + 'model_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e32faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD\n",
    "\n",
    "experiment_name_to_load = \"increase lambda_reg over schedule\"\n",
    "load_dir = os.path.join('results', experiment_name_to_load)\n",
    "\n",
    "model.load_state_dict(torch.load(os.path.join(load_dir,'model_state_dict.pth')))\n",
    "\n",
    "with open(os.path.join(load_dir,'loss_curves.p'), 'rb') as f:\n",
    "    loss_curves_train, loss_curves_valid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42565b2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import util\n",
    "\n",
    "ds = ds_train # Choose whether to view performance on training or on validation data\n",
    "d1 = random.choice(ds)\n",
    "d2 = random.choice(ds)\n",
    "\n",
    "img_A = d1[fa_key].unsqueeze(0)\n",
    "img_B = d2[fa_key].unsqueeze(0)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    model_output = model(img_A,img_B)\n",
    "\n",
    "img_B_warped = warp(img_B, model_output.deformation_AB)\n",
    "\n",
    "preview_slices = (80,80,80)\n",
    "\n",
    "print(\"moving:\")\n",
    "util.preview_image(img_B[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"warped moving:\")\n",
    "util.preview_image(img_B_warped[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"target:\")\n",
    "util.preview_image(img_A[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"checkerboard of warped moving and target:\")\n",
    "util.preview_checkerboard(img_A[0,0].cpu(), img_B_warped[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"deformation vector field:\")\n",
    "util.preview_3D_vector_field(model_output.deformation_AB[0].cpu(), slices=preview_slices)\n",
    "print(\"deformed grid:\")\n",
    "util.preview_3D_deformation(model_output.deformation_AB[0].cpu(),5, slices=preview_slices)\n",
    "print(\"jacobian determinant:\")\n",
    "det = util.jacobian_determinant(model_output.deformation_AB[0].cpu())\n",
    "util.preview_image(det, normalize_by='slice', threshold=0, slices=preview_slices)\n",
    "print(\"sim loss:\", model_output.sim_loss.item())\n",
    "print(\"gradicon loss:\", model_output.gradicon_loss.item())\n",
    "print(\"overall loss:\", model_output.all_loss.item())\n",
    "num_folds = (det<0).sum()\n",
    "print(\"Number of folds:\", num_folds, f\"(folding rate {100*num_folds/np.prod(det.shape)}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008bb66a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
