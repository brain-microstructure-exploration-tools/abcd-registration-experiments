{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee94cda0",
   "metadata": {},
   "source": [
    "ICON or GradICON deformable registration of DTI images. (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a72721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import shutil\n",
    "from collections import namedtuple\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import monai\n",
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "import footsteps\n",
    "import pickle\n",
    "import util\n",
    "\n",
    "from dti_warp import WarpDTI, TensorTransformType, MseLossDTI\n",
    "from util import ComposeDDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2129f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "spatial_size = (144,144,144)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8ecdb9",
   "metadata": {},
   "source": [
    "The input images are known to be $140\\times140\\times140$, and we can pad them out to $144$ in each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7490849",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/data/ebrahim-data/abcd/registration-experiments/ISBI-2023-project/dti_fit_images_nontest/')\n",
    "fa_dir = data_dir/'fa'\n",
    "dti_dir = data_dir/'dti'\n",
    "data = [{'dti':str(path), 'fa':str(path.parent.parent/'fa'/path.name), \"filename\":path.name} for path in dti_dir.glob('*')]\n",
    "data_train, data_valid = monai.data.utils.partition_dataset(data, ratios=(8,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b368d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = ['fa', 'dti']\n",
    "\n",
    "base_transforms = [\n",
    "    monai.transforms.LoadImageD(keys=k),\n",
    "    monai.transforms.EnsureChannelFirstD(keys=k),\n",
    "    monai.transforms.SpatialPadD(keys=k, spatial_size=spatial_size, mode=\"constant\"),\n",
    "    monai.transforms.ToTensorD(keys=k),\n",
    "    monai.transforms.ToDeviceD(keys=k, device=device),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed049b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_valid = monai.transforms.Compose(base_transforms)\n",
    "transform_train = monai.transforms.Compose(base_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e54a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(b1, b2):\n",
    "    \"\"\"Return image similarity loss given two batches b1 and b2 of shape (batch_size, channels, H,W,D).\n",
    "    It is scaled up a bit here.\"\"\"\n",
    "    return 10000*((b1-b2)**2).mean()\n",
    "\n",
    "def mse_tensors(b1, b2):\n",
    "    \"\"\"Return the mean of the squared distances between tensors in two batches of DTIs,\n",
    "    each of shape (batch_size, channels, H,W,D).\"\"\"\n",
    "    return blehsdfjsadflkjahsdf\n",
    "\n",
    "def ncc_loss(b1, b2):\n",
    "    \"\"\"Return the negative NCC loss given two batches b1 and b2 of shape (batch_size, channels, H,W,D).\n",
    "    It is averaged over batches and channels.\"\"\"\n",
    "    mu1 = b1.mean(dim=(2,3,4)) # means\n",
    "    mu2 = b2.mean(dim=(2,3,4))\n",
    "    alpha1 = (b1**2).mean(dim=(2,3,4)) # second moments\n",
    "    alpha2 = (b2**2).mean(dim=(2,3,4))\n",
    "    alpha12 = (b1*b2).mean(dim=(2,3,4)) # cross term\n",
    "    numerator = alpha12 - mu1*mu2\n",
    "    denominator = torch.sqrt((alpha1 - mu1**2) * (alpha2-mu2**2))\n",
    "    ncc = numerator / denominator\n",
    "    return -ncc.mean() # average over batches and channels\n",
    "\n",
    "H, W, D = spatial_size\n",
    "\n",
    "# Compute discrete spatial derivatives\n",
    "def diff_and_trim(array, axis):\n",
    "    \"\"\"Take the discrete difference along a spatial axis, which should be 2,3, or 4.\n",
    "    Return a difference tensor with all spatial axes trimmed by 1.\"\"\"\n",
    "    return torch.diff(array, axis=axis)[:, :, :(H-1), :(W-1), :(D-1)]\n",
    "\n",
    "def size_of_spatial_derivative(u):\n",
    "    \"\"\"Return the squared Frobenius norm of the spatial derivative of the given displacement field.\n",
    "    To clarify, this is about the derivative of the actual displacement field map, not the deformation\n",
    "    that the displacement field map defines. The expected input shape is (batch,3,H,W,D).\n",
    "    Output shape is (batch).\"\"\"\n",
    "    dx = diff_and_trim(u, 2)\n",
    "    dy = diff_and_trim(u, 3)\n",
    "    dz = diff_and_trim(u, 4)\n",
    "    return(dx**2 + dy**2 + dz**2).sum(axis=1).mean(axis=[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c0d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import Union\n",
    "\n",
    "ModelOutput = namedtuple(\"ModelOutput\", \"all_loss, sim_loss, icon_loss, deformation_AB, sim_loss_weighted, icon_loss_weighted\")\n",
    "\n",
    "class IconLossType(Enum):\n",
    "    ICON = 0\n",
    "    GRADICON = 1\n",
    "    \n",
    "# A deformable registration model\n",
    "class RegModel(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 device,\n",
    "                 lambda_reg,\n",
    "                 lambda_sim,\n",
    "                 down_convolutions,\n",
    "                 depth,\n",
    "                 max_channels,\n",
    "                 init_channels,\n",
    "                 icon_loss_type : IconLossType = IconLossType.GRADICON,\n",
    "                 downsample_early = True\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Create a deformable registration network\n",
    "        \n",
    "        Args:\n",
    "            device:\n",
    "            lambda_reg: Hyperparameter for weight of icon/gradicon loss\n",
    "            lambda_sim: Hyperparameter for weight of similarity loss (not independent from lambda_reg so\n",
    "                not really a new hyperparameter)\n",
    "            compute_sim_loss: A function that compares two batches of images and returns a similarity loss\n",
    "            down_convolutions: How many stride=2 convolutions to include in the down-convolution part of the unets\n",
    "                               when at the original image scale. We assume the original image size is divisible by\n",
    "                               2**down_convolutions\n",
    "            depth: Total number of layers in half of the unet. Increase this to increase model capacity.\n",
    "                   Must be >= down_convolutions\n",
    "            max_channels: As you go to deeper layers, channels grow by powers of two... up to a maximum given here.\n",
    "            init_channels: how many channels in the first layer\n",
    "            icon_loss_type: whether to use ICON or GradICON\n",
    "            downsample_early: The CNN can contain strided and unstrided convolutions to achieve the requested\n",
    "                              depth; this paramter decides whether to prefer putting strided convolutions earlier\n",
    "                              or later.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.icon_loss_type = icon_loss_type\n",
    "        if depth < down_convolutions:\n",
    "            raise ValueError(\"Must have depth >= down_convolutions\")\n",
    "        # (We will assume that the original image size is divisible by 2**n.)\n",
    "\n",
    "        \n",
    "        \n",
    "        num_twos = down_convolutions # The number of 2's we will put in the sequence of convolutional strides.\n",
    "        num_ones = depth-down_convolutions # The number of 1's\n",
    "        num_one_two_pairs = min(num_ones, num_twos) # The number of 1,2 pairs to stick in the middle\n",
    "        if downsample_early:\n",
    "            stride_sequence = (2,)*(num_twos-num_one_two_pairs) + (1,2)*num_one_two_pairs + (1,)*(num_ones-num_one_two_pairs)\n",
    "        else:\n",
    "            stride_sequence = (1,)*(num_ones-num_one_two_pairs) + (1,2)*num_one_two_pairs + (2,)*(num_twos-num_one_two_pairs)\n",
    "        channel_sequence = [min(init_channels*2**c,max_channels) for c in range(num_twos+num_ones+1)]\n",
    "        \n",
    "        self.reg_net = monai.networks.nets.UNet(\n",
    "            3,  # spatial dims\n",
    "            12, # input channels (6 for lower triangular entries of fixed image and 6 for moving image)\n",
    "            3,  # output channels (to represent 3D displacement vector field)\n",
    "            channel_sequence,\n",
    "            stride_sequence,\n",
    "            dropout=0.2,\n",
    "            norm=\"batch\"\n",
    "        )\n",
    "        self.strides = stride_sequence\n",
    "        self.channels = channel_sequence\n",
    "        \n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.lambda_sim = lambda_sim\n",
    "        self.compute_sim_loss = MseLossDTI(device=device)\n",
    "        \n",
    "        self.warp = WarpDTI(device=device, tensor_transform_type=TensorTransformType.FINITE_STRAIN)\n",
    "        self.compose_ddf = ComposeDDF()\n",
    "        self.to(device)\n",
    "    \n",
    "    def update_lambda_reg(self, new_lambda_reg):\n",
    "        self.lambda_reg = new_lambda_reg\n",
    "        \n",
    "    def forward(self, img_A, img_B, return_warp_only = False) -> Union[ModelOutput,torch.Tensor]:\n",
    "        deformation_AB = self.reg_net(torch.cat([img_A, img_B], dim=1)) # deforms img_B to the space of img_A\n",
    "        if return_warp_only:\n",
    "            return deformation_AB\n",
    "        deformation_BA = self.reg_net(torch.cat([img_B, img_A], dim=1)) # deforms img_A to the space of img_B\n",
    "\n",
    "        img_B_warped = self.warp(img_B, deformation_AB)\n",
    "        img_A_warped = self.warp(img_A, deformation_BA)\n",
    "        sim_loss_A = self.compute_sim_loss(img_A, img_B_warped)\n",
    "        sim_loss_B = self.compute_sim_loss(img_B, img_A_warped)\n",
    "        composite_deformation_A = self.compose_ddf(deformation_AB, deformation_BA)\n",
    "        composite_deformation_B = self.compose_ddf(deformation_BA, deformation_AB)\n",
    "        \n",
    "        if self.icon_loss_type == IconLossType.GRADICON:\n",
    "            icon_loss_A = size_of_spatial_derivative(composite_deformation_A).mean()\n",
    "            icon_loss_B = size_of_spatial_derivative(composite_deformation_B).mean()\n",
    "        elif self.icon_loss_type == IconLossType.ICON:\n",
    "            icon_loss_A = (composite_deformation_A**2).mean()\n",
    "            icon_loss_B = (composite_deformation_B**2).mean()\n",
    "        \n",
    "        sim_loss = sim_loss_A + sim_loss_B\n",
    "        icon_loss = icon_loss_A + icon_loss_B\n",
    "    \n",
    "        return ModelOutput(\n",
    "            all_loss = self.lambda_sim * sim_loss + self.lambda_reg * icon_loss,\n",
    "            sim_loss = sim_loss,\n",
    "            icon_loss = icon_loss,\n",
    "            sim_loss_weighted = self.lambda_sim * sim_loss,\n",
    "            icon_loss_weighted = self.lambda_reg * icon_loss,\n",
    "            deformation_AB = deformation_AB\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2010a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JacobianDeterminant(torch.nn.Module):\n",
    "    \"\"\"Given a batch of displacement vector fields vf, compute the jacobian determinant scalar field.\"\"\"\n",
    "\n",
    "    def __init__(self, spatial_dims):\n",
    "        super().__init__()\n",
    "        self.spatial_dims = spatial_dims\n",
    "    def diff_and_trim(self, array, axis):\n",
    "        H,W,D = self.spatial_dims\n",
    "        return torch.diff(array, axis=axis)[:, :, :(H-1), :(W-1), :(D-1)]\n",
    "\n",
    "    def forward(self, vf):\n",
    "        \"\"\"\n",
    "        vf is assumed to be a vector field of shape (B,3,H,W,D),\n",
    "        and it is interpreted as a displacement field.\n",
    "        So it is defining a batch of discretely sampled maps from a subset of 3-space into 3-space,\n",
    "        namely (for batch index b) the map that sends point (x,y,z) to the point (x,y,z)+vf[b,:,x,y,z].\n",
    "        This function computes a jacobian determinant by taking discrete differences in each spatial direction.\n",
    "\n",
    "        Returns a numpy array of shape (b,H-1,W-1,D-1).\n",
    "        \"\"\"\n",
    "        dx = self.diff_and_trim(vf, 2)\n",
    "        dy = self.diff_and_trim(vf, 3)\n",
    "        dz = self.diff_and_trim(vf, 4)\n",
    "        \n",
    "        # Add derivative of identity map\n",
    "        dx[:,0] += 1\n",
    "        dy[:,1] += 1\n",
    "        dz[:,2] += 1\n",
    "\n",
    "        # Compute determinant at each spatial location\n",
    "        det = dx[:,0]*(dy[:,1]*dz[:,2]-dz[:,1]*dy[:,2]) - dy[:,0]*(dx[:,1]*dz[:,2] -\n",
    "                                                dz[:,1]*dx[:,2]) + dz[:,0]*(dx[:,1]*dy[:,2]-dy[:,1]*dx[:,2])\n",
    "\n",
    "        return det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfaa119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossCurves:\n",
    "    def __init__(self, name : str, include_folds : bool = False, spatial_dims : tuple = None):\n",
    "        self.name = name\n",
    "        \n",
    "        self.epochs =[]\n",
    "        self.all_losses = []\n",
    "        self.sim_losses = []\n",
    "        self.icon_losses = []\n",
    "        \n",
    "        self.include_folds = include_folds\n",
    "        if include_folds:\n",
    "            if spatial_dims is None:\n",
    "                raise Exception(\"Need argument spatial_dims to include fold count.\")\n",
    "            self.jacobian_determinant = JacobianDeterminant(spatial_dims)\n",
    "            self.fold_counts = []\n",
    "        \n",
    "        self.clear_buffers()\n",
    "        \n",
    "    def clear_buffers(self):\n",
    "        self.all_losses_buffer = []\n",
    "        self.sim_losses_buffer = []\n",
    "        self.icon_losses_buffer = []\n",
    "        \n",
    "        if self.include_folds:\n",
    "            self.fold_counts_buffer = []\n",
    "        \n",
    "    def add_to_buffer(self, model_output : ModelOutput):\n",
    "        self.all_losses_buffer.append(model_output.all_loss.item())\n",
    "        self.sim_losses_buffer.append(model_output.sim_loss.item())\n",
    "        self.icon_losses_buffer.append(model_output.icon_loss.item())\n",
    "        \n",
    "        if self.include_folds:\n",
    "            det = self.jacobian_determinant(model_output.deformation_AB)\n",
    "            num_folds = (det<0).sum(dim=(1,2,3))\n",
    "            num_folds_mean = num_folds.to(dtype=torch.float).mean().item() # average over batch\n",
    "            self.fold_counts_buffer.append(num_folds_mean)\n",
    "        \n",
    "    def aggregate_buffers_for_epoch(self, epoch : int):\n",
    "        self.epochs.append(epoch)\n",
    "        self.all_losses.append(np.mean(self.all_losses_buffer))\n",
    "        self.sim_losses.append(np.mean(self.sim_losses_buffer))\n",
    "        self.icon_losses.append(np.mean(self.icon_losses_buffer))\n",
    "        if self.include_folds:\n",
    "            self.fold_counts.append(np.mean(self.fold_counts_buffer))\n",
    "        self.clear_buffers()\n",
    "        \n",
    "    def plot(self, savepath=None):\n",
    "        fig, axs = plt.subplots(1,3 if not self.include_folds else 4,figsize = (15,5))\n",
    "        axs[0].plot(self.epochs, self.all_losses)\n",
    "        axs[0].set_title(f\"{self.name}: overall loss\")\n",
    "        axs[1].plot(self.epochs, self.sim_losses)\n",
    "        axs[1].set_title(f\"{self.name}: similarity loss\")\n",
    "        axs[2].plot(self.epochs, self.icon_losses, label=\"icon loss\")\n",
    "        axs[2].set_title(f\"{self.name}: icon loss\")\n",
    "        if self.include_folds:\n",
    "            axs[3].plot(self.epochs, self.fold_counts, label=\"average folds\")\n",
    "            axs[3].set_title(f\"{self.name}: average folds\")\n",
    "        for ax in axs:\n",
    "            ax.set_xlabel(\"epoch\")\n",
    "        if savepath is not None:\n",
    "            plt.savefig(savepath)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23566e4d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The SVD in the tensor transformations take a lot longer than it takes to copy into GPU,\n",
    "# so there's not much sense in using CacheDataset. Might as well save the GPU memory and use it for model size.\n",
    "\n",
    "cache_dir = Path('./PersistentDatasetCacheDir')\n",
    "if cache_dir.exists():\n",
    "    shutil.rmtree(cache_dir)\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "ds_train = monai.data.PersistentDataset(data_train, transform_train, cache_dir=cache_dir/'train')\n",
    "ds_valid = monai.data.PersistentDataset(data_valid, transform_valid, cache_dir=cache_dir/'valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23821225",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Model creation and training for single scale approach, with affine augmentation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b485f9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = monai.data.DataLoader(ds_train, shuffle=True, batch_size=2, drop_last=True)\n",
    "dl_valid = monai.data.DataLoader(ds_valid, shuffle=True, batch_size=2, drop_last=True)\n",
    "\n",
    "validate_when = lambda e : ((e%5==0) and (e!=0)) or (e==max_epochs-1)\n",
    "print_aggregate_when = lambda e : True\n",
    "last_printed_epoch = -1\n",
    "\n",
    "schedule_lambda_reg = True\n",
    "lambda_reg_step_size = 0.1 # How much to increase lambda_reg each time it advances\n",
    "cooldown = 3 # How many epochs to allow before checking whether training loss increases and advacing lambda_reg if so\n",
    "cooldown_counter = cooldown\n",
    "lambda_reg_goal = 2 # Stop training once lambda_reg advances past this\n",
    "\n",
    "# from customRandAffine import AffineAugmentation\n",
    "# affine_aug = AffineAugmentation(spatial_size, 0.8)\n",
    "\n",
    "model = RegModel(\n",
    "    lambda_reg = 0.1,\n",
    "    lambda_sim = 1e7,\n",
    "    device=device,\n",
    "    down_convolutions=4,\n",
    "    depth=4,\n",
    "    max_channels=256,\n",
    "    init_channels=32,\n",
    "    icon_loss_type=IconLossType.ICON,\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "schedule_lr = False\n",
    "min_lr=1e-5\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1**(1/3000))\n",
    "\n",
    "loss_curves_train = LossCurves(\"training\")\n",
    "loss_curves_valid = LossCurves(\"validation\", include_folds=True, spatial_dims=spatial_size)\n",
    "\n",
    "batches_per_epoch_train = len(dl_train)//2\n",
    "print(\"Number of trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "print(\"Number of batches per epoch:\", batches_per_epoch_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fd046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "e=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124a9017",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "\n",
    "max_epochs = 10\n",
    "while e < max_epochs:\n",
    "#     current_lr = scheduler.get_last_lr()[0]\n",
    "    current_lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    print(f'Epoch {e+1}/{max_epochs} (LR = {current_lr:.1e}, lambda_reg = {model.lambda_reg:.1e}):')\n",
    "    \n",
    "    # Train\n",
    "    model.train()\n",
    "    dl_train_iter = iter(dl_train)\n",
    "    while True:\n",
    "        try:\n",
    "            b1 = next(dl_train_iter)\n",
    "            b2 = next(dl_train_iter)\n",
    "        except StopIteration:\n",
    "            break\n",
    "            \n",
    "#         fixed, moving = affine_aug(b1[fa_key], b2[fa_key])\n",
    "        fixed, moving = b1['dti'], b2['dti']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        model_output = model(fixed, moving)\n",
    "        model_output.all_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_curves_train.add_to_buffer(model_output)\n",
    "        del(model_output)\n",
    "        print('.', end='')\n",
    "    print()\n",
    "\n",
    "    if schedule_lr:\n",
    "        if scheduler.get_last_lr()[0] > min_lr:\n",
    "            scheduler.step()\n",
    "\n",
    "    loss_curves_train.aggregate_buffers_for_epoch(e)\n",
    "    if print_aggregate_when(e):\n",
    "        l_all = np.mean(loss_curves_train.all_losses[last_printed_epoch+1:])\n",
    "        l_sim = np.mean(loss_curves_train.sim_losses[last_printed_epoch+1:])\n",
    "        l_icon = np.mean(loss_curves_train.icon_losses[last_printed_epoch+1:])\n",
    "        print(f\"\\tTraining loss: {l_all:.4f} (sim (weighted)={model.lambda_sim*l_sim:.4f}, ic (weighted)={model.lambda_reg*l_icon:.4f})\")\n",
    "        print(f\"\\t(aggregated from epochs {last_printed_epoch+2} to {len(loss_curves_train.all_losses)})\")\n",
    "        last_printed_epoch = e\n",
    "    \n",
    "    # Validate\n",
    "    if validate_when(e):\n",
    "        model.eval()\n",
    "        dl_valid_iter = iter(dl_valid)\n",
    "        while True:\n",
    "            try:\n",
    "                b1 = next(dl_valid_iter)\n",
    "                b2 = next(dl_valid_iter)\n",
    "            except StopIteration:\n",
    "                break\n",
    "            \n",
    "#             fixed, moving = affine_aug(b1[fa_key], b2[fa_key])\n",
    "            fixed, moving = b1['dti'], b2['dti']\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model_output = model(fixed, moving)\n",
    "                loss_curves_valid.add_to_buffer(model_output)\n",
    "        loss_curves_valid.aggregate_buffers_for_epoch(e) \n",
    "        print(\"\\tValidation loss:\", loss_curves_valid.all_losses[-1])\n",
    "        print(\"\\tAverage folds:\", loss_curves_valid.fold_counts[-1])\n",
    "    \n",
    "    if schedule_lambda_reg:\n",
    "        cooldown_counter -= 1;\n",
    "        if cooldown_counter<=0 and loss_curves_train.all_losses[-1] > loss_curves_train.all_losses[-2]:\n",
    "            print(f\"Updating lambda_reg.\")\n",
    "            model.update_lambda_reg(model.lambda_reg + lambda_reg_step_size)\n",
    "            cooldown_counter = cooldown # reset cooldown_counter\n",
    "        if model.lambda_reg > lambda_reg_goal:\n",
    "            print(\"Reached goal lambda_reg.\")\n",
    "            break\n",
    "            \n",
    "    e += 1\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41b6c8f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Saving, plotting, and loading\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd70383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE\n",
    "\n",
    "loss_curves_train.plot(savepath = footsteps.output_dir + 'loss_plot_train.png')\n",
    "loss_curves_valid.plot(savepath = footsteps.output_dir + 'loss_plot_valid.png')\n",
    "\n",
    "with open(footsteps.output_dir + 'loss_curves.p', 'wb') as f:\n",
    "    pickle.dump([loss_curves_train, loss_curves_valid],f)\n",
    "\n",
    "torch.save(model.state_dict(), footsteps.output_dir + 'model_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e32faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD\n",
    "\n",
    "experiment_name_to_load = \"beh-1\"\n",
    "load_dir = os.path.join('results', experiment_name_to_load)\n",
    "\n",
    "model.load_state_dict(torch.load(os.path.join(load_dir,'model_state_dict.pth')))\n",
    "\n",
    "with open(os.path.join(load_dir,'loss_curves.p'), 'rb') as f:\n",
    "    loss_curves_train, loss_curves_valid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f5b1eb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Preview\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3f3522",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Quick preview\n",
    "\n",
    "dl = dl_valid # Choose whether to view performance on training or on validation data\n",
    "it = iter(dl)\n",
    "d1 = next(it)\n",
    "d2 = next(it)\n",
    "\n",
    "img_A = d1['fa']\n",
    "img_B = d2['fa']\n",
    "\n",
    "# img_A, img_B  = affine_aug(img_A, img_B)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    ddf = model(d1['dti'], d2['dti'], return_warp_only=True)\n",
    "\n",
    "warp = monai.networks.blocks.Warp(mode='bilinear', padding_mode='zeros')\n",
    "img_B_warped = warp(img_B, ddf)\n",
    "\n",
    "preview_slices = (80,80,80)\n",
    "\n",
    "print(\"moving:\")\n",
    "util.preview_image(img_B[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"warped moving:\")\n",
    "util.preview_image(img_B_warped[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"target:\")\n",
    "util.preview_image(img_A[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"checkerboard of warped moving and target:\")\n",
    "util.preview_checkerboard(img_A[0,0].cpu(), img_B_warped[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"deformation vector field:\")\n",
    "util.preview_3D_vector_field(ddf[0].cpu(), slices=preview_slices)\n",
    "print(\"deformed grid:\")\n",
    "util.preview_3D_deformation(ddf[0].cpu(),5, slices=preview_slices)\n",
    "print(\"jacobian determinant:\")\n",
    "det = util.jacobian_determinant(ddf[0].cpu())\n",
    "util.preview_image(det, normalize_by='slice', threshold=0, slices=preview_slices)\n",
    "num_folds = (det<0).sum()\n",
    "print(\"Number of folds:\", num_folds, f\"(folding rate {100*num_folds/np.prod(det.shape)}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ebb1c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Slow but more informative preview\n",
    "\n",
    "dl = dl_valid # Choose whether to view performance on training or on validation data\n",
    "it = iter(dl)\n",
    "d1 = next(it)\n",
    "d2 = next(it)\n",
    "\n",
    "img_A = d1['fa']\n",
    "img_B = d2['fa']\n",
    "\n",
    "# img_A, img_B  = affine_aug(img_A, img_B)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    model_output = model(d1['dti'], d2['dti'])\n",
    "\n",
    "warp = monai.networks.blocks.Warp(mode='bilinear', padding_mode='zeros')\n",
    "img_B_warped = warp(img_B, model_output.deformation_AB)\n",
    "\n",
    "preview_slices = (80,80,80)\n",
    "\n",
    "print(\"moving:\")\n",
    "util.preview_image(img_B[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"warped moving:\")\n",
    "util.preview_image(img_B_warped[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"target:\")\n",
    "util.preview_image(img_A[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"checkerboard of warped moving and target:\")\n",
    "util.preview_checkerboard(img_A[0,0].cpu(), img_B_warped[0,0].cpu(), figsize=(18,10), slices=preview_slices)\n",
    "print(\"deformation vector field:\")\n",
    "util.preview_3D_vector_field(model_output.deformation_AB[0].cpu(), slices=preview_slices)\n",
    "print(\"deformed grid:\")\n",
    "util.preview_3D_deformation(model_output.deformation_AB[0].cpu(),5, slices=preview_slices)\n",
    "print(\"jacobian determinant:\")\n",
    "det = util.jacobian_determinant(model_output.deformation_AB[0].cpu())\n",
    "util.preview_image(det, normalize_by='slice', threshold=0, slices=preview_slices)\n",
    "print(\"sim loss:\", model_output.sim_loss.item())\n",
    "print(\"(grad?)icon loss:\", model_output.icon_loss.item())\n",
    "print(\"overall loss:\", model_output.all_loss.item())\n",
    "num_folds = (det<0).sum()\n",
    "print(\"Number of folds:\", num_folds, f\"(folding rate {100*num_folds/np.prod(det.shape)}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
