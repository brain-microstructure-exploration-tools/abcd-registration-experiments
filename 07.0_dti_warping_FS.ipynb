{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb8f609",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "import dipy.io.image\n",
    "import dipy.reconst.dti\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ebf5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview(img):\n",
    "    fig,axs = plt.subplots(1,3,figsize=(20,10))\n",
    "    axs[0].imshow(img[62,:,:].T, origin='lower', cmap='gray')\n",
    "    axs[1].imshow(img[:,:,80].T, origin='lower', cmap='gray')\n",
    "    axs[2].imshow(img[:,75,:].T, origin='lower', cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "dti_image_paths = list(Path('dti_fit_images_nontest/dti/').glob('*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0604f74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dti_image_path = random.choice(dti_image_paths)\n",
    "print(dti_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060506e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data, affine = dipy.io.image.load_nifti(dti_image_path)\n",
    "img_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e871e866",
   "metadata": {},
   "source": [
    "Now `img_data` is a numpy array of shape (140,140,140,6), representing the lower triangular entries of a diffusion tensor on a space of shape (140,140,140). I believe they are in the order Dxx, Dxy, Dyy, Dxz, Dyz, Dzz; see [here](https://dipy.org/documentation/1.4.0./reference/dipy.reconst)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4816090",
   "metadata": {},
   "outputs": [],
   "source": [
    "dti = dipy.reconst.dti.from_lower_triangular(img_data)\n",
    "\n",
    "assert((dti[:,:,:,0,1]==dti[:,:,:,1,0]).all())\n",
    "assert((dti[:,:,:,1,2]==dti[:,:,:,2,1]).all())\n",
    "assert((dti[:,:,:,0,2]==dti[:,:,:,2,0]).all())\n",
    "dti.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2314a12f",
   "metadata": {},
   "source": [
    "Above we have produced the 3x3 symmetric matrices from the lower triangular part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8e0d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2db91f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dti_tensor = torch.tensor(dti).permute((3,4,0,1,2)).unsqueeze(0)\n",
    "\n",
    "# Need to think about dipy axis order more carefully, but for now let's see if solving will work at all.\n",
    "dti_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38467d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute a warp from FA images, just so we have a warp to play with\n",
    "\n",
    "from fa_deformable_registration_models.reg_model1 import RegModel\n",
    "\n",
    "reg_model = RegModel(device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093c0a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FA of this DTI image and then also load some other random FA image\n",
    "\n",
    "fa_image_path = dti_image_path.parent.parent/'fa'/dti_image_path.name\n",
    "fa_image_path2 = random.choice(list((dti_image_path.parent.parent/'fa').glob('*')))\n",
    "print(fa_image_path, fa_image_path2, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4762356b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Turn the FA images into tensors and compute a deformation that aligns our original FA image to the random one\n",
    "\n",
    "fa_img, affine = dipy.io.image.load_nifti(fa_image_path)\n",
    "fa_img2, affine = dipy.io.image.load_nifti(fa_image_path2)\n",
    "\n",
    "fa_tensor1 = torch.tensor(fa_img, dtype=torch.float32).unsqueeze(0)\n",
    "fa_tensor2 = torch.tensor(fa_img2, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "ddf, fa_tensor1_warped = reg_model.forward(fa_tensor2, fa_tensor1, include_warped_image=True)\n",
    "\n",
    "from util import preview_3D_vector_field\n",
    "preview_3D_vector_field(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e581798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have a warp to work with, so let's start from the DTI img_data again and show how we warp it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f318ee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spatial_derivatives import DerivativeOfDDF\n",
    "\n",
    "deriv_ddf = DerivativeOfDDF(device=reg_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0b08b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute the derivative matrix field of the warp\n",
    "\n",
    "c,h,w,d = ddf.shape\n",
    "b=1\n",
    "assert(c==3)\n",
    "J = deriv_ddf(ddf.unsqueeze(0)).reshape(b,3,3,h,w,d)\n",
    "J.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce6eb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name some operations to make it easier to interpret the steps below\n",
    "from util import batchify\n",
    "dipy2torch_lotri_batch = lambda t : t.permute(0,4,1,2,3)\n",
    "torch2dipy_lotri_batch = lambda t : t.permute(0,2,3,4,1)\n",
    "dipy2torch_mat_batch = lambda t : t.permute(0,4,5,1,2,3)\n",
    "torch2dipy_mat_batch = lambda t : t.permute(0,3,4,5,1,2)\n",
    "dipy_lotri2mat = dipy.reconst.dti.from_lower_triangular\n",
    "dipy_lotri2mat_batch = batchify(dipy_lotri2mat)\n",
    "dipy_mat2lotri = dipy.reconst.dti.lower_triangular\n",
    "dipy_mat2lotri_batch = batchify(dipy.reconst.dti.lower_triangular)\n",
    "torch_lotri2mat_batch = lambda t : dipy2torch_mat_batch(dipy_lotri2mat_batch(torch2dipy_lotri_batch(t)))\n",
    "torch_mat2lotri_batch = lambda t : dipy2torch_lotri_batch(dipy_mat2lotri_batch(torch2dipy_mat_batch(t)))\n",
    "torch_mat_batch_absorbspatial = lambda t : t.permute((0,3,4,5,1,2)).reshape((-1,3,3))\n",
    "torch_mat_batch_expandspatial = lambda t,h,w,d : t.reshape(b,h,w,d,3,3).permute((0,4,5,1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c86df82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take our original DTI and make it a tensor\n",
    "# and use a naming convention that clearly explains the shapes of things\n",
    "F_dipy_lotri_batch = torch.tensor(img_data).unsqueeze(0).float().to(J)\n",
    "J_torch_mat_batch = J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d649d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warp the DTI, spatially moving tensors but not transforming the tensors yet\n",
    "F_torch_lotri_batch = dipy2torch_lotri_batch(F_dipy_lotri_batch)\n",
    "F_warped_torch_lotri_batch = reg_model.model.warp(F_torch_lotri_batch, ddf.unsqueeze(0))\n",
    "F_warped_torch_mat_batch = torch_lotri2mat_batch(F_warped_torch_lotri_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c5035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the spatial dimensions into the batch dimension\n",
    "\n",
    "F_warped_torch_mat_batch_nospatial = torch_mat_batch_absorbspatial(F_warped_torch_mat_batch)\n",
    "J_torch_mat_batch_nospatial = torch_mat_batch_absorbspatial(J_torch_mat_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0b3774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SVD of jacobian\n",
    "U, S, Vh = torch.linalg.svd(J_torch_mat_batch_nospatial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f9046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduce the orthogonal component of the jacobian, in the sense of its polar decomposition\n",
    "Jrot_torch_mat_batch_nospatial = torch.matmul(U, Vh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21b7239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check that Jrot is an orthogonal matrix\n",
    "(torch.matmul(Jrot_torch_mat_batch_nospatial, Jrot_torch_mat_batch_nospatial.permute((0,2,1))) - torch.repeat_interleave(torch.eye(3).unsqueeze(0), Jrot_torch_mat_batch_nospatial.shape[0], dim=0)).max().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca3f4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform tensors using the tensor transformation law, but using only the rotational component Jrot of J\n",
    "F_warped_transformed_torch_mat_batch_nospatial = torch.matmul(\n",
    "    Jrot_torch_mat_batch_nospatial.permute(0,2,1),\n",
    "    torch.matmul(\n",
    "        F_warped_torch_mat_batch_nospatial,\n",
    "        Jrot_torch_mat_batch_nospatial,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd35ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the spatial dimensions back out of the batch dimension\n",
    "\n",
    "F_warped_transformed_torch_mat_batch =\\\n",
    "    torch_mat_batch_expandspatial(F_warped_transformed_torch_mat_batch_nospatial, h, w, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0570ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to dipy indexing  so we can compute a new FA image out of our fully transformed DTI\n",
    "F_warped_transformed_dipy_lotri = dipy_mat2lotri(torch2dipy_mat_batch(F_warped_transformed_torch_mat_batch)[0])\n",
    "\n",
    "# Compute the new FA image\n",
    "eig = dipy.reconst.dti.eig_from_lo_tri(F_warped_transformed_dipy_lotri) # has eigenvals and eigenvecs\n",
    "eigvals = eig[:,:,:,:3] # take only the eigenvals\n",
    "fa_after_transform = dipy.reconst.dti.fractional_anisotropy(eigvals)\n",
    "print(fa_after_transform.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c419038e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"FA image 1:\")\n",
    "preview(fa_img)\n",
    "print(\"FA image 2:\")\n",
    "preview(fa_img2)\n",
    "print(\"The result of inferring a deformation from FA image 1 to FA image 2, using that deformation to transform DTI image 1, and then computing the FA of the resulting transformed DTI:\")\n",
    "preview(fa_after_transform)\n",
    "print(\"The result of applying that same deformation directly to FA image 1:\")\n",
    "preview(fa_tensor1_warped[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0189bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the difference between transforming the FA image directly and transforming the DTI\n",
    "# Theoretically there should be no difference because using the orthogonal matrix Jrot\n",
    "# to transform the diffusion tensors should preserve eigenvalues.\n",
    "\n",
    "absolute_difference = np.abs(fa_tensor1_warped[0] - fa_after_transform)\n",
    "print(\"Mean absolute difference:\", np.mean(absolute_difference))\n",
    "print(\"99.9th percentile:\", np.percentile(absolute_difference, 99.9))\n",
    "print(\"Max:\", np.max(absolute_difference))\n",
    "preview(absolute_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de49757",
   "metadata": {},
   "source": [
    "Moving voxels around for the sake of spatial correspondence should not affect our description of white matter microstructure. Actual water diffusion at the molecular scale in that fiber bundle is a microstructure property, and therefore it shouldn't change just because some voxels were moved around. It makes sense to transform DTs with orientation changes alone, and to never scale the eigenvalues while doing so. In the DT description, it is only the rotational aspect that cares about spatial arrangement of other voxels. Any other aspect is going to be a microsctructure descriptor that should essentially be treated like a scalar, i.e. invariant of the coordinate system.\n",
    "\n",
    "If we have successfully preserved eigenvalues in our chain of transformations, the image above should be zero.\n",
    "However, we see that there is some error, especially at the brain mask edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49051eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peel off a few layers from the brain mask boundary and check the absolute difference again, to see\n",
    "# the extent to which the errors occur at the mask boundary\n",
    "\n",
    "import ants\n",
    "brainmask_path_2 = fa_image_path2.parent.parent/'brainmask'/fa_image_path2.name\n",
    "brainmask2 = ants.image_read(str(brainmask_path_2))\n",
    "\n",
    "mask = brainmask2.morphology('erode',3)\n",
    "\n",
    "absolute_difference_masked = absolute_difference * mask.numpy()\n",
    "print(\"Mean absolute difference:\", np.mean(absolute_difference_masked))\n",
    "print(\"99.9th percentile:\", np.percentile(absolute_difference_masked, 99.9))\n",
    "print(\"Max:\", np.max(absolute_difference_masked))\n",
    "preview(absolute_difference_masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d96cd46",
   "metadata": {},
   "source": [
    "If we go above and leave out the tensor transformation step, e.g. by setting `F_warped_transformed_torch_mat_batch_nospatial = F_warped_torch_mat_batch_nospatial`, then there's not much change in the error. There's still a significant error of about the same magnitude. Therefore I believe the error could come from the method of _interpolation_ of diffusion tensors, rather than coming from an error in the transformations. If I go above and change the interpolation to nearest-neighbor (on both of the warps, the warp directly being applied to the FA and the warp being applied to the DTI), then the error goes away almost entirely. So that's strong evidence that the incorrect linear interpolation of diffusion tensors is messing with the eigenvalues.\n",
    "\n",
    "Or at least this is what we're observing: linear interpolation does not commute with FA value computation. Linear interpolation of DTs is not very natural (as pointed out [here](https://onlinelibrary.wiley.com/doi/pdf/10.1002/mrm.20334)), but I don't think linear interpolation of FA values is totally natural either. Perhaps it's alright for the purpose of learning. Let's stick with it for now, see how it does with deep learning, and leave the problem of improving interpolation for a future direciton."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbaceb5",
   "metadata": {},
   "source": [
    "Here is an encapsulation of the above code for DTI transformation into a convenient module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6882a089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dti_warp\n",
    "warp_dti = dti_warp.WarpDTI(device = reg_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53c7751",
   "metadata": {},
   "outputs": [],
   "source": [
    "dti = F_torch_lotri_batch\n",
    "dti_warped = warp_dti(dti, ddf.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541cc13e",
   "metadata": {},
   "source": [
    "As another sanity check, let's look at images of the principal direction of diffusion before and after warp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17fc277",
   "metadata": {},
   "outputs": [],
   "source": [
    "eig = dipy.reconst.dti.eig_from_lo_tri(torch2dipy_lotri_batch(dti)[0])\n",
    "princ_diffusion_direction = eig[:,:,:,3:6] # take first eigenvector (principal direction of diffusion)\n",
    "princ_eigenvalue = eig[:,:,:,0]\n",
    "\n",
    "eig2 = dipy.reconst.dti.eig_from_lo_tri(torch2dipy_lotri_batch(dti_warped)[0])\n",
    "princ_diffusion_direction_after_warp = eig2[:,:,:,3:6] # take first eigenvector (principal direction of diffusion)\n",
    "princ_eigenvalue_after_warp = eig2[:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24bc3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to above but let's view both at the same time\n",
    "\n",
    "import vtkmodules.vtkInteractionStyle\n",
    "import vtkmodules.vtkRenderingOpenGL2\n",
    "from vtkmodules.vtkCommonColor import vtkNamedColors\n",
    "from vtkmodules.vtkCommonCore import VTK_DOUBLE\n",
    "from vtkmodules.vtkCommonDataModel import vtkImageData\n",
    "from vtkmodules.vtkFiltersGeometry import vtkImageDataGeometryFilter\n",
    "from vtkmodules.vtkInteractionStyle import vtkInteractorStyleTrackballCamera\n",
    "from vtkmodules.vtkFiltersCore import vtkGlyph3D\n",
    "from vtkmodules.vtkFiltersCore import vtkTensorGlyph\n",
    "from vtkmodules.vtkFiltersSources import vtkLineSource\n",
    "from vtkmodules.vtkCommonCore import (vtkPoints, vtkDoubleArray)\n",
    "from vtkmodules.vtkRenderingCore import (\n",
    "    vtkActor,\n",
    "    vtkPolyDataMapper,\n",
    "    vtkRenderWindow,\n",
    "    vtkRenderWindowInteractor,\n",
    "    vtkRenderer\n",
    ")\n",
    "from vtkmodules.vtkCommonDataModel import vtkPolyData\n",
    "\n",
    "colors = vtkNamedColors()\n",
    "\n",
    "def view_principal_diffusion_direction_renderer_only(princ_eigenvalue, princ_diffusion_direction, axial_slice_index):\n",
    "    imageData = vtkImageData()\n",
    "    h,w,d = princ_eigenvalue.shape\n",
    "    imageData.SetDimensions(h,w,1)\n",
    "    imageData.AllocateScalars(VTK_DOUBLE, 1)\n",
    "\n",
    "    dims = imageData.GetDimensions()\n",
    "\n",
    "    max_eigenval = np.max(princ_eigenvalue[:,:,axial_slice_index])\n",
    "    vecs = vtkDoubleArray()\n",
    "    vecs.SetNumberOfComponents(3)\n",
    "    vecs.SetName(\"princ_diffusion_direction\")\n",
    "    for i in range(imageData.GetNumberOfPoints()):\n",
    "        x,y,z = imageData.GetPoint(i)\n",
    "        x,y,z = int(x), int(y), int(z)\n",
    "        vecs.InsertComponent(i,0,princ_diffusion_direction[x,y,axial_slice_index,0])\n",
    "        vecs.InsertComponent(i,1,princ_diffusion_direction[x,y,axial_slice_index,1]) \n",
    "        vecs.InsertComponent(i,2,princ_diffusion_direction[x,y,axial_slice_index,2])\n",
    "    \n",
    "    for y in range(dims[1]):\n",
    "        for x in range(dims[0]):\n",
    "            imageData.SetScalarComponentFromDouble(x, y, 0, 0, princ_eigenvalue[x,y,70]/max_eigenval)\n",
    "\n",
    "    imageData.GetPointData().AddArray(vecs)\n",
    "    imageData.GetPointData().SetActiveVectors('princ_diffusion_direction')\n",
    "    \n",
    "\n",
    "    lineSource = vtkLineSource()\n",
    "    lineSource.SetPoint1(0,0,0)\n",
    "    lineSource.SetPoint2(1.0,0,0)\n",
    "    glyph3D = vtkGlyph3D()\n",
    "    glyph3D.SetSourceConnection(lineSource.GetOutputPort())\n",
    "    glyph3D.SetInputData(imageData)\n",
    "    glyph3D.OrientOn()\n",
    "    glyph3D.ScalingOn()\n",
    "    glyph3D.SetVectorModeToUseVector()\n",
    "    glyph3D.SetScaleModeToScaleByScalar()\n",
    "    \n",
    "\n",
    "    mapper = vtkPolyDataMapper()\n",
    "    mapper.SetInputConnection(glyph3D.GetOutputPort())\n",
    "    mapper.ScalarVisibilityOn()\n",
    "\n",
    "    actor = vtkActor()\n",
    "    actor.SetMapper(mapper)\n",
    "    actor.GetProperty().SetLineWidth(2)\n",
    "    actor.GetProperty().SetColor(colors.GetColor3d('White'))\n",
    "\n",
    "    # Setup rendering\n",
    "    renderer = vtkRenderer()\n",
    "    renderer.AddActor(actor)\n",
    "    renderer.SetBackground(colors.GetColor3d('Black'))\n",
    "    renderer.ResetCamera()\n",
    "    \n",
    "    return renderer\n",
    "\n",
    "def view_both(axial_slice, princ_eigenvalue, princ_diffusion_direction, princ_eigenvalue_after_warp, princ_diffusion_direction_after_warp):\n",
    "    renderer1 = view_principal_diffusion_direction_renderer_only(princ_eigenvalue, princ_diffusion_direction,axial_slice)\n",
    "    renderer2 = view_principal_diffusion_direction_renderer_only(princ_eigenvalue_after_warp, princ_diffusion_direction_after_warp,axial_slice)\n",
    "    renderWindow = vtkRenderWindow()\n",
    "    renderWindow.AddRenderer(renderer1)\n",
    "    renderWindow.AddRenderer(renderer2)\n",
    "    renderer1.SetViewport([0, 0, 0.5, 1])\n",
    "    renderer2.SetViewport([0.5, 0, 1, 1])\n",
    "    renderer2.SetActiveCamera(renderer1.GetActiveCamera())\n",
    "    renderWindow.SetSize(1900,800)\n",
    "\n",
    "    renderWindowInteractor = vtkRenderWindowInteractor()\n",
    "    style = vtkInteractorStyleTrackballCamera()\n",
    "    renderWindowInteractor.SetInteractorStyle(style)\n",
    "\n",
    "    renderWindowInteractor.SetRenderWindow(renderWindow)\n",
    "    renderWindowInteractor.Initialize()\n",
    "    renderWindowInteractor.Start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b04321",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_both(75, princ_eigenvalue, princ_diffusion_direction, princ_eigenvalue_after_warp, princ_diffusion_direction_after_warp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a987233",
   "metadata": {},
   "source": [
    "Let's now try with a handmade rotation+scaling and observe that the colors remain the same (eigenvalues are preserved) and the vectors rotate properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19058b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example_ddf_3d(s_x, s_y=None, s_z=None, th=2*np.pi/8, oy=0.5, oz=0.5, scaling = 1.0):\n",
    "    \"\"\"Get an example DDF (direct displacement field).\n",
    "    Arguments:\n",
    "        s_x, s_y. s_z: The x,y,z scales. Provide s_x only to have them be the same scale.\n",
    "            \"Scale\" here really means \"resolution.\" Think of it as the same underlying displacement,\n",
    "            but meant to be applied to images at different resolutions.\n",
    "        th: rotation angle in radians\n",
    "        oy, oz: the rotation center in [0,1]\\times [0,1] coordinates\n",
    "        scaling: any scaling to also perform\n",
    "    \"\"\"\n",
    "    if s_y is None:\n",
    "        s_y=s_x\n",
    "    if s_z is None:\n",
    "        s_z = s_x\n",
    "    m = np.array([[np.cos(th), -np.sin(th)],[np.sin(th), np.cos(th)]]) * scaling\n",
    "    ddf = torch.tensor(\n",
    "        [[[\n",
    "            [\n",
    "                (y-oy*s_y) * m[1,0] + (z-oz*s_z) * m[1,1] - (z-oz*s_z), # z component\n",
    "                (y-oy*s_y) * m[0,0] + (z-oz*s_z) * m[0,1] - (y-oy*s_y), # y component\n",
    "                0, # x component\n",
    "            ]\n",
    "            for x in range(s_x)]\n",
    "            for y in range(s_y)]\n",
    "            for z in range(s_z)\n",
    "        ]\n",
    "    ).permute((3,0,1,2)).float()\n",
    "    return ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4ebbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = get_example_ddf_3d(140, th=2*np.pi/8, scaling=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7927f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_3D_vector_field(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d586fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "warp_dti = dti_warp.WarpDTI(\n",
    "    device = reg_model.device,\n",
    "    tensor_transform_type=dti_warp.TensorTransformType.FINITE_STRAIN\n",
    ")\n",
    "dti = F_torch_lotri_batch\n",
    "dti_warped = warp_dti(dti, ddf.unsqueeze(0))\n",
    "\n",
    "eig = dipy.reconst.dti.eig_from_lo_tri(torch2dipy_lotri_batch(dti)[0])\n",
    "princ_diffusion_direction = eig[:,:,:,3:6] # take first eigenvector (principal direction of diffusion)\n",
    "princ_eigenvalue = eig[:,:,:,0]\n",
    "\n",
    "eig2 = dipy.reconst.dti.eig_from_lo_tri(torch2dipy_lotri_batch(dti_warped)[0])\n",
    "princ_diffusion_direction_after_warp = eig2[:,:,:,3:6] # take first eigenvector (principal direction of diffusion)\n",
    "princ_eigenvalue_after_warp = eig2[:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd0a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_both(75, princ_eigenvalue, princ_diffusion_direction, princ_eigenvalue_after_warp, princ_diffusion_direction_after_warp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
